\documentclass[12pt]{article}
\usepackage[left=.75in, right=.75in, top=1in, bottom = 1in]{geometry}
\usepackage{enumitem, amssymb, amsmath, amsfonts, mathtools, parskip}
\usepackage{cancel}

% \newcommand\italicmath{\mathversion{italic}}
% \DeclareMathVersion{italic}
% \SetSymbolFont{operators}{italic}{OT1}{cmr} {m}{it}
% \SetSymbolFont{letters}  {italic}{OML}{cmm} {m}{it}
% \SetSymbolFont{symbols}  {italic}{OMS}{cmsy}{m}{n}
% \SetMathAlphabet\mathsf{italic}{OT1}{cmss}{m}{sl}
% \SetMathAlphabet\mathit{italic}{OT1}{cmr}{m}{it}

% \newcommand\bitalicmath{\mathversion{bitalic}}
% \DeclareMathVersion{bitalic}
% \SetSymbolFont{operators}{bitalic}{OT1}{cmr} {bx}{it}
% \SetSymbolFont{letters}  {bitalic}{OML}{cmm} {b}{it}
% \SetSymbolFont{symbols}  {bitalic}{OMS}{cmsy}{b}{n}
% \SetMathAlphabet\mathsf{bitalic}{OT1}{cmss}{bx}{sl}
% \SetMathAlphabet\mathit{bitalic}{OT1}{cmr}{bx}{it}

\begin{document}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
\section{Solving System of Linear Equations \(Ax=b\)}

% P Norm and Condition Number
\subsection{\(p\)-Norm and Condition Number}
\vspace{10pt}\noindent
% Vectors
\begin{minipage}[t]{.49\textwidth}
	% Vector P Norm
	\underline{Vector \(p\)-Norm}: \ \ 
	\fbox{\(\Vert \vec{x} \Vert_p \ =\ \sqrt[\leftroot{-1}\uproot{5}{ p }]{\ \displaystyle \sum_i |x_i|^p\ }\)}

	% 1 and infty norms
	\vspace{10pt}\hspace{10pt}\(\begin{array}{r l}
		{\text{\(1\)-Norm}}: &
			\Vert \vec{x} \Vert_1 \ =\ \displaystyle \sum_i |x_i|\\[20pt]
		{\text{\(\infty\)-Norm}}: &
			\Vert \vec{x} \Vert_\infty \ =\ \max |x_i|
	\end{array}\)

	% Vector Norm Extra Info
	\vspace{20pt}
	\(\setlength{\arraycolsep}{4pt}\begin{array}{l r c c c c}
		\bullet & \Vert x \Vert_1 &\geq&  \Vert x \Vert_2 			&\geq& \Vert x \Vert_\infty\\[15pt]
		\bullet & \Vert x \Vert_1 &\leq&  \sqrt{n}\ \Vert x \Vert_2 	&\leq& \sqrt{n}\ \Vert x \Vert_\infty
	\end{array}\)
\end{minipage}
\begin{minipage}[t]{.49\textwidth}
	\setlength{\parindent}{.5cm}
	% Func/Vector Condition Number
	\noindent
	\underline{Function/Vector Condition Number}:\\[15pt]
	\indent\(\begin{aligned}
		\text{cond}\Big( f(x) \Big) 
		&= \left\vert \frac{[f(\hat{x}) - f(x)] / f(x)}{[\hat{x} - x] / x} \right\vert\\[5pt]
		&= \left\vert \frac{\Delta y / y}{\Delta x / x}\right\vert 
			= \left\vert \frac{y' \cdot \Delta x / y}{\Delta x / x}\right\vert\\[5pt]
		&= \left\vert \frac{x f'(x)}{f(x)}\right\vert
	\end{aligned}\)
\end{minipage}

% Matrices
\vspace{20pt}\noindent
\begin{minipage}[t]{.49\textwidth}
	% Matrix P Norm
	\underline{Matrix \(p\)-Norm}: \ \ 
	\fbox{\(\displaystyle \Vert A \Vert_p \ =\ \max_{x \neq 0} \frac{\Vert Ax \Vert}{\Vert x \Vert}\)}

	% 1 and infty norms
	\vspace{10pt}\hspace{10pt}\(\begin{array}{r l}
		{\text{\(1\)-Norm}}: &
			\displaystyle \Vert A \Vert_1 \ =\ \max_{j}\ \sum_i \vert a_{ij} \vert\\[20pt]
		{\text{\(\infty\)-Norm}}: &
			\displaystyle \Vert A \Vert_\infty \ =\ \max_{i}\ \sum_j \vert a_{ij} \vert
	\end{array}\)

	% Matrix Norm Extra Info
	\vspace{10pt}
	\(\left.\setlength{\arraycolsep}{3pt}\begin{array}{l r c l}
		\bullet& \Vert AB \Vert &\leq& \Vert A \Vert \cdot \Vert B \Vert\\[15pt]
		\bullet& \Vert Ax \Vert &\leq& \Vert A \Vert \cdot \Vert x \Vert
	\end{array}\right\} \begin{gathered}
		\scriptstyle \text{For \(p\)-norms (not} \\[-5pt]
		\scriptstyle \text{necessarily in general)}
	\end{gathered}\)
\end{minipage}
\begin{minipage}[t]{.49\textwidth}
	\setlength{\parindent}{.5cm}
	% Matrix Condition Number
	\noindent
	\underline{Matrix Condition Number}:\\[15pt]
	\indent\(\begin{aligned}
		&\boxed{ \text{cond}_p(A) = \Vert A \Vert_p \cdot \Vert A^{-1} \Vert_p }
					\hspace{20pt} {\scriptstyle(\text{\(\infty\) if singular})}\\[5pt]
		&\hspace{20pt}= \frac{{\displaystyle \max_{x \neq 0}}\ \Vert Ax \Vert_p / \Vert x \Vert_p}
			{{\displaystyle \min_{x \neq 0}}\ \Vert Ax \Vert_p / \Vert x \Vert_p}
			\ =\ \text{cond}_p(\gamma A)
			\ \geq\ 1\\[10pt]
		&\bullet\ \text{Diagonal, } D: \ \text{cond}(D) = \tfrac{\max \vert d_i \vert}{\min \vert d_i \vert}\\[3pt]
		&\bullet\ \begin{aligned}[t]
			&\Vert z \Vert = \Vert A^{-1} y \Vert \ \leq\ \Vert A^{-1} \Vert \cdot \Vert y \Vert\\[3pt]
			&\rightarrow \tfrac{\Vert z \Vert}{\Vert y \Vert} 
				\ \leq\ \max \tfrac{\Vert z \Vert}{\Vert y \Vert} 
				\stackrel{?}{=} \Vert A^{-1} \Vert \hspace{15pt} \text{\scriptsize(optimize)}
		\end{aligned}
	\end{aligned}\)
\end{minipage}

%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
% Residual, and Error Bounds when Solving Ax = b
\newpage
\subsection{Error Bounds and Residuals}
% Error Bound
\vspace{10pt}\noindent
\underline{Error Bound}:\ \ \(\boxed{ \displaystyle \frac{\Vert \hat{x} - x \Vert}{\Vert x \Vert} 
	\ \lessapprox\ \text{cond}(A)\ \epsilon_\text{mach} } 
	\ \rightarrow \ \begin{minipage}{8cm}
		\scriptsize
		A computed solution is expected to lose about \(\log_{10}(\text{cond}(A))\) digits, so
		the input data must be more accurate to these digits and 
		the working precision must carry more than these digits.
	\end{minipage}\)

% Error Bound for Change in Vector b
\vspace{15pt}\noindent
\(\begin{aligned}[t]
	&A\hat{x} = b + \Delta b = Ax + A\Delta x\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{array}{l c c c}
			\bullet&\Vert b \Vert 			&\leq& \Vert A \Vert \cdot \Vert x \Vert\\[10pt]
			\bullet\ &\Vert \Delta x \Vert 	&\leq& \Vert A^{-1} \Vert \cdot \Vert \Delta b \Vert
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}}
\end{aligned}
\hspace{2cm}
% Residual
\begin{aligned}[t]
	&A\hat{x} + r = b\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{aligned}
			\bullet\ \Vert \Delta x \Vert 
				&= \Vert A^{-1}(A\hat{x} - b) \Vert
				= \Vert -A^{-1} r \Vert \\[5pt]
			&\leq \Vert A^{-1} \Vert \cdot \Vert r \Vert
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert r \Vert}{\Vert A \Vert \cdot \Vert \hat{x} \Vert}}
\end{aligned}\)

% Error Bound for Change in Matrix A
\vspace{20pt}\noindent
\(\begin{aligned}[t]
	&(A + \Delta A)\hat{x} = b\\[5pt]
	&\hspace{10pt} \begin{aligned}[t]
			\bullet\ \Vert \Delta x \Vert &= \Vert - A^{-1} (\Delta A) \hat{x} \Vert\\[10pt]
			&\leq \Vert A^{-1} \Vert \cdot \Vert \Delta A \Vert \cdot \Vert \hat{x} \Vert
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}}
\end{aligned}
\hspace{1.5cm}
% Residual
\begin{aligned}[t]
	&(A + \Delta A)\hat{x} = b\\[5pt]
	&\hspace{10pt} \begin{aligned}
		\bullet\ \Vert r \Vert 
			&= \Vert b - A \hat{x} \Vert
			= \Vert \Delta A \cdot \hat{x} \Vert \\[5pt]
		&\leq \Vert \Delta A \Vert \cdot \Vert \hat{x} \Vert
	\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert r \Vert}{\Vert A \Vert \cdot \Vert \hat{x} \Vert} 
		\leq \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}}
		\ , \ \ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \tfrac{\Vert A^{-1} \Vert \cdot \Vert r \Vert}{\Vert \hat{x} \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
\end{aligned}\)

% Calculus Error Bounds
\vspace{20pt}\noindent
\(\begin{aligned}[t]
	&\Big[ A(t) x(t)\ =\ b(t) \Big] 
		= \Big[ \big(A_0 + \Delta A \cdot t \big) x(t)\ =\ b_0 + \Delta b \cdot t \Big]\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{2pt}\begin{array}{l r c l}
			\bullet&x'(t) &=& \frac{b'(t) - A'(t)x(t)}{A(t)} = A^{-1}(t) \Big[ \Delta b - \Delta A\cdot x(t) \Big]\\[10pt]
			\bullet&x(t) &=& x_0 + x'(0)t + \mathcal{O}(t^2)
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert x(t) - x_0 \Vert}{\Vert x_0 \Vert} 
		\leq \text{cond}(A) \left( \tfrac{\Vert \Delta b \Vert}{\Vert b \Vert} 
		+ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert} \right) \vert t \vert 
		+ \mathcal{O}(t^2)}
\end{aligned}\)

%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
% LU Decomposition to Solve Linear System of Eq
\newpage
\subsection{Gaussian Elimination with LU/PLU/PLDUQ Decomposition}
% Elementary Elimination Matrices
\vspace{10pt}\noindent
\underline{Elementary Elimination Matrices, \(L_k\)}\\[10pt]
\(\begin{aligned}[t]
	&\\[-20pt]
	&\text{\scriptsize\(\left(\begin{matrix}
		1 & \dots & 0 & 0 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & 1 & 0 & \dots & 0\\
		0 & \dots & \tfrac{-a_{k+1}}{a_k} & 1 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & \tfrac{-a_n}{a_k} & 0 & \dots & 1\\
	\end{matrix}\right) 
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_k\\
		a_{k+1}\\
		\vdots\\
		a_n
	\end{matrix}\right) 
	=
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_k\\
		0\\
		\vdots\\
		0
	\end{matrix}\right)\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\bullet\ a_k\ \text{is the ``pivot''}\\
	&\bullet\ \text{is lower triangular}\\
	&\bullet\ \scriptstyle \forall i \neq j\ \ (L_k^{-1})_{ij}\ =\ - (L_k)_{ij} 
\end{aligned}
\hspace{30pt}
\begin{aligned}[t]
	&\text{Ex}:\\
	&\text{\scriptsize\(\left(\begin{matrix}
			1 & 0 & \dots\\
			-a_1/a_2 & 1 & \dots\\
			\vdots & \vdots & \ddots
		\end{matrix}\right) 
		\left(\begin{matrix}
			a_1\\
			a_2\\
			\vdots
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			a_1\\ 
			0\\
			\vdots
		\end{matrix}\right)\)}\\
\end{aligned}
\) 

% LU/PLU Factorization
\vspace{20pt}\noindent
\underline{LU/PLU Factorization (w/ partial pivoting)}

\vspace{5pt}
\(\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= LU \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(L\) is gen. triang.)}\\[-7pt]
					&\text{\scriptsize(\(U\) is upp. triang.)}
				\end{aligned}\\[5pt]
			L &= (\dots L_2 P_2 L_1 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\} b &= (\dots L_2 P_2 L_1 P_1) Ax \\[5pt]
			L^{-1}b &= (P_1^T L_1^{-1} P_2^T L_2^{-1} \dots)^{-1} Ax \\[5pt]
			&= L^{-1} (L U)x = y
		\end{aligned}\\[5pt]
	&\boxed{\begin{gathered}
			b = Ly\\[-5pt]
			\text{\scriptsize(forw.-sub.)}
		\end{gathered} 
		\ ,\ \ 
		\begin{gathered}
			y = Ux\\[-5pt]
			\text{\scriptsize(back.-sub.)}
		\end{gathered}}
\end{aligned}
\hspace{25pt}
\begin{aligned}[t]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize Permutation matrix, \\ 
			\(P_i\), rowswaps s.t. \\
			\(a_k \neq 0\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3.5cm}
			\scriptsize \(P_i\) rowswaps s.t. \(a_k\) is \\
			largest s.t. \(a_{k+i}/a_k \leq 1\)\\
			for numerical stability/\\
			minimize errors
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3.5cm}
		\scriptsize Pivoting isn't needed if \\
		\(A\) is diag. dom.\\
		\((a_{jj} > \sum_{i, i \neq j} a_{ij})\)
	\end{minipage}\\[5pt]
	&\bullet\ \text{\scriptsize \(A\) can be singular}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= PLU \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(P\) is rowswap permu.)}\\[-7pt]
					&\text{\scriptsize(\(L\) is unit low. triang.)}\\[-7pt]
					&\text{\scriptsize(\(U\) is upp. triang.)}
				\end{aligned}\\[5pt]
			P &= (\dots P_2 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\}b &= (\dots P_2 P_1)Ax\\[5pt]
			P^T b &= (P_1^T P_2^T \dots)^{-1} Ax\\[5pt]
			&= P^T (PLU)x = Ly
		\end{aligned}\\[10pt]
	&\boxed{P^T b = Ly\ ,\ \ y = Ux}
\end{aligned}\)

% LDU Diagonal Uniquness
\vspace{20pt}
\(\begin{aligned}[t]
	&\boxed{P^T A = LDU \hspace{20pt} \text{\scriptsize(D is diag.)}}\\[10pt]
	&\bullet\ \scriptstyle LDU\ \text{is unique up to } D\\
	&\bullet\ \scriptstyle LDU\ \text{is unique if } L/U \text{ are unit low./upp. diag., resp.}
\end{aligned}
% PAQ Full pivoting
\hspace{2cm}
\begin{aligned}[t]
	&\boxed{P^T A Q^T = LDU \hspace{20pt} \begin{aligned}
			&\text{\scriptsize(P is permu. for rows)}\\[-7pt]
			&\text{\scriptsize(Q is permu. for cols.)}
		\end{aligned}}\\[5pt]
	&\bullet\ \text{\scriptsize ``Complete pivoting'' search for largest \(a_k\)}\\[-3pt]
	&\bullet\ \text{\scriptsize Would be most numerically stable}\\[-3pt]
	&\bullet\ \text{\scriptsize Expensive, so not really used}
\end{aligned}\)

% Erro Bounds
\vspace{20pt}\noindent
\underline{Error Bound}:\ \(\tfrac{\Vert r \Vert}{\Vert A \Vert \Vert x \Vert} 
	\ \leq\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
	\ \leq\ \begin{gathered}[t]
		\rho\ n^2 \epsilon_\text{mach}\\[-5pt]
		\text{\scriptsize(Wilkinson)}
	\end{gathered}
	\ \sim\ \begin{gathered}[t]
		n \epsilon_\text{mach}\\[-5pt]
		\text{\scriptsize (usually)}
	\end{gathered}
	\hspace{40pt} \begin{minipage}{6cm}
		\scriptsize
		(growth factor, \(\rho\), is the largest entry at any point during factorization - usually at \(U\) - \\ 
		divided by the largest entry of A)
	\end{minipage}\)

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
% Gaussian-Jordan
\newpage
\subsection{Gaussian-Jordan with MD Decomposition}
% Elementary Elimination Matrices
\vspace{10pt}\noindent
\underline{Elementary Elimination Matrices, \(M_k\)}\\[10pt]
\(\begin{aligned}[t]
	&\\[-20pt]
	&\text{\scriptsize\(\left(\begin{matrix}
		1 & \dots & \tfrac{-a_1}{a_k} & 0 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & 1 & 0 & \dots & 0\\
		0 & \dots & \tfrac{-a_{k+1}}{a_k} & 1 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & \tfrac{-a_n}{a_k} & 0 & \dots & 1\\
	\end{matrix}\right) 
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_{k-1}\\
		a_k\\
		a_{k+1}\\
		\vdots\\
		a_n
	\end{matrix}\right) 
	=
	\left(\begin{matrix}
		0\\
		\vdots\\
		0\\
		a_k\\
		0\\
		\vdots\\
		0
	\end{matrix}\right)\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\bullet\ a_k\ \text{is the ``pivot''}\\
	&\bullet\ \scriptstyle \forall i \neq j\ \ (M_k^{-1})_{ij}\ =\ - (M_k)_{ij} 
\end{aligned}
\) 

% MD Factorization
\vspace{20pt}\noindent
\underline{MD Factorization (w/ partial pivoting)}

\vspace{5pt}
\(\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= MD \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(M\) is elem. elim.)}\\[-7pt]
					&\text{\scriptsize(\(D\) is diag.)}
				\end{aligned}\\[5pt]
			M &= (\dots M_2 P_2 M_1 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\} b &= (\dots M_2 P_2 M_1 P_1) Ax \\[5pt]
			M^{-1}b &= (P_1^T M_1^{-1} P_2^T M_2^{-1} \dots)^{-1} Ax \\[5pt]
			&= M^{-1} (MD)x = y
		\end{aligned}\\[5pt]
	&\boxed{\begin{gathered}
			M^{-1}b=y
		\end{gathered} 
		\ ,\ \ 
		\begin{gathered}[t]
			y = Dx\\[-5pt]
			\text{\scriptsize(division)}
		\end{gathered}}
\end{aligned}
\hspace{25pt}
\begin{aligned}[t]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize Permutation matrix, \\ 
			\(P_i\), rowswaps s.t. \\
			\(a_k \neq 0\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize \(P_i\) rowswaps cannot \\
			ensure numerical \\
			stability \((\leq 1)\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize 
			Division is \(\mathcal{O}(n)\), \\
			so may be useful for\\
			parallel comps.
		\end{minipage}\\[5pt]
	&\bullet\ \text{\scriptsize Can also find \(A^{-1}\)}
\end{aligned}
\hspace{30pt}
\text{\scriptsize\(
	\begin{aligned}[t]
		&\underline{\text{Finding } A^{-1}}\\[5pt]
		&D^{-1} M^{-1} (A | I) = (I | A^{-1})\\[5pt]
		&\hspace{10pt} \begin{aligned}[t]
			&= D^{-1} M^{-1} 
				\left[\left.\begin{matrix}
					a_{11} & \dots \\
					\vdots & a_{nn}	
				\end{matrix}\right|
				\begin{matrix}
					1 & 0\\
					0 & 1
				\end{matrix}\right]\\[5pt]
			&= \left[\begin{matrix}
					1 & 0\\
					0 & 1
				\end{matrix}
				\left|\begin{matrix}
					a'_{11} & \dots \\
					\vdots & a'_{nn}	
				\end{matrix}\right.\right]
		\end{aligned}
	\end{aligned}\)}
\)

%----------------------------------------------------------------
% Symmetric
\vspace{15pt}
\subsection{Symmetric Matrices}
% Positive Definite Def
\vspace{5pt}
\underline{Positive Definite}:\ \ \(\boxed{x^T Ax \geq 0}\) \\[15pt]
% Cholesky Factorization
\underline{Cholesky Factorization for Sym., Pos. Def.}:\ \ \(\boxed{A = LL^T = LDL^T}\)\\[15pt]
\text{\scriptsize\(\begin{aligned}
	&\left(\begin{matrix}
			a_{11} & a_{21} & a_{31} & \dots\\
			a_{21} & a_{22} & a_{32} & \dots\\
			a_{31} & a_{32} & a_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			l_{11} & 0 		& 0 	 & \dots\\
			l_{21} & l_{22} & 0 	 & \dots\\
			l_{31} & l_{32} & l_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)
		\left(\begin{matrix}
			l_{11} 	& l_{21} 	& l_{31} & \dots\\
			0 		& l_{22} 	& l_{32} & \dots\\
			0 		& 0 		& l_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)  
		= 
		\left(\begin{matrix}
			l_{11}^2 		& \dots 						& \dots 							& \dots\\
			l_{21} l_{11} 	& l_{21}^2 + l_{22}^2 			& \dots 							& \dots\\
			l_{31} l_{11} 	& l_{31}l_{21} + l_{32}l_{22} 	& l_{31}^2 + l_{32}^2 + l_{33}^2 	& \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)\\[5pt]
	% Notes
	&\begin{aligned}[t]
			&\bullet\ \text{Pivoting not needed}\\
			&\bullet\ \text{Well defined (always works)}\\
		\end{aligned}
		\hspace{2cm}
		\begin{aligned}[t]
			&\bullet\ \text{Only lower triangle needed for storage}\\
			&\bullet\ A = LDL^T \text{ is sometimes useful, where \(D\) is diag.}
		\end{aligned}
\end{aligned}\)}

% Symmetrix Indefinite Matrices
\vspace{10pt}
\underline{Symmetric Indefinite Matrices}\\[5pt]
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{Pivoting Needed}:\ \boxed{PAP^T = LDL^T}\\
	&\bullet\ \text{Ideally, \(D\) is diag., but if not possible, 
		then \(D\) is tridiag. (Aasen) or 1x1/2x2 block diag. (Bunch, Parlett, Kaufmann, etc.)}
\end{aligned}\)}

%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
\newpage
% Banded
\subsection{Banded Matrices}
\vspace{5pt}
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{Similar to normal Gaussian Elim., but less work since more zeroes}\\
	&\bullet\ \text{Pivoting means bandwidth will expand no more than double}\\
	&\bullet\ \text{Only \(\mathcal{O}(\beta n)\) storage needed} 
\end{aligned}\)}

%-----------------------------------------------------------------
% Rank 1 Update with Sherman-Morrison
\vspace{10pt}
\subsection{Rank-1 Update with Sherman-Morrison}
\vspace{5pt}
\(
	\begin{aligned}[t]
		\Tilde{A} \tilde{x} = b &= (A - uv^T) \tilde{x} \\[10pt]
		\ \rightarrow \ \tilde{x} &= \Tilde{A}^{-1}b 
	\end{aligned}
	\hspace{10pt}
	\rule[-62pt]{.5pt}{80pt}
	\hspace{10pt}
	\boxed{
	\setlength{\arraycolsep}{3pt}\begin{array}[t]{r c r c r c l}
		\Tilde{A}^{-1} 	&=& (A-uv^T)^{-1} 	&=& A^{-1} 		&+& \tfrac{A^{-1} u}{1 - v^T (A^{-1}u)}\ v^T A^{-1}\\[10pt]
		\Tilde{A}^{-1}b &=& \tilde{x} 				&=& (A^{-1} b) 	&+& \tfrac{A^{-1} u}{1 - v^T (A^{-1}u)}\ v^T (A^{-1} b)\\[10pt]
		& & & & 										x 			&+& \tfrac{y}{1 - v^T y}\ v^T x
	\end{array} }
\)

% General Woodbury Formula
\vspace{10pt}
\underline{General Woodbury Formula}: \ \ 
	\(\boxed{ (A-UV^T)^{-1} = A^{-1} + (A^{-1}U)(I - V^T A^{-1} U)^{-1}\ v^T A^{-1} }\) \\[10pt]
% Notes
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{\(U\) and \(V\) are general \(n \times k\) matrices}\\
	&\bullet\ \text{No guarantee of numerical stability, so caution is needed}
\end{aligned}\)}

%------------------------------------------------------------------
% Complexity
\vspace{10pt}
\subsection{Complexity}
\vspace{5pt}
\(\setlength{\arraycolsep}{3pt}\begin{array}{r c c c c c c l}
	\text{Explicit Inversion}: 
		&\begin{aligned}
				\scriptstyle LUA^{-1} &\scriptstyle\ =\ I\\[-5pt]
				\scriptstyle D^{-1}M^{-1} I &\scriptstyle\ =\ A^{-1}
			\end{aligned}
		&\rightarrow
		&\mathcal{O}(n^3)
		&\ ,\ 
		&A^{-1}b = x
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[15pt]
	\text{Gaussian Elimination}: 
		&A = LU
		&\rightarrow
		&\mathcal{O}(n^3/3)
		&\ ,\ 
		&LUx = b 
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[5pt]
	\text{Gaussian-Jordan}: 
		&A = MD
		&\rightarrow
		&\mathcal{O}(n^3/2)
		&\ ,\ 
		&MDx = b
		&\rightarrow
		&\mathcal{O}(n)
		\\[5pt]
	\text{Symmetric}:
		&\begin{aligned}
				\scriptstyle A &\scriptstyle\ =\ LL^T\\[-5pt]
				\scriptstyle PAP^T &\scriptstyle\ =\ LDL^T
			\end{aligned}
		&\rightarrow
		&\mathcal{O}(n^3/6)
		&\ ,\ 
		&LL^Tx = b
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[15pt]		
	\text{Banded}:
		&A_\beta = LU
		&\rightarrow
		&\mathcal{O}(\beta^2 n)
		&\ ,\ 
		&LUx = b
		&\rightarrow
		&\mathcal{O}(\beta n)
		\\[10pt]	
	\text{Sherman-Woodbury}:
		&\Tilde{A} = A-uv^T
		&\rightarrow
		&\mathcal{O}(n^2)
		&\ ,\ 
		&\tilde{x} = \Tilde{A}b
		&\rightarrow
		&\mathcal{O}(n^2)
\end{array}\)

%-----------------------------------------------------------------
\vspace{20pt}
% Diagonal Scaling
\begin{minipage}[t]{.45\textwidth}
	\subsection{Diagonal Scaling}
	% Ill conditioned
	\vspace{5pt}
	Ill-conditioned\\[5pt]
	\text{\scriptsize\(\begin{aligned}[t]
		\left(\begin{matrix}
			1 & 0 \\
			0 & \epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			x_1 \\ x_2
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			1 \\ \epsilon
		\end{matrix}\right)
	\end{aligned}\)}\\[15pt]	
	% Well conditioned
	Well-conditioned\\[5pt]
	\text{\scriptsize\(\begin{aligned}[t]
		\left(\begin{matrix}
			1 & 0 \\
			0 & 1/\epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			1 & 0 \\
			0 & \epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			x_1 \\ x_2
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			1 & 0 \\
			0 & 1/\epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			1 \\ \epsilon
		\end{matrix}\right)
	\end{aligned}\)}	
	
	% Notes
	\vspace{10pt}
	\(\begin{aligned}
		\text{\scriptsize \(\bullet\ \) No general way to correct poor scaling}
	\end{aligned}\)
\end{minipage}
%-----------------------------------------------------------------
% Iterative Refinement
\begin{minipage}[t]{.53\textwidth}
	\subsection{Iterative Refinement}
	\vspace{5pt}
	\text{\scriptsize\(\arraycolsep=3pt \begin{array}{r c l c l}
		r_0 &=& b - Ax_0 = A \Delta x_0\\
		r_1 &=& b - A(x_0 + \Delta x_0) &=& b - Ax_1 = A \Delta x_1\\
		r_2 &=& b - A(x_1 + \Delta x_1) &=& b - Ax_2 = A \Delta x_2\\[5pt]
		\cline{1-3}
		\multicolumn{1}{|r}{\rule[-7pt]{0pt}{20pt} x} &=& \multicolumn{1}{l|}{\displaystyle x_0\ +\ \lim_{n=0}^{\infty} \Delta x_n}
			& & \text{\scriptsize(terminate when \(r_n\) is small enough)}\\
		\cline{1-3}
	\end{array}\)}

	% Notes
	\vspace{10pt}
	\text{\scriptsize\(\begin{aligned}
		&\bullet\ \text{Double storage needed to hold original matrix}\\
		&\bullet\ \text{\(r_n\) usually must be computed with higher precision than \(x_n\)}\\
		&\bullet\ \text{Useful for badly scaled systmes, or making unstable systems stable}\\
		&\bullet\ \text{If \(x_n\) is not accurate, \(r_n\) might not need better accuracy}
	\end{aligned}\)}
\end{minipage}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Least r Linear Regression/Fit
\newpage
\section{Least \(\Vert r \Vert\) Linear Regression/Fit for \(Ax + r = b\)}

% Assumptions
\(\begin{aligned}
	&\bullet\ A = A_{m \times n} \hspace{10pt} {\scriptstyle(m > n)}\\[5pt]
	&\bullet\ \text{rank}(A) = n \hspace{10pt} \text{\scriptsize(full column rank)}\begin{aligned}[t]
			&\rightarrow\ x \text{ is unique (see proof???)}
		\end{aligned}
\end{aligned}\)

% Example: Vandermonde Matrix 
\vspace{15pt}
\underline{Example - Vandermonde Matrix, \(A\)}:\\[10pt]
\(
	Ax = \text{\scriptsize \(
		\left(\begin{matrix}
			- \vec{f}(t_1) -\\ 
			\vdots\\ 
			- \vec{f}(t_m) -
		\end{matrix}\right) 
		\left(\begin{matrix}
			|\\
			\vec{x}\\
			| 
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			y(t_1)\\ 
			\vdots\\ 
			y(t_m)
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			|\\ 
			\vec{y}\\ 
			|
		\end{matrix}\right) 
	\)} 
	= (x^T A^T)^T 
	\hspace{7pt} , \hspace{15pt}
	y(t) = \sum_{i=1}^n x_i f_i(t) = \vec{x} \cdot \vec{f}
\)

\vspace{15pt}
\(
	% Vector b decomp.
	\begin{aligned}
		&\text{\underline{Decompose \(b\)}}:\\[5pt]
		&{\arraycolsep=3pt \begin{array}{r c c c c}
				b &=& Ax &+& r \\[3pt]
				&=& y 	&+& r \\[3pt]
				&=& Pb 	&+& P_\perp b
			\end{array}}
	\end{aligned}
	\hspace{.5cm}
	\rule[-35pt]{.5pt}{75pt}
	\hspace{.5cm}
	% Orthogonal Projector Matrix
	\begin{aligned}
		&\underline{\text{Projector of A, \(P\)}}\\[10pt]
		&\begin{array}{r l}
			% Projector Def
			\text{Projector}: & \begin{gathered}[t]
					P^2 = P\\[-3pt]
					\text{\scriptsize(Idempotent)}
				\end{gathered}
				\ \rightarrow\ 
				\begin{gathered}[t]
					PA = A\\[-3pt]
					\text{\scriptsize(Projector of A)}
				\end{gathered}\\[15pt]
			% Ortho. Proj.
			\underline{\text{Orthogonal Projector}}: & P^T = P \ \rightarrow\ P_\perp A = (I - P)A = 0
		\end{array}
	\end{aligned}
\)

% Minimize residual, r
\vspace{15pt}
\underline{Minimize residual, \(r\)}:\\[10pt]
\(\begin{aligned}[t]
	\nabla \Vert r \Vert_2^2 &= 0 \hspace{20pt} 
		\text{\scriptsize\(\left(\tfrac{\partial r^2}{\partial x_i} = 0 \right)\)}\\[5pt]
	&= \nabla\left[ (b-Ax)^T(b-Ax) \right]\\[5pt]
	&= \nabla\left( b^T b - 2 x^T A^T b + x^T A^T Ax \right)\\[5pt]
	0 &= 2 A^T Ax - 2 A^T b\\
	&\downarrow\\
	A^TAx &= A^T b
\end{aligned}
\hspace{2cm}
\begin{aligned}[t]
	\Vert r \Vert_2^2 &= \Vert Pr + P_\perp r \Vert_2^2 = \Vert b - Ax \Vert^2\\
	&= \Vert Pr \Vert^2 + \Vert P_\perp r \Vert^2\\
	&= \cancel{ \Vert Pb - Ax \Vert_2^2 } + \Vert P_\perp b \Vert_2^2 \\
	&\downarrow\\
	Ax &= Pb\\
	A^T Ax &= A^T P b = (P^T A)^T b \\
	A^T Ax &= A^T b \hspace{10pt} \text{\scriptsize(System of Normal Equations)}
\end{aligned}\)

% Cross Product Matrix
\vspace{15pt}
\begin{minipage}[t]{.49\textwidth}
	\underline{Cross-Product Matrix of \(A\)}:\ \ \(\boxed{A^T A}\)\\[10pt]
	\(\begin{array}{r l}
		\text{Symmetric}: & (A^T A)^T = A^T A\\[10pt]
		\text{Pos. Def.}: & \begin{aligned}[t]
				&\text{rank}(A) = n\\
				&\rightarrow \ \begin{aligned}[t]
						&\big\langle x \big| A^T Ax \big\rangle = x^T A^T A x\\
						&= (Ax)^T (Ax) \\
						&= \Vert Ax \Vert^2 \geq 0
					\end{aligned} \\
			\end{aligned} \\[2.2cm]
		\text{Nonsingular}: & \begin{aligned}[t]
				&A^T A x = 0 \\
				&\rightarrow\ \Vert Ax \Vert^2 = 0 = Ax \\
				&\rightarrow\ (x = 0)
			\end{aligned} 
	\end{array}\)	
\end{minipage}
% System of Normal Equations
\hfill
\begin{minipage}[t]{.49\textwidth}
	\underline{System of Normal Equations}: \ \ \(\boxed{A^T Ax = A^T b}\)\\[20pt]
	\underline{Pseudoinverse, \(A^+\)}\\[10pt]
	\(
		\boxed{ \begin{aligned}
			x &= (A^T A)^{-1} A^T b \\[3pt]
			&\equiv A^+ b
		\end{aligned} }
		\ \ \ \rightarrow \ \ \
		\boxed{ \begin{aligned}
			A^+ &\equiv (A^T A)^{-1} A^T \\
			A^+ A &= I
		\end{aligned} }
	\)\\[20pt]
	\underline{Ortho. Proj., \(P\)}\\[10pt]
	\(
		\boxed{ \begin{aligned}[t]
			Ax &= A (A^T A)^{-1} A^T b \\
			&= Pb
		\end{aligned} }
		\ \ \ \rightarrow \ \ \
		\boxed{ \begin{aligned}[t]
			P &= A (A^T A)^{-1} A^T \\
			&= A A^+ 
		\end{aligned} }
	\)
\end{minipage}

%------------------------------------------------------------------------
% Residual, and Error Bounds when Solving Ax = b
\subsection{Error Bounds and Residuals}
% Error Bound
\vspace{10pt}\noindent
\underline{Error Bound}:\ \ \(\boxed{ \displaystyle \frac{\Vert \Delta x \Vert}{\Vert x \Vert} 
	\ \lessapprox\ \text{cond}(A)\ \epsilon_\text{mach} } 
	\ \rightarrow \ \begin{minipage}{8cm}
		\scriptsize
		A computed solution is expected to lose about \(\log_{10}(\text{cond}(A))\) digits, so
		the input data must be more accurate to these digits and 
		the working precision must carry more than these digits.
	\end{minipage}\)

\vspace{15pt}
\(\begin{aligned}
	&\bullet\ \Vert A \Vert = \max_{x \neq 0} \frac{\Vert Ax \Vert}{\Vert x \Vert} 
		= \max_{x \neq 0} \frac{\Vert AA^+ b \Vert}{\Vert A^+ b \Vert}\\
	&\bullet\ \text{cond}(A) = \begin{cases}
			\Vert A \Vert_2 \cdot \Vert A^+ \Vert_2 & \hspace{10pt} \text{rank}(A) = n\\
			\infty & \hspace{10pt} \text{rank}(A) < n
		\end{cases}\\
	&\bullet\ \text{info can be lost}\\
	&\bullet\ \text{cond}(A^T A) = [\text{cond}(A)]^2 
\end{aligned}\)

% Error Bound for Change in Vector b
\vspace{15pt}\noindent
\(\begin{aligned}[t]
	&A^T A(x + \Delta x) = A^T A(b + \Delta b)\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{array}{l c c c}
			\bullet\ &\Vert \Delta x \Vert 	&\leq& \Vert A^+ \Vert \cdot \Vert \Delta b \Vert
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \begin{aligned}[t]
			\tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
				&\leq \left( \text{cond}(A) \tfrac{\Vert b \Vert}{\Vert Ax \Vert} \right) 
				\tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}\\[5pt]
			&= \left( \text{cond}(A) \tfrac{1}{\cos \theta} \right) 
				\tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}
		\end{aligned} }
\end{aligned}
% Residual
\hfill
\begin{aligned}[t]
	&(A + \Delta A)^T (A + \Delta A)(x + \Delta x) = (A + \Delta A)^T b\\[10pt]
	&\hspace{10pt} \begin{aligned}
			&\bullet\ \text{\scriptsize\(\begin{aligned}[t]
					&\cancel{A^T A x} + A^T \Delta A x + (\Delta A)^T A x + \bcancel{ (\Delta A)^T \Delta A x }\\
					&\ \ + A^T A \Delta x 
						+ \bcancel{ A^T \Delta A \Delta x} 
						+ \bcancel{ (\Delta A)^T A \Delta x } 
						+ \bcancel{ (\Delta A)^T \Delta A \Delta x }
				\end{aligned}\ \ =\ \ \cancel{A^T b} + (\Delta A)^T b \)}\\[10pt]
			&\bullet\ \begin{aligned}[t]
					\Vert \Delta x \Vert 
						&= \Vert (A^T A)^{-1}(\Delta A)^T r - A^+ \Delta A x \Vert\\[5pt]
					&\leq \Vert (A^T A)^{-1} \Vert \cdot \Vert \Delta A \Vert \cdot \Vert r \Vert 
						+ \Vert A^+ \Vert \cdot \Vert \Delta A \Vert \cdot \Vert x \Vert
				\end{aligned}
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \begin{aligned}[t]
			\tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
				&\leq \left( \scriptstyle [\text{cond}(A)]^2 \tfrac{\Vert r \Vert}{\Vert Ax \Vert} 
				+ \text{cond}(A) \right) \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}\\[5pt]
			&= \big( \scriptstyle [\text{cond}(A)]^2 \tan \theta
				+ \text{cond}(A) \big) \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
		\end{aligned} }
\end{aligned}\)

\vspace{20pt}
\(\begin{aligned}
	&\bullet\ \text{Augemented Systems - skipped}\\
	&\bullet\ \Big\{ Q = Q_{m \times n}\ \Big|\ \text{span}(Q) = \text{span}(A)\ , \ \ Q^T Q = I \Big\} 
		\ \rightarrow\ P = Q Q^T \hspace{10pt} \text{\scriptsize(orthonormal projector?)}\\
	&\bullet\ Q^+ = (Q^T Q)^{-1} Q^T = Q^T\\
	&\bullet\ \begin{aligned}[t]
			Q^T Ax &= Q^T Pb = Q^T Q Q^T b \\
			Q^T Ax &= Q^T b \hspace{10pt} \text{\scriptsize(System of Orthonormal Equations?)}
		\end{aligned}\\[20pt]
	&\bullet\ \text{2-norm Preserved}: \ \begin{aligned}[t]
			\Vert Qv \Vert^2 = \langle Qv | Qv \rangle = \langle v | Q^TQv \rangle = \Vert v \Vert^2
		\end{aligned}\\	
	&\bullet\ Rx = c_1 \ \rightarrow\
		Q^T b = 
		\left(\begin{matrix}
			R \\ 
			0
		\end{matrix}\right) 
		\vec{x} + 
		\left(\begin{matrix}
			0\\
			\vec{c}_2
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			\vec{c}_1\\
			\vec{c}_2
		\end{matrix}\right) 
		\hspace{20pt} \text{\scriptsize(\(R\) is upp. triang.)}\\
	&\bullet\ \text{Reduced QR Factorization}:\ A = Q \left(\begin{matrix}
			R\\ 0
		\end{matrix}\right)
		= 
		\left(\begin{matrix}
			Q_1 & Q_2
		\end{matrix}\right)
		\left(\begin{matrix}
			R\\ 0
		\end{matrix}\right)
		= Q_1 R
\end{aligned}\)

\newpage
\(\begin{aligned}
	&\bullet\ \text{Householder Transformation/Elementary Reflector}:\ 
		H = I - (2\hat{v}) \hat{v}^T = I - \frac{(2v)v^T}{v^Tv}\\
	&\bullet\ H = H^T = H^{-1} \hspace{10pt} \text{\scriptsize(symmetric and orthogonal)}\\
	&\bullet\ \alpha e_1 = Ha = a - (2v)\frac{v \cdot a}{v \cdot v} 
		\ \rightarrow\ v = (a - \alpha e_1) \frac{v \cdot v}{2v \cdot a}
		\ \rightarrow\ v = (a - \alpha e_1)\\
	&\bullet\ \alpha = \pm \Vert a \Vert \ \Rightarrow\ \text{Choose } \alpha = - \text{sign}(a_1) \Vert a \Vert\\
	&\bullet\ \text{Just use \(v\) to transform \(a\) - don't find \(H\)}\\
	&\bullet\ \langle H_i \dots H_1r | e_j \rangle = 0 \hspace{10pt} \text{\scriptsize\((1 \leq j \leq i)\)}\\
	&\bullet\ \text{\(\begin{aligned}[t]
			Q^TAx + Q^Tr &= Q^Tb\\
			\left(\begin{matrix}
					\\[-12pt]
					& Q_1^T &
					\\[2pt]
					\hline
					\\[-10pt]
					& Q_2^T &
					\\[1pt]
				\end{matrix}\right)
				\left.\left(\begin{matrix} 
					\\[-5pt]
					Q_1\\
					\\[-3pt]
				\end{matrix}\right.\right| \left.\begin{matrix}
					\\[-5pt]
					Q_2\\
					\\[-3pt]
				\end{matrix} \right) 
				\left( \begin{matrix}
					R\\
					0
				\end{matrix}\right)
				x
				+ 
				\left(\begin{matrix}
					\\[-12pt]
					& Q_1^T &
					\\[2pt]
					\hline
					\\[-10pt]
					& Q_2^T &
					\\[1pt]
				\end{matrix}\right) r
				&= 
				\left(\begin{matrix}
					\\[-12pt]
					& Q_1^T &
					\\[2pt]
					\hline
					\\[-10pt]
					& Q_2^T &
					\\[1pt]
				\end{matrix}\right) b\\
			\left( \begin{matrix}
					Rx\\
					0
				\end{matrix}\right)
				+ \left( \begin{matrix}
					0\\
					r'
				\end{matrix}\right)	
				&= \left( \begin{matrix}
					Q_1^T b\\
					Q_2^T b
				\end{matrix}\right)	
		\end{aligned}\)}\\
	&\bullet\ v_i = \text{\scriptsize\(\left(\begin{matrix}
			0\\
			\vdots\\
			a_i\\
			\vdots\\
			a_m
		\end{matrix}\right)\)} - \alpha e_i 
		= \left(\begin{matrix}
			0&\dots&0&(a_i - \alpha)&a_{i+1}&\dots&a_m
		\end{matrix}\right)^T
\end{aligned}\)

\(\begin{aligned}
	&\bullet\ \text{Givens Rotation}:\ 
		\left(\begin{matrix}
			c & s\\
			-s& c
		\end{matrix}\right) 
		\ \rightarrow \ Gx = 
		G \left(\begin{matrix} 
			x_1\\
			x_2 
		\end{matrix}\right) 
		= \pm 
		\left(\begin{matrix} 
			\Vert x \Vert \\
			0 
		\end{matrix}\right)\\
	&\bullet\ c = \frac{a_1}{\sqrt{a_1^2 + a_2^2}}\ , \ \ s = \frac{a_2}{\sqrt{a_1^2 + a_2^2}}\\
	&\bullet\ \text{Avoid squaring any number \(\gg 1\) to prevent overflow/underflow}\\
	&\bullet\ t = \tfrac{a_2}{a_1} < 1 \ \rightarrow \ c = \frac{1}{\sqrt{1+t^2}}\ , \ \ s = c \cdot t\\
	&\bullet\ \tau = \tfrac{a_1}{a_2} < 1 \ \rightarrow \ s = \frac{1}{\sqrt{1+\tau^2}}\ , \ \ c = s \cdot \tau\\
\end{aligned}\)

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Matrix Types
\newpage
\section{Matrix Types}

% Hermitian
\bigskip\bigskip
Hermitian:
\boldmath $$H = H^\dagger$$ \unboldmath

% Unitary
Unitary:
\boldmath $$UU^\dagger = I$$ \unboldmath

% Factor Hermitian Into Unitary
\par \bigskip \bigskip
$H = UDU^{-1}$
\begin{itemize}
	\item D is real
\end{itemize}

% Unitary From Hermitian
\bigskip
$U = e^{iH}$
\begin{itemize}
	\item $U = e^{iH} = U_{H} e^{iD} (U_{H})^{-1}$
\end{itemize}

\end{document}