\documentclass[12pt]{article}
\usepackage[left=.75in, right=.75in, top=1in, bottom = 1in]{geometry}
\usepackage{enumitem, amssymb, amsmath, amsfonts, mathtools, parskip}
\usepackage{cancel}

\newcommand{\rdiag}{\raisebox{0.5ex}{\scalebox{0.7}{$\diagdown$}}}

% \newcommand\italicmath{\mathversion{italic}}
% \DeclareMathVersion{italic}
% \SetSymbolFont{operators}{italic}{OT1}{cmr} {m}{it}
% \SetSymbolFont{letters}  {italic}{OML}{cmm} {m}{it}
% \SetSymbolFont{symbols}  {italic}{OMS}{cmsy}{m}{n}
% \SetMathAlphabet\mathsf{italic}{OT1}{cmss}{m}{sl}
% \SetMathAlphabet\mathit{italic}{OT1}{cmr}{m}{it}

% \newcommand\bitalicmath{\mathversion{bitalic}}
% \DeclareMathVersion{bitalic}
% \SetSymbolFont{operators}{bitalic}{OT1}{cmr} {bx}{it}
% \SetSymbolFont{letters}  {bitalic}{OML}{cmm} {b}{it}
% \SetSymbolFont{symbols}  {bitalic}{OMS}{cmsy}{b}{n}
% \SetMathAlphabet\mathsf{bitalic}{OT1}{cmss}{bx}{sl}
% \SetMathAlphabet\mathit{bitalic}{OT1}{cmr}{bx}{it}

\begin{document}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
\section{Solving System of Linear Equations \(Ax=b\)}

% P Norm and Condition Number
\subsection{\(p\)-Norm and Condition Number}
\vspace{10pt}\noindent
% Vectors
\begin{minipage}[t]{.49\textwidth}
	% Vector P Norm
	\underline{Vector \(p\)-Norm}: \ \ 
	\fbox{\(\Vert \vec{x} \Vert_p \ =\ \sqrt[\leftroot{-1}\uproot{5}{ p }]{\ \displaystyle \sum_i |x_i|^p\ }\)}

	% 1 and infty norms
	\vspace{10pt}\hspace{10pt}\(\begin{array}{r l}
		{\text{\(1\)-Norm}}: &
			\Vert \vec{x} \Vert_1 \ =\ \displaystyle \sum_i |x_i|\\[20pt]
		{\text{\(\infty\)-Norm}}: &
			\Vert \vec{x} \Vert_\infty \ =\ \max |x_i|
	\end{array}\)

	% Vector Norm Extra Info
	\vspace{20pt}
	\(\setlength{\arraycolsep}{4pt}\begin{array}{l r c c c c}
		\bullet & \Vert x \Vert_1 &\geq&  \Vert x \Vert_2 			&\geq& \Vert x \Vert_\infty\\[15pt]
		\bullet & \Vert x \Vert_1 &\leq&  \sqrt{n}\ \Vert x \Vert_2 	&\leq& \sqrt{n}\ \Vert x \Vert_\infty
	\end{array}\)
\end{minipage}
\begin{minipage}[t]{.49\textwidth}
	\setlength{\parindent}{.5cm}
	% Func/Vector Condition Number
	\noindent
	\underline{Function/Vector Condition Number}:\\[15pt]
	\indent\(\begin{aligned}
		\text{cond}\Big( f(x) \Big) 
		&= \left\vert \frac{[f(\hat{x}) - f(x)] / f(x)}{[\hat{x} - x] / x} \right\vert\\[5pt]
		&= \left\vert \frac{\Delta y / y}{\Delta x / x}\right\vert 
			= \left\vert \frac{y' \cdot \Delta x / y}{\Delta x / x}\right\vert\\[5pt]
		&= \left\vert \frac{x f'(x)}{f(x)}\right\vert
	\end{aligned}\)
\end{minipage}

% Matrices
\vspace{20pt}\noindent
\begin{minipage}[t]{.49\textwidth}
	% Matrix P Norm
	\underline{Matrix \(p\)-Norm}: \ \ 
	\fbox{\(\displaystyle \Vert A \Vert_p \ =\ \max_{x \neq 0} \frac{\Vert Ax \Vert}{\Vert x \Vert}\)}

	% 1 and infty norms
	\vspace{10pt}\hspace{10pt}\(\begin{array}{r l}
		{\text{\(1\)-Norm}}: &
			\displaystyle \Vert A \Vert_1 \ =\ \max_{j}\ \sum_i \vert a_{ij} \vert\\[20pt]
		{\text{\(\infty\)-Norm}}: &
			\displaystyle \Vert A \Vert_\infty \ =\ \max_{i}\ \sum_j \vert a_{ij} \vert
	\end{array}\)

	% Matrix Norm Extra Info
	\vspace{10pt}
	\(\left.\setlength{\arraycolsep}{3pt}\begin{array}{l r c l}
		\bullet& \Vert AB \Vert &\leq& \Vert A \Vert \cdot \Vert B \Vert\\[15pt]
		\bullet& \Vert Ax \Vert &\leq& \Vert A \Vert \cdot \Vert x \Vert
	\end{array}\right\} \begin{gathered}
		\scriptstyle \text{For \(p\)-norms (not} \\[-5pt]
		\scriptstyle \text{necessarily in general)}
	\end{gathered}\)
\end{minipage}
\begin{minipage}[t]{.49\textwidth}
	\setlength{\parindent}{.5cm}
	% Matrix Condition Number
	\noindent
	\underline{Matrix Condition Number}:\\[15pt]
	\indent\(\begin{aligned}
		&\boxed{ \text{cond}_p(A) = \Vert A \Vert_p \cdot \Vert A^{-1} \Vert_p }
					\hspace{20pt} {\scriptstyle(\text{\(\infty\) if singular})}\\[5pt]
		&\hspace{20pt}= \frac{{\displaystyle \max_{x \neq 0}}\ \Vert Ax \Vert_p / \Vert x \Vert_p}
			{{\displaystyle \min_{x \neq 0}}\ \Vert Ax \Vert_p / \Vert x \Vert_p}
			\ =\ \text{cond}_p(\gamma A)
			\ \geq\ 1\\[10pt]
		&\bullet\ \text{Diagonal, } D: \ \text{cond}(D) = \tfrac{\max \vert d_i \vert}{\min \vert d_i \vert}\\[3pt]
		&\bullet\ \begin{aligned}[t]
			&\Vert z \Vert = \Vert A^{-1} y \Vert \ \leq\ \Vert A^{-1} \Vert \cdot \Vert y \Vert\\[3pt]
			&\rightarrow \tfrac{\Vert z \Vert}{\Vert y \Vert} 
				\ \leq\ \max \tfrac{\Vert z \Vert}{\Vert y \Vert} 
				\stackrel{?}{=} \Vert A^{-1} \Vert \hspace{15pt} \text{\scriptsize(optimize)}
		\end{aligned}
	\end{aligned}\)
\end{minipage}

%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
% Residual, and Error Bounds when Solving Ax = b
\newpage
\subsection{Error Bounds and Residuals}
% Error Bound
\vspace{10pt}\noindent
\underline{Error Bound}:\ \ \(\boxed{ \displaystyle \frac{\Vert \hat{x} - x \Vert}{\Vert x \Vert} 
	\ \lessapprox\ \text{cond}(A)\ \epsilon_\text{mach} } 
	\ \rightarrow \ \begin{minipage}{8cm}
		\scriptsize
		A computed solution is expected to lose about \(\log_{10}(\text{cond}(A))\) digits, so
		the input data must be more accurate to these digits and 
		the working precision must carry more than these digits.
	\end{minipage}\)

% Error Bound for Change in Vector b
\vspace{15pt}\noindent
\(\begin{aligned}[t]
	&A\hat{x} = b + \Delta b = Ax + A\Delta x\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{array}{l c c c}
			\bullet&\Vert b \Vert 			&\leq& \Vert A \Vert \cdot \Vert x \Vert\\[10pt]
			\bullet\ &\Vert \Delta x \Vert 	&\leq& \Vert A^{-1} \Vert \cdot \Vert \Delta b \Vert
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}}
\end{aligned}
\hspace{2cm}
% Residual
\begin{aligned}[t]
	&A\hat{x} + r = b\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{aligned}
			\bullet\ \Vert \Delta x \Vert 
				&= \Vert A^{-1}(A\hat{x} - b) \Vert
				= \Vert -A^{-1} r \Vert \\[5pt]
			&\leq \Vert A^{-1} \Vert \cdot \Vert r \Vert
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert r \Vert}{\Vert A \Vert \cdot \Vert \hat{x} \Vert}}
\end{aligned}\)

% Error Bound for Change in Matrix A
\vspace{20pt}\noindent
\(\begin{aligned}[t]
	&(A + \Delta A)\hat{x} = b\\[5pt]
	&\hspace{10pt} \begin{aligned}[t]
			\bullet\ \Vert \Delta x \Vert &= \Vert - A^{-1} (\Delta A) \hat{x} \Vert\\[10pt]
			&\leq \Vert A^{-1} \Vert \cdot \Vert \Delta A \Vert \cdot \Vert \hat{x} \Vert
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}}
\end{aligned}
\hspace{1.5cm}
% Residual
\begin{aligned}[t]
	&(A + \Delta A)\hat{x} = b\\[5pt]
	&\hspace{10pt} \begin{aligned}
		\bullet\ \Vert r \Vert 
			&= \Vert b - A \hat{x} \Vert
			= \Vert \Delta A \cdot \hat{x} \Vert \\[5pt]
		&\leq \Vert \Delta A \Vert \cdot \Vert \hat{x} \Vert
	\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert r \Vert}{\Vert A \Vert \cdot \Vert \hat{x} \Vert} 
		\leq \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}}
		\ , \ \ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \tfrac{\Vert A^{-1} \Vert \cdot \Vert r \Vert}{\Vert \hat{x} \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
\end{aligned}\)

% Calculus Error Bounds
\vspace{20pt}\noindent
\(\begin{aligned}[t]
	&\Big[ A(t) x(t)\ =\ b(t) \Big] 
		= \Big[ \big(A_0 + \Delta A \cdot t \big) x(t)\ =\ b_0 + \Delta b \cdot t \Big]\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{2pt}\begin{array}{l r c l}
			\bullet&x'(t) &=& \frac{b'(t) - A'(t)x(t)}{A(t)} = A^{-1}(t) \Big[ \Delta b - \Delta A\cdot x(t) \Big]\\[10pt]
			\bullet&x(t) &=& x_0 + x'(0)t + \mathcal{O}(t^2)
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert x(t) - x_0 \Vert}{\Vert x_0 \Vert} 
		\leq \text{cond}(A) \left( \tfrac{\Vert \Delta b \Vert}{\Vert b \Vert} 
		+ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert} \right) \vert t \vert 
		+ \mathcal{O}(t^2)}
\end{aligned}\)

%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------
% LU Decomposition to Solve Linear System of Eq
\newpage
\subsection{Gaussian Elimination with LU/PLU/PLDUQ Decomposition}
% Elementary Elimination Matrices
\vspace{10pt}\noindent
\underline{Elementary Elimination Matrices, \(L_k\)}\\[10pt]
\(\begin{aligned}[t]
	&\\[-20pt]
	&\text{\scriptsize\(\left(\begin{matrix}
		1 & \dots & 0 & 0 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & 1 & 0 & \dots & 0\\
		0 & \dots & \tfrac{-a_{k+1}}{a_k} & 1 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & \tfrac{-a_n}{a_k} & 0 & \dots & 1\\
	\end{matrix}\right) 
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_k\\
		a_{k+1}\\
		\vdots\\
		a_n
	\end{matrix}\right) 
	=
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_k\\
		0\\
		\vdots\\
		0
	\end{matrix}\right)\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\bullet\ a_k\ \text{is the ``pivot''}\\
	&\bullet\ \text{is lower triangular}\\
	&\bullet\ \scriptstyle \forall i \neq j\ \ (L_k^{-1})_{ij}\ =\ - (L_k)_{ij} 
\end{aligned}
\hspace{30pt}
\begin{aligned}[t]
	&\text{Ex}:\\
	&\text{\scriptsize\(\left(\begin{matrix}
			1 & 0 & \dots\\
			-a_1/a_2 & 1 & \dots\\
			\vdots & \vdots & \ddots
		\end{matrix}\right) 
		\left(\begin{matrix}
			a_1\\
			a_2\\
			\vdots
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			a_1\\ 
			0\\
			\vdots
		\end{matrix}\right)\)}\\
\end{aligned}
\) 

% LU/PLU Factorization
\vspace{20pt}\noindent
\underline{LU/PLU Factorization (w/ partial pivoting)}

\vspace{5pt}
\(\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= LU \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(L\) is gen. triang.)}\\[-7pt]
					&\text{\scriptsize(\(U\) is upp. triang.)}
				\end{aligned}\\[5pt]
			L &= (\dots L_2 P_2 L_1 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\} b &= (\dots L_2 P_2 L_1 P_1) Ax \\[5pt]
			L^{-1}b &= (P_1^T L_1^{-1} P_2^T L_2^{-1} \dots)^{-1} Ax \\[5pt]
			&= L^{-1} (L U)x = y
		\end{aligned}\\[5pt]
	&\boxed{\begin{gathered}
			b = Ly\\[-5pt]
			\text{\scriptsize(forw.-sub.)}
		\end{gathered} 
		\ ,\ \ 
		\begin{gathered}
			y = Ux\\[-5pt]
			\text{\scriptsize(back.-sub.)}
		\end{gathered}}
\end{aligned}
\hspace{25pt}
\begin{aligned}[t]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize Permutation matrix, \\ 
			\(P_i\), rowswaps s.t. \\
			\(a_k \neq 0\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3.5cm}
			\scriptsize \(P_i\) rowswaps s.t. \(a_k\) is \\
			largest s.t. \(a_{k+i}/a_k \leq 1\)\\
			for numerical stability/\\
			minimize errors
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3.5cm}
		\scriptsize Pivoting isn't needed if \\
		\(A\) is diag. dom.\\
		\((a_{jj} > \sum_{i, i \neq j} a_{ij})\)
	\end{minipage}\\[5pt]
	&\bullet\ \text{\scriptsize \(A\) can be singular}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= PLU \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(P\) is rowswap permu.)}\\[-7pt]
					&\text{\scriptsize(\(L\) is unit low. triang.)}\\[-7pt]
					&\text{\scriptsize(\(U\) is upp. triang.)}
				\end{aligned}\\[5pt]
			P &= (\dots P_2 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\}b &= (\dots P_2 P_1)Ax\\[5pt]
			P^T b &= (P_1^T P_2^T \dots)^{-1} Ax\\[5pt]
			&= P^T (PLU)x = Ly
		\end{aligned}\\[10pt]
	&\boxed{P^T b = Ly\ ,\ \ y = Ux}
\end{aligned}\)

% LDU Diagonal Uniquness
\vspace{20pt}
\(\begin{aligned}[t]
	&\boxed{P^T A = LDU \hspace{20pt} \text{\scriptsize(D is diag.)}}\\[10pt]
	&\bullet\ \scriptstyle LDU\ \text{is unique up to } D\\
	&\bullet\ \scriptstyle LDU\ \text{is unique if } L/U \text{ are unit low./upp. diag., resp.}
\end{aligned}
% PAQ Full pivoting
\hspace{2cm}
\begin{aligned}[t]
	&\boxed{P^T A Q^T = LDU \hspace{20pt} \begin{aligned}
			&\text{\scriptsize(P is permu. for rows)}\\[-7pt]
			&\text{\scriptsize(Q is permu. for cols.)}
		\end{aligned}}\\[5pt]
	&\bullet\ \text{\scriptsize ``Complete pivoting'' search for largest \(a_k\)}\\[-3pt]
	&\bullet\ \text{\scriptsize Would be most numerically stable}\\[-3pt]
	&\bullet\ \text{\scriptsize Expensive, so not really used}
\end{aligned}\)

% Erro Bounds
\vspace{20pt}\noindent
\underline{Error Bound}:\ \(\tfrac{\Vert r \Vert}{\Vert A \Vert \Vert x \Vert} 
	\ \leq\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
	\ \leq\ \begin{gathered}[t]
		\rho\ n^2 \epsilon_\text{mach}\\[-5pt]
		\text{\scriptsize(Wilkinson)}
	\end{gathered}
	\ \sim\ \begin{gathered}[t]
		n \epsilon_\text{mach}\\[-5pt]
		\text{\scriptsize (usually)}
	\end{gathered}
	\hspace{40pt} \begin{minipage}{6cm}
		\scriptsize
		(growth factor, \(\rho\), is the largest entry at any point during factorization - usually at \(U\) - \\ 
		divided by the largest entry of A)
	\end{minipage}\)

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
% Gaussian-Jordan
\newpage
\subsection{Gaussian-Jordan with MD Decomposition}
% Elementary Elimination Matrices
\vspace{10pt}\noindent
\underline{Elementary Elimination Matrices, \(M_k\)}\\[10pt]
\(\begin{aligned}[t]
	&\\[-20pt]
	&\text{\scriptsize\(\left(\begin{matrix}
		1 & \dots & \tfrac{-a_1}{a_k} & 0 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & 1 & 0 & \dots & 0\\
		0 & \dots & \tfrac{-a_{k+1}}{a_k} & 1 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & \tfrac{-a_n}{a_k} & 0 & \dots & 1\\
	\end{matrix}\right) 
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_{k-1}\\
		a_k\\
		a_{k+1}\\
		\vdots\\
		a_n
	\end{matrix}\right) 
	=
	\left(\begin{matrix}
		0\\
		\vdots\\
		0\\
		a_k\\
		0\\
		\vdots\\
		0
	\end{matrix}\right)\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\bullet\ a_k\ \text{is the ``pivot''}\\
	&\bullet\ \scriptstyle \forall i \neq j\ \ (M_k^{-1})_{ij}\ =\ - (M_k)_{ij} 
\end{aligned}
\) 

% MD Factorization
\vspace{20pt}\noindent
\underline{MD Factorization (w/ partial pivoting)}

\vspace{5pt}
\(\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= MD \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(M\) is elem. elim.)}\\[-7pt]
					&\text{\scriptsize(\(D\) is diag.)}
				\end{aligned}\\[5pt]
			M &= (\dots M_2 P_2 M_1 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\} b &= (\dots M_2 P_2 M_1 P_1) Ax \\[5pt]
			M^{-1}b &= (P_1^T M_1^{-1} P_2^T M_2^{-1} \dots)^{-1} Ax \\[5pt]
			&= M^{-1} (MD)x = y
		\end{aligned}\\[5pt]
	&\boxed{\begin{gathered}
			M^{-1}b=y
		\end{gathered} 
		\ ,\ \ 
		\begin{gathered}[t]
			y = Dx\\[-5pt]
			\text{\scriptsize(division)}
		\end{gathered}}
\end{aligned}
\hspace{25pt}
\begin{aligned}[t]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize Permutation matrix, \\ 
			\(P_i\), rowswaps s.t. \\
			\(a_k \neq 0\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize \(P_i\) rowswaps cannot \\
			ensure numerical \\
			stability \((\leq 1)\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize 
			Division is \(\mathcal{O}(n)\), \\
			so may be useful for\\
			parallel comps.
		\end{minipage}\\[5pt]
	&\bullet\ \text{\scriptsize Can also find \(A^{-1}\)}
\end{aligned}
\hspace{30pt}
\text{\scriptsize\(
	\begin{aligned}[t]
		&\underline{\text{Finding } A^{-1}}\\[5pt]
		&D^{-1} M^{-1} (A | I) = (I | A^{-1})\\[5pt]
		&\hspace{10pt} \begin{aligned}[t]
			&= D^{-1} M^{-1} 
				\left[\left.\begin{matrix}
					a_{11} & \dots \\
					\vdots & a_{nn}	
				\end{matrix}\right|
				\begin{matrix}
					1 & 0\\
					0 & 1
				\end{matrix}\right]\\[5pt]
			&= \left[\begin{matrix}
					1 & 0\\
					0 & 1
				\end{matrix}
				\left|\begin{matrix}
					a'_{11} & \dots \\
					\vdots & a'_{nn}	
				\end{matrix}\right.\right]
		\end{aligned}
	\end{aligned}\)}
\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Symmetric
\vspace{15pt}
\subsection{Symmetric Matrices}
% Positive Definite Def
\vspace{5pt}
\underline{Positive Definite}:\ \ \(\boxed{x^T Ax \geq 0}\) \\[15pt]
% Cholesky Factorization
\underline{Cholesky Factorization for Sym., Pos. Def.}:\ \ \(\boxed{A = LL^T = LDL^T}\)\\[15pt]
\text{\scriptsize\(\begin{aligned}
	&\left(\begin{matrix}
			a_{11} & a_{21} & a_{31} & \dots\\
			a_{21} & a_{22} & a_{32} & \dots\\
			a_{31} & a_{32} & a_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			l_{11} & 0 		& 0 	 & \dots\\
			l_{21} & l_{22} & 0 	 & \dots\\
			l_{31} & l_{32} & l_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)
		\left(\begin{matrix}
			l_{11} 	& l_{21} 	& l_{31} & \dots\\
			0 		& l_{22} 	& l_{32} & \dots\\
			0 		& 0 		& l_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)  
		= 
		\left(\begin{matrix}
			l_{11}^2 		& \dots 						& \dots 							& \dots\\
			l_{21} l_{11} 	& l_{21}^2 + l_{22}^2 			& \dots 							& \dots\\
			l_{31} l_{11} 	& l_{31}l_{21} + l_{32}l_{22} 	& l_{31}^2 + l_{32}^2 + l_{33}^2 	& \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)\\[5pt]
	% Notes
	&\begin{aligned}[t]
			&\bullet\ \text{Pivoting not needed}\\
			&\bullet\ \text{Well defined (always works)}\\
		\end{aligned}
		\hspace{2cm}
		\begin{aligned}[t]
			&\bullet\ \text{Only lower triangle needed for storage}\\
			&\bullet\ A = LDL^T \text{ is sometimes useful, where \(D\) is diag.}
		\end{aligned}
\end{aligned}\)}

% Symmetrix Indefinite Matrices
\vspace{10pt}
\underline{Symmetric Indefinite Matrices}\\[5pt]
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{Pivoting Needed}:\ \boxed{PAP^T = LDL^T}\\
	&\bullet\ \text{Ideally, \(D\) is diag., but if not possible, 
		then \(D\) is tridiag. (Aasen) or 1x1/2x2 block diag. (Bunch, Parlett, Kaufmann, etc.)}
\end{aligned}\)}

%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
\newpage
% Banded
\subsection{Banded Matrices}
\vspace{5pt}
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{Similar to normal Gaussian Elim., but less work since more zeroes}\\
	&\bullet\ \text{Pivoting means bandwidth will expand no more than double}\\
	&\bullet\ \text{Only \(\mathcal{O}(\beta n)\) storage needed} 
\end{aligned}\)}

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Rank 1 Update with Sherman-Morrison
\vspace{10pt}
\subsection{Rank-1 Update with Sherman-Morrison}
\vspace{5pt}
\(
	\begin{aligned}[t]
		\Tilde{A} \tilde{x} = b &= (A - uv^T) \tilde{x} \\[10pt]
		\ \rightarrow \ \tilde{x} &= \Tilde{A}^{-1}b 
	\end{aligned}
	\hspace{10pt}
	\rule[-62pt]{.5pt}{80pt}
	\hspace{10pt}
	\boxed{
	\setlength{\arraycolsep}{3pt}\begin{array}[t]{r c r c r c l}
		\Tilde{A}^{-1} 	&=& (A-uv^T)^{-1} 	&=& A^{-1} 		&+& \tfrac{A^{-1} u}{1 - v^T (A^{-1}u)}\ v^T A^{-1}\\[10pt]
		\Tilde{A}^{-1}b &=& \tilde{x} 				&=& (A^{-1} b) 	&+& \tfrac{A^{-1} u}{1 - v^T (A^{-1}u)}\ v^T (A^{-1} b)\\[10pt]
		& & & & 										x 			&+& \tfrac{y}{1 - v^T y}\ v^T x
	\end{array} }
\)

% General Woodbury Formula
\vspace{10pt}
\underline{General Woodbury Formula}: \ \ 
	\(\boxed{ (A-UV^T)^{-1} = A^{-1} + (A^{-1}U)(I - V^T A^{-1} U)^{-1}\ v^T A^{-1} }\) \\[10pt]
% Notes
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{\(U\) and \(V\) are general \(n \times k\) matrices}\\
	&\bullet\ \text{No guarantee of numerical stability, so caution is needed}
\end{aligned}\)}

%------------------------------------------------------------------
%------------------------------------------------------------------
% Complexity
\vspace{10pt}
\subsection{Complexity}
\vspace{5pt}
\(\setlength{\arraycolsep}{3pt}\begin{array}{r c c c c c c l}
	\text{Explicit Inversion}: 
		&\begin{aligned}
				\scriptstyle LUA^{-1} &\scriptstyle\ =\ I\\[-5pt]
				\scriptstyle D^{-1}M^{-1} I &\scriptstyle\ =\ A^{-1}
			\end{aligned}
		&\rightarrow
		&\mathcal{O}(n^3)
		&\ ,\ 
		&A^{-1}b = x
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[15pt]
	\text{Gaussian Elimination}: 
		&A = LU
		&\rightarrow
		&\mathcal{O}(n^3/3)
		&\ ,\ 
		&LUx = b 
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[5pt]
	\text{Gaussian-Jordan}: 
		&A = MD
		&\rightarrow
		&\mathcal{O}(n^3/2)
		&\ ,\ 
		&MDx = b
		&\rightarrow
		&\mathcal{O}(n)
		\\[5pt]
	\text{Symmetric}:
		&\begin{aligned}
				\scriptstyle A &\scriptstyle\ =\ LL^T\\[-5pt]
				\scriptstyle PAP^T &\scriptstyle\ =\ LDL^T
			\end{aligned}
		&\rightarrow
		&\mathcal{O}(n^3/6)
		&\ ,\ 
		&LL^Tx = b
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[15pt]		
	\text{Banded}:
		&A_\beta = LU
		&\rightarrow
		&\mathcal{O}(\beta^2 n)
		&\ ,\ 
		&LUx = b
		&\rightarrow
		&\mathcal{O}(\beta n)
		\\[10pt]	
	\text{Sherman-Woodbury}:
		&\Tilde{A} = A-uv^T
		&\rightarrow
		&\mathcal{O}(n^2)
		&\ ,\ 
		&\tilde{x} = \Tilde{A}b
		&\rightarrow
		&\mathcal{O}(n^2)
\end{array}\)

%------------------------------------------------------------------
%-----------------------------------------------------------------
\vspace{20pt}
% Diagonal Scaling
\begin{minipage}[t]{.45\textwidth}
	\subsection{Diagonal Scaling}
	% Ill conditioned
	\vspace{5pt}
	Ill-conditioned\\[5pt]
	\text{\scriptsize\(\begin{aligned}[t]
		\left(\begin{matrix}
			1 & 0 \\
			0 & \epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			x_1 \\ x_2
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			1 \\ \epsilon
		\end{matrix}\right)
	\end{aligned}\)}\\[15pt]	
	% Well conditioned
	Well-conditioned\\[5pt]
	\text{\scriptsize\(\begin{aligned}[t]
		\left(\begin{matrix}
			1 & 0 \\
			0 & 1/\epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			1 & 0 \\
			0 & \epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			x_1 \\ x_2
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			1 & 0 \\
			0 & 1/\epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			1 \\ \epsilon
		\end{matrix}\right)
	\end{aligned}\)}	
	
	% Notes
	\vspace{10pt}
	\(\begin{aligned}
		\text{\scriptsize \(\bullet\ \) No general way to correct poor scaling}
	\end{aligned}\)
\end{minipage}
%------------------------------------------------------------------
%-----------------------------------------------------------------
% Iterative Refinement
\begin{minipage}[t]{.53\textwidth}
	\subsection{Iterative Refinement}
	\vspace{5pt}
	\text{\scriptsize\(\arraycolsep=3pt \begin{array}{r c l c l}
		r_0 &=& b - Ax_0 = A \Delta x_0\\
		r_1 &=& b - A(x_0 + \Delta x_0) &=& b - Ax_1 = A \Delta x_1\\
		r_2 &=& b - A(x_1 + \Delta x_1) &=& b - Ax_2 = A \Delta x_2\\[5pt]
		\cline{1-3}
		\multicolumn{1}{|r}{\rule[-7pt]{0pt}{20pt} x} &=& \multicolumn{1}{l|}{\displaystyle x_0\ +\ \lim_{n=0}^{\infty} \Delta x_n}
			& & \text{\scriptsize(terminate when \(r_n\) is small enough)}\\
		\cline{1-3}
	\end{array}\)}

	% Notes
	\vspace{10pt}
	\text{\scriptsize\(\begin{aligned}
		&\bullet\ \text{Double storage needed to hold original matrix}\\
		&\bullet\ \text{\(r_n\) usually must be computed with higher precision than \(x_n\)}\\
		&\bullet\ \text{Useful for badly scaled systmes, or making unstable systems stable}\\
		&\bullet\ \text{If \(x_n\) is not accurate, \(r_n\) might not need better accuracy}
	\end{aligned}\)}
\end{minipage}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Least r Linear Regression/Fit
\newpage
\section{Least \(\Vert r \Vert\) Linear Regression/Fit for \(Ax + r = b\)}

% Assumptions
\(\begin{aligned}[t]
	&\bullet\ A = A_{m \times n} \hspace{10pt} \boxed{ \text{\scriptsize\((m > n\),\ \ underdetermined\()\)} }\\[5pt]
	&\bullet\ \Vert r\text{\scriptsize\((y=Ax)\)} \Vert \text{ is cont. \& coer.} 
	\ \rightarrow \ \exists \Vert r\text{\scriptsize\((y)\)} \Vert_\text{min}
\end{aligned}
\hfill
\begin{aligned}[t]
	&\bullet \ r\text{\scriptsize\((y)\)} \text{ is strictly convex} \ \rightarrow \ y=Ax \text{ is unique}\\[5pt]
	&\bullet\ \boxed{ \begin{gathered}[t]
			\text{rank}(A) = n\\[-3pt]
			\text{\scriptsize(full column rank)}
		\end{gathered} }
		\ \Rightarrow
		\arraycolsep=3pt \begin{array}[t]{r c c c c}
			A(x_1 - x_2) &=& 0 & &			\text{\scriptsize(unique \(x\))}\\
			(x_1 - x_2) &=& 0 &\rightarrow& x_1 = x_2 
		\end{array}
\end{aligned}\)

% Example: Vandermonde Matrix 
\vspace{5pt}
\underline{Example - Vandermonde Matrix, \(A\)}:\\[10pt]
\(
	Ax = \text{\scriptsize \(
		\left(\begin{matrix}
			- \vec{f}(t_1) -\\ 
			\vdots\\ 
			- \vec{f}(t_m) -
		\end{matrix}\right) 
		\left(\begin{matrix}
			|\\
			\vec{x}\\
			| 
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			y(t_1)\\ 
			\vdots\\ 
			y(t_m)
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			|\\ 
			\vec{y}\\ 
			|
		\end{matrix}\right) 
	\)} 
	= (x^T A^T)^T 
	\hspace{7pt} , \hspace{15pt}
	y(t) = \sum_{i=1}^n x_i f_i(t) = \vec{x} \cdot \vec{f}
\)

\vspace{15pt}
\(
	% Vector b decomp.
	\begin{aligned}
		&\text{\underline{Decompose \(b\)}}:\\[5pt]
		&{\arraycolsep=3pt \begin{array}{r c c c c}
				b &=& Ax &+& r \\[3pt]
				&=& y 	&+& r \\[3pt]
				&=& Pb 	&+& P_\perp b
			\end{array}}
	\end{aligned}
	\hspace{.5cm}
	\rule[-35pt]{.5pt}{75pt}
	\hspace{.5cm}
	% Orthogonal Projector Matrix
	\begin{aligned}
		&\underline{\text{Projector of A, \(P\)}}\\[10pt]
		&\begin{array}{r l}
			% Projector Def
			\text{Projector}: & \begin{gathered}[t]
					P^2 = P\\[-3pt]
					\text{\scriptsize(Idempotent)}
				\end{gathered}
				\ \rightarrow\ 
				\begin{gathered}[t]
					PA = A\\[-3pt]
					\text{\scriptsize(Projector of A)}
				\end{gathered}\\[15pt]
			% Ortho. Proj.
			\underline{\text{Orthogonal Projector}}: & P^T = P \ \rightarrow\ P_\perp A = (I - P)A = 0
		\end{array}
	\end{aligned}
\)

% Minimize residual, r
\vspace{15pt}
\underline{Minimize residual, \(r\)}:\\[10pt]
\(\begin{aligned}[t]
	\nabla \Vert r \Vert_2^2 &= 0 \hspace{20pt} 
		\text{\scriptsize\(\left(\tfrac{\partial r^2}{\partial x_i} = 0 \right)\)}\\[5pt]
	&= \nabla\left[ (b-Ax)^T(b-Ax) \right]\\[5pt]
	&= \nabla\left( b^T b - 2 x^T A^T b + x^T A^T Ax \right)\\[5pt]
	0 &= 2 A^T Ax - 2 A^T b\\
	&\downarrow\\
	A^TAx &= A^T b \hspace{10pt} \text{\scriptsize(Solvable with Cholesky)}
\end{aligned}
\hspace{2cm}
\begin{aligned}[t]
	\Vert r \Vert_2^2 &= \Vert Pr + P_\perp r \Vert_2^2 = \Vert b - Ax \Vert^2\\
	&= \Vert Pr \Vert^2 + \Vert P_\perp r \Vert^2\\
	&= \cancel{ \Vert Pb - Ax \Vert_2^2 } + \Vert P_\perp b \Vert_2^2 \\
	&\downarrow\\
	Ax &= Pb\\
	A^T Ax &= A^T P b = (P^T A)^T b \\
	A^T Ax &= A^T b \hspace{10pt} \text{\scriptsize(System of Normal Equations)}
\end{aligned}\)

% Cross Product Matrix
\vspace{10pt}
\begin{minipage}[t]{.49\textwidth}
	\underline{Cross-Product Matrix of \(A\)}:\ \ \(\boxed{A^T A}\)\\[10pt]
	\(\begin{array}{r l}
		\text{Symmetric}: & (A^T A)^T = A^T A\\[10pt]
		\text{Pos. Def.}: & \begin{aligned}[t]
				&\text{rank}(A) = n\\
				&\rightarrow \ \begin{aligned}[t]
						&\big\langle x \big| A^T Ax \big\rangle = x^T A^T A x\\
						&= (Ax)^T (Ax) \\
						&= \Vert Ax \Vert^2 \geq 0
					\end{aligned} \\
			\end{aligned} \\[2.2cm]
		\text{Nonsingular}: & \begin{aligned}[t]
				&A^T A x = 0 \\
				&\rightarrow\ \Vert Ax \Vert^2 = 0 = Ax \\
				&\rightarrow\ (x = 0)
			\end{aligned} 
	\end{array}\)	
\end{minipage}
% System of Normal Equations
\hfill
\begin{minipage}[t]{.49\textwidth}
	\underline{System of Normal Equations}: \ \ \(\boxed{A^T Ax = A^T b}\)\\[20pt]
	\underline{Pseudoinverse, \(A^+\)}\\[10pt]
	\(
		\boxed{ \begin{aligned}
			x &= (A^T A)^{-1} A^T b \\[3pt]
			&\equiv A^+ b
		\end{aligned} }
		\ \ \ \rightarrow \ \ \
		\boxed{ \begin{aligned}
			A^+ &\equiv (A^T A)^{-1} A^T \\
			A^+ A &= I
		\end{aligned} }
	\)\\[20pt]
	\underline{Ortho. Proj., \(P\)}\\[10pt]
	\(
		\boxed{ \begin{aligned}[t]
			Ax &= A (A^T A)^{-1} A^T b \\
			&= Pb
		\end{aligned} }
		\ \ \ \rightarrow \ \ \
		\boxed{ \begin{aligned}[t]
			P &= A (A^T A)^{-1} A^T \\
			&= A A^+ 
		\end{aligned} }
	\)
\end{minipage}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
% Issues with System of Normal Equations
\newpage
\underline{System of Normal Equations Issues}:\\[10pt]
\(\begin{aligned}
	&\bullet\ \text{Info can be lost forming \(A^TA\), e.g, \ } 
		A = \text{\scriptsize\(\left(\begin{matrix}
			1 & 0 \\ 
			\epsilon & 0 \\ 
			0 & \epsilon
		\end{matrix}\right)\)} 
		\ \rightarrow \ 
		A^T A = \text{\scriptsize\(\left(\begin{matrix}
			1 + \epsilon^2 & 1 \\ 
			1 & 1 + \epsilon^2
		\end{matrix}\right)\)} 
		\ \approx \ \text{\scriptsize\(\left(\begin{matrix}
			1 & 1 \\ 
			1 & 1
		\end{matrix}\right)\)} 
		\hspace{10pt} \text{\scriptsize(singular)} \\[.3cm]
	&\bullet\ \text{System of Normal Equations:}\ \ \boxed{ \text{cond}(A^T A) = [\text{cond}(A)]^2 }
\end{aligned}\)

%------------------------------------------------------------------------
%------------------------------------------------------------------------
% Residual, and Error Bounds when Solving Ax = b
\vspace{15pt}
\subsection{Error Bounds and Residuals}
% Error Bound
\vspace{10pt}\noindent
\underline{Error Bound}:\ \ \(\boxed{ \displaystyle \frac{\Vert \Delta x \Vert}{\Vert x \Vert} 
	\ \lessapprox\ \text{cond}(A)\ \epsilon_\text{mach} } 
	\ \rightarrow \ \begin{minipage}{8cm}
		\scriptsize
		A computed solution is expected to lose about \(\log_{10}(\text{cond}(A))\) digits, so
		the input data must be more accurate to these digits and 
		the working precision must carry more than these digits.
	\end{minipage}\)

% Norm and Conditioning
\vspace{10pt}
\underline{Norm and Conditioning}:\\[10pt]
% Norm
\begin{minipage}[t]{.49\textwidth}
	\(\boxed{ \Vert A \Vert = \max_{x \neq 0} \left( \frac{\Vert Ax \Vert}{\Vert x \Vert} 
	= \frac{\Vert AA^+ b \Vert}{\Vert A^+ b \Vert} \right) }\)
\end{minipage}
% Cond(A)
\begin{minipage}[t]{.49\textwidth}
	\(\boxed{ \text{cond}(A) = \begin{cases}
		\Vert A \Vert_2 \cdot \Vert A^+ \Vert_2 & \hspace{10pt} \text{rank}(A) = n\\
		\infty & \hspace{10pt} \text{rank}(A) < n
	\end{cases} }\)
\end{minipage}

% Error Bound for Change in Vector b
\vspace{15pt}\noindent
\(\begin{aligned}[t]
	&A^T A(x + \Delta x) = A^T A(b + \Delta b)\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{array}{l c c c}
			\bullet\ &\Vert \Delta x \Vert 	&\leq& \Vert A^+ \Vert \cdot \Vert \Delta b \Vert
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \begin{aligned}[t]
			\tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
				&\leq \left( \text{cond}(A) \tfrac{\Vert b \Vert}{\Vert Ax \Vert} \right) 
				\tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}\\[5pt]
			&= \left( \text{cond}(A) \tfrac{1}{\cos \theta} \right) 
				\tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}
		\end{aligned} }\\[20pt]
	% Other notes
	&\begin{aligned}
			&\bullet\ \text{\scriptsize Cond. number is a func. of cond\((A)\) and \(b\)}\\
			&\bullet\ \text{\scriptsize \(Pb \approx 0\) or \(\theta\approx 90^\circ\) is highly sensitive}
		\end{aligned}
\end{aligned}
% Residual
\hfill
\begin{aligned}[t]
	&(A + \Delta A)^T (A + \Delta A)(x + \Delta x) = (A + \Delta A)^T b\\[10pt]
	&\hspace{10pt} \begin{aligned}
			&\bullet\ \text{\scriptsize\(\begin{aligned}[t]
					&\cancel{A^T A x} + A^T \Delta A x + (\Delta A)^T A x + \bcancel{ (\Delta A)^T \Delta A x }\\
					&\ \ + A^T A \Delta x 
						+ \bcancel{ A^T \Delta A \Delta x} 
						+ \bcancel{ (\Delta A)^T A \Delta x } 
						+ \bcancel{ (\Delta A)^T \Delta A \Delta x }
				\end{aligned}\ \ =\ \ \cancel{A^T b} + (\Delta A)^T b \)}\\[10pt]
			&\bullet\ \begin{aligned}[t]
					\Vert \Delta x \Vert 
						&= \Vert (A^T A)^{-1}(\Delta A)^T r - A^+ \Delta A x \Vert\\[5pt]
					&\leq \Vert (A^T A)^{-1} \Vert \cdot \Vert \Delta A \Vert \cdot \Vert r \Vert 
						+ \Vert A^+ \Vert \cdot \Vert \Delta A \Vert \cdot \Vert x \Vert
				\end{aligned}
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \begin{aligned}[t]
			\tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
				&\leq \left( \scriptstyle [\text{cond}(A)]^2 \tfrac{\Vert r \Vert}{\Vert Ax \Vert} 
				+ \text{cond}(A) \right) \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}\\[5pt]
			&= \big( \scriptstyle [\text{cond}(A)]^2 \tan \theta
				+ \text{cond}(A) \big) \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
		\end{aligned} }
\end{aligned}\)

%--------------------------------------------------------------
%--------------------------------------------------------------
% Augmented Matrix
\vspace{15pt}
\subsection{Solving \(A^TAx = A^Tb\) with an Augmented Matrix}
\vspace{10pt}
% Equations
\[
	\arraycolsep=1pt
	\begin{array}{r c c c c}
		r &+& Ax &\ =\ & b\\[5pt]
		A^Tr & & &\ =\ & 0
	\end{array}
	\ \ \Rightarrow \ \
	\left(\begin{matrix}
		I & A\\
		A^T & 0
	\end{matrix}\right)
	\left(\begin{matrix}
		r \\
		x
	\end{matrix}\right)
	= 
	\left(\begin{matrix}
		b\\
		0
	\end{matrix}\right)
	\ \ \Rightarrow \ \
	\left(\begin{matrix}
		\alpha I & A\\
		A^T & 0
	\end{matrix}\right)
	\left(\begin{matrix}
		r/\alpha \\
		x
	\end{matrix}\right)
	= 
	\left(\begin{matrix}
		b\\
		0
	\end{matrix}\right)
\]

% Notes
\vspace{5pt}
\(\begin{aligned}
	&\bullet\ \text{Solvable with \(LU\) Decomp or Symm. Pos. Def. Methods} \\
	&\bullet\ \alpha \text{ "controls the relative weights of the two subsystems in chooing pivots from either"}\\
	&\bullet\ \alpha = \max a_{ij}/1000 \hspace{20pt} \text{\scriptsize(rule of thumb)}\\
	&\bullet\ \text{MATLAB uses it for large, sparse systems}
\end{aligned}\)

%------------------------------------------------------------
%------------------------------------------------------------
%------------------------------------------------------------
%------------------------------------------------------------
% QR Decomposition
\newpage
\subsection{QR Decomposition}
% Motivation
\vspace{5pt}
\underline{Motivation}: \ \(
	\begin{gathered}
		Q^T A = \text{\scriptsize\(\left(\begin{matrix}
				R \\ 
				0
			\end{matrix}\right)\)}\\[5pt]
		\text{\scriptsize(\(R\) is upp. triag.)}
	\end{gathered}
	\ \ \rightarrow \ \
	\arraycolsep=2pt
	\begin{array}{r c c c l}
		Q^T Ax &+& Q^T r &=& Q^T b \\[5pt]
		\text{\scriptsize\(\left(\begin{matrix}
				Rx \\ 
				0
			\end{matrix}\right)\)}
			&+& 
			\text{\scriptsize\(\left(\begin{matrix}
				r_1' \\[2pt]
				r_2'
			\end{matrix}\right)\)}
			&=&
			\text{\scriptsize\(\left(\begin{matrix}
				b_1' \\[2pt]
				b_2'
			\end{matrix}\right)\)}
	\end{array}
	\ \ \rightarrow \ \
	\begin{aligned}
		\Vert r' \Vert^2 &= \cancel{ \Vert b_1' - Rx \Vert^2 } + \Vert b_2' \Vert^2 \\
		&\downarrow\\
		\Aboxed{Rx &= b_1'} \ , \ r' = \text{\scriptsize\(\left(\begin{matrix}
				0 \\ 
				b_2'
			\end{matrix}\right)\)} 
			\hspace{12pt} \text{\scriptsize(solve with back-sub)}
	\end{aligned}
\)

% QR Decomp
\vspace{5pt}
\(\hspace{20pt} \begin{aligned}[t]
	&\underline{\text{Orthogonal Matrix, \(Q\)}}\\[2pt]
	&\boxed{ Q^T Q = QQ^T = I }
\end{aligned} 
\hfill
\begin{aligned}[t]
	&\underline{\text{QR Factorization}}\\[2pt]
	&\boxed{ A = Q \left(\begin{matrix}
			R\\ 0
		\end{matrix}\right) }
\end{aligned}
\hfill
\begin{aligned}[t]
	&\underline{\text{Reduced QR Factorization}}\\[2pt]
	&\boxed{ A = Q \left(\begin{matrix}
			R\\ 0
		\end{matrix}\right)
		= 
		\bigg(\begin{matrix}
			Q_\parallel & Q_\perp
		\end{matrix}\bigg)
		\left(\begin{matrix}
			R\\ 0
		\end{matrix}\right)
		= Q_\parallel R }
\end{aligned}\)

% Interpretations
\vspace{15pt}
% 1. Q^T is a rotation
\begin{minipage}[t]{.49\textwidth}
	\textbf{1.} \hspace{5pt} \begin{minipage}[t]{7cm}
		\textbf{\(Q^T\) is a span\((A)\) Plane Rotation}\\[3pt]
		\textbf{through \(\mathbb{R}^m\) to span\(\big( [R\ \ 0]^T \big)\)}
	\end{minipage}

	% Orthogonal Matrix Rotates/Reflects and preserves the norm
	\vspace{10pt}
	\fbox{
		\(\begin{aligned}
			&\text{2-norm Preserved {\scriptsize(\(Q\) is a rotation/reflection)}}\\[5pt]
			&\bullet\ \begin{aligned}[t]
					\Vert Q v \Vert^2 &= \langle v | Q^TQv \rangle = \Vert v \Vert^2\\
					\Vert Q^T v \Vert^2 &= \langle v | QQ^Tv \rangle = \Vert v \Vert^2
				\end{aligned}\\[5pt]
		\end{aligned}\)
	}

	% Describe how Q^T transforms A to R
	\vspace{10pt}
	\(\begin{gathered}
		\begin{array}{l c l}
			\bullet\ Q^T = H_n\dots H_1 &\ &\bullet\ H_i^T H_i = H_i H_i^T = I \\[3pt]
			\bullet\ A = [a_1\ \dots\ a_n] &\ &\bullet\ I_n = [e_1\ \dots\ e_n]
		\end{array}\\[10pt]
		\arraycolsep=3pt \begin{array}{r c l}
				H_1 a_1 &=& \alpha_1 e_1 
					\hspace{15pt}
					\text{\scriptsize\(\left( \Vert a_1 \Vert = \vert \alpha_1 \vert \right)\)}
					\\[10pt]
				H_i\dots H_1 a_i &=& \begin{gathered}[t]
					{\scriptstyle \sum}^i_j\ c_j e_j \ =\ H_n\dots H_1 a_i\\[5pt]
					\text{\scriptsize\(\left( \Vert a_i \Vert^2 = \vert \alpha_1 \vert^2 
						= {\scriptstyle \sum}_j^i\ c_j^2 \right)\)}
				\end{gathered}
					\\[27pt]
				\langle r | a_i \rangle &=& 0
					\hspace{27pt}
					\text{\scriptsize\((1 \leq i \leq n)\)}
					\\[7pt]
				\langle H_i \dots H_1r | e_j \rangle &=& 0 
					\hspace{27pt}
					\text{\scriptsize\((1 \leq j \leq i)\)}
			\end{array}
	\end{gathered}\)

	% Conclusion
	\vspace{15pt}
	\(\rightarrow\) \fbox{ 
		\begin{minipage}[t]{7.5cm}
			\(Q^TA\) rotates \(A\) until the column vectors are aligned with certain axes described above
		\end{minipage}
	}
\end{minipage}
\vline
% 2. A is a linear sum of Q
\hfill
\begin{minipage}[t]{.48\textwidth}
	\textbf{2.} \hspace{5pt} \begin{minipage}[t]{7cm}
		\textbf{\(A\) is a Lin. Sum of \(Q_\parallel\)'s Orthogonal}\\[3pt]
		\textbf{Column Vectors Given by \(R\)}
	\end{minipage}

	% System of Orthogonal Equations
	\vspace{15pt}
	\(\begin{aligned}
		&\Big\{ Q_\parallel = Q_{m \times n}\ \Big|\ \text{span}(Q_\parallel) = \text{span}(A) \Big\} \\[5pt]
		&\rightarrow\ Q^+ = (Q^T Q)^{-1} Q^T = Q^T\\[5pt]
		&\rightarrow\ P = Q_\parallel Q_\parallel^T\\[5pt]
		&\rightarrow\ \begin{aligned}[t]
				Q_\parallel^T Ax &= Q_\parallel^T Pb = \cancel{Q_\parallel^T Q_\parallel} Q_\parallel^T b \\[4pt]
				&= Q_\parallel^T b \hspace{10pt} \text{\scriptsize(System of Orthogonal Equations?)}
			\end{aligned}
	\end{aligned}\)

	% Describe how R changes Q to A
	\vspace{20pt}
	\(\begin{aligned}
		&A = Q_\parallel R = \text{\scriptsize\(
			\left(\begin{matrix}
				| & \hspace{-7pt} |	& \hspace{-7pt} | \\
				\vec{q_1} &\hspace{-7pt}\dots &\hspace{-7pt}\vec{q_n}\\
				| & \hspace{-7pt} |	& \hspace{-7pt} | \\
			\end{matrix}\right) 
			\left(\begin{matrix}
				r_{11} & \hspace{-5pt} \dots \hspace{-5pt} & r_{1n} \\[-5pt]
				0 & \hspace{-5pt} \ddots \hspace{-5pt} & \vdots \\
				0 & \hspace{-5pt} 0 \hspace{-5pt} & r_{nn}
			\end{matrix}\right)\)}
			= \text{\scriptsize\(
				\left(\begin{matrix}
					| & \hspace{-7pt} |	& \hspace{-7pt} | \\
					\vec{a_1} &\hspace{-7pt}\dots &\hspace{-7pt}\vec{a_n}\\
					| & \hspace{-7pt} |	& \hspace{-7pt} | \\
				\end{matrix}\right)\)} \\
		&\bullet\ \vec{a_j} = \sum_i^j r_{ij}\cdot \vec{q_i}
	\end{aligned}\)

	% Conclusion
	\vspace{15pt}
	\(\rightarrow\) \fbox{ 
		\begin{minipage}[t]{7.5cm}
			\(R\) transforms the \(Q_\parallel\) column vectors about \\[5pt]
			span\((A)\), an \(\mathbb{R}^n\) plane, until they equal the\\[5pt]
			column vectors of \(A\)
		\end{minipage}
	}
\end{minipage}

%---------------------------------------------------------------		
%---------------------------------------------------------------		
%---------------------------------------------------------------		
%---------------------------------------------------------------		
% Householder Transformations
\newpage
\subsubsection{Householder Transformation/Elementary Reflector, \(H\)}
% Define H
\[
	\begin{aligned}
		H\vec{a_1} &\ =\ \alpha_1 \vec{e_1} 
			\hspace{20pt} \begin{gathered}
				\text{\scriptsize\( \Vert a_1 \Vert = \vert \alpha_1 \vert \)}\\[-7pt]
				\text{\scriptsize(rotation)}
			\end{gathered}\\[5pt]
		&\ =\ \begin{gathered}[t]
				\boxed{ \vec{a_1} - 2\hat{v} (\hat{v} \cdot \vec{a_1}) }\\[-1pt]
				\text{\scriptsize\(\big[ v_\perp \text{ bisects } \theta(a_1, e_1) \big]\)}
			\end{gathered}
	\end{aligned}
	\ \ \ \rightarrow \ \ \
	\boxed{ H = I - 2vv^T = I - \frac{2vv^T}{v^Tv} }
	\hspace{20pt}
	\begin{aligned}
		&\bullet\ H = H^T = H^{-1}\\[-5pt]
		&\text{\scriptsize(symmetric and orthogonal)}
	\end{aligned}
\]

% Solve for v
\vspace{5pt}
\(\begin{aligned}
	&\bullet\ 
		\begin{aligned}[t]
			\alpha_1 e_1 = a_1 - (2v_1)\frac{v_1 \cdot a_1}{v_1 \cdot v_1} 
				&\ \ \Rightarrow\ \ v_1 = (a_1 - \alpha e_1) \cancel{ \frac{v_1 \cdot v_1}{2v_1 \cdot a_1} } 
				\hspace{15pt} \text{\scriptsize(magnitude doesn't matter)}
				\\[5pt]
			&\ \ \rightarrow\ \ \boxed{ v_1 = (a_1 - \alpha e_1) }
				\\[10pt]
			\alpha_1 = \pm \Vert a_1 \Vert
				& \ \ \rightarrow\ \ \boxed{ \alpha_i = - \text{sign}(a_i) \Vert a_i \Vert }
				\hspace{15pt} \text{\scriptsize(avoid "cancellation" in finite-calc. of \(v\) above)}
				\\[5pt]
			H_j\dots H_1 a_i = a_i^j 
				& \ \ \rightarrow\ \ \boxed{ v_{j+1} = \text{\scriptsize\(\left(\begin{matrix}
					0\\
					\vdots\\
					(a_i^j)_i\\
					\vdots\\
					(a_i^j)_m
				\end{matrix}\right)\)} - \alpha_i e_i }
				% Extra notes
				\hspace{17pt}
				\begin{aligned}
					&\bullet\ \text{\scriptsize Store \(v_i\) and \(R\) into \(A\) and an extra \(n\)-vector.}\\
					&\bullet\ \text{\scriptsize \(Q\) and \(H\) can be computed if needed.}\\
					&\bullet\ \text{\scriptsize When column \(i\) is completed, row \(i\) is too.}
				\end{aligned}
		\end{aligned}
\end{aligned}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Givens Rotation
\vspace{10pt}
\subsubsection{Givens Rotation, \(G\)}
\vspace{5pt}
\[\begin{aligned}
	\boxed{ G = 
		\left(\begin{matrix}
			c & s\\
			-s& c
		\end{matrix}\right) }
		&\ \rightarrow \ Gx = 
		G \left(\begin{matrix} 
			a_1\\
			a_2 
		\end{matrix}\right) 
		= \pm 
		\left(\begin{matrix} 
			\Vert a \Vert \\
			0 
		\end{matrix}\right)
		\hspace{25pt}
		% Extra Notes
		\begin{aligned}
			&\bullet\ \text{\scriptsize creates 0's one at a time}\\[-5pt]
			&\bullet\ \text{\scriptsize useful for spare matrices}\\[-5pt]
			&\bullet\ \text{\scriptsize When column \(i\) is completed, row \(i\) is too.}\\[-5pt]
		\end{aligned}\\[10pt]
	&\ \rightarrow \ \boxed{ c = \frac{a_1}{\sqrt{a_1^2 + a_2^2}}\ , \ \ s = \frac{a_2}{\sqrt{a_1^2 + a_2^2}} }
\end{aligned}\]

% Minor change to reduce chance of overflow/underflow
\(\begin{aligned}
	&\text{Avoid squaring any number \(\gg 1\) to prevent overflow/underflow}\\[5pt]
	&\hspace{15pt} \begin{array}{c r c l c l}
			\bullet & t = \tfrac{a_2}{a_1} < 1 &\rightarrow& c = \frac{1}{\sqrt{1+t^2}} &,& s = c \cdot t\\[5pt]
			\bullet & \tau = \tfrac{a_1}{a_2} < 1 &\rightarrow& s = \frac{1}{\sqrt{1+\tau^2}} &,& c = s \cdot \tau\\
		\end{array}
\end{aligned}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Gram-Schmidt Orthogonalization
\vspace{10pt}
\subsubsection{Gram-Schmidt Orthogonalization}
\vspace{10pt}
% Gram Schmidt Matrices
\(
	\begin{aligned}
		Q_\parallel^T = 
		\left(\begin{aligned}
			&\widehat{q_1} : q_1 = a_1\\
			&\widehat{q_2} : q_2 = a_2 - \widehat{q_1} (\widehat{q_1} \cdot a_2)\\
			&\widehat{q_3} : q_3 = a_3 - \widehat{q_1} (\widehat{q_1} \cdot a_3) - \widehat{q_2} (\widehat{q_2} \cdot a_3)\\
			&\hspace{3pt} \vdots\\
			&\widehat{q_n} : q_n = a_n - {\scriptstyle\sum}_j^n\ \widehat{q_j} (\widehat{q_j} \cdot a_n)
		\end{aligned}\right)
	\end{aligned}
	\ , \ \ 
	\begin{aligned}
		R = 
		\left(\begin{matrix}
			\Vert a_1 \Vert & \widehat{q_1} \cdot a_2 & \widehat{q_1} \cdot a_3 & \dots & \widehat{q_1} \cdot a_n\\
			0 & \Vert a_2 \Vert & \widehat{q_2} \cdot a_3 & \dots & \widehat{q_2} \cdot a_n\\
			0 & 0 & \Vert a_3 \Vert & \dots & \widehat{q_3} \cdot a_n\\
			\vdots & \vdots & \ddots & \ddots & \vdots\\
			0 & 0 & \dots & 0 & \Vert a_n \Vert
		\end{matrix}\right)
	\end{aligned}
\)

%-------------------------------------------------------------
%-------------------------------------------------------------
%-------------------------------------------------------------
%-------------------------------------------------------------
\newpage
% Classical Column Oriented Gram Schidt
\underline{Classical, Column Oriented}:\ \ Find \(\widehat{q_i}\), then solve for \(\widehat{q_{i+1}}\), and
	continue up to \(\widehat{q_n}\).

% Notes
\(\hspace{10pt} \begin{aligned}[t]
	&\bullet\ \text{For a program, obviously \(a_k\) can be replaced by \(q_k\), so less storage is needed.}\\
	&\bullet\ \text{Cancellation that causes loss of orthogonality occurs more when ill-conditioned.}\\
	&\bullet\ \text{As a result, performing \(Q^T_\parallel b = b'_1\) isn't always best.}\\
	&\bullet\ \text{Can't column-pivot, since that depends on rows being completed first.}
\end{aligned}\)

% Modified Row Oriented Gram Schidt
\vspace{10pt}
\underline{Modified, Row Oriented}:\ \ Let
	\(q_i^\text{\scriptsize\([k]\)} = a_i - {\scriptstyle\sum}_j^k\ \widehat{q_j} (\widehat{q_j} \cdot a_i)\). For all
	\(1\leq i \leq n\), solve for \(q_i^\text{\scriptsize\([k]\)}\) starting first \\[5pt] at \(k=1\), then continue 
	until \(k=n\).

% Notes
\(\hspace{10pt} \begin{aligned}
	&\bullet\ \text{Allows for column pivoting since rows are completed first.}\\
	&\bullet\ \text{Cancellation, though still present, is less severe.}
\end{aligned}\)

% Augmented Gram Schmidt matrices
\vspace{10pt}
\underline{Augmented Matrix}:\\[10pt]
\(\begin{aligned}
	\left(\begin{matrix}
			A
		\end{matrix}\right.
		|
		\left.\begin{matrix}
			b
		\end{matrix}\right)
		&= 
		\left(\begin{matrix}
			Q_\parallel
		\end{matrix}\right.
		|
		\left.\begin{matrix}
			q_{n+1}
		\end{matrix}\right)
		\text{\scriptsize\(
			\left(\begin{matrix}
				R & b'_1\\
				0 & \rho
			\end{matrix}\right)
		\)}\\[5pt]
	\left(\arraycolsep=2pt \begin{array}{c c c c}
			| & | & | & |\\
			a_1 & .. & a_n & b\\
			| & | & | & |
		\end{array}\right)
		&=
		\left(\arraycolsep=1pt \begin{array}{c c c c}
			| & | & | & |\\
			\widehat{q_1} & \dots & \widehat{q_n} & q_{n+1}\\
			| & | & | & |
		\end{array}\right)
		\text{\scriptsize\(\arraycolsep=1pt
			\left(\begin{array}{c c c c}
				r_{11} & \dots & r_{1n} & |\\
				0 & \ddots & \vdots & b'_1\\[-5pt]
				\vdots & \ddots & r_{nn} & |\\[5pt]
				0 & \dots & 0 & \rho\\
			\end{array}\right)
		\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}
	&\bullet\ \begin{minipage}[t]{7cm}
			\scriptsize Use Gram-Schmidt QR on this, then solve \(Rx = b'_1\)
		\end{minipage}\\
	&\bullet\ \begin{minipage}[t]{7cm}
			\scriptsize This method is prefered numerically to reduce \\
			cancelling effects
		\end{minipage}\\
	&\bullet\ \begin{minipage}[t]{7cm}
			\scriptsize Text didn't recommend what \(q_{+1}\) or \(\rho\) should be.
		\end{minipage}\\ 
	&\bullet\ \text{\scriptsize \(\rho\) or \((q_{n+1})_i\) looks like it should be 0.}\\
	&\bullet\ \text{\scriptsize Idk, not much explained.}
\end{aligned}\)

% Extra
\vspace{10pt}
\underline{Reorthogonalizing}:\ \ Repeating procedure to straighten vectors (usually not needed)

%-----------------------------------------------------------
%-----------------------------------------------------------
% Column-Pivoting
\vspace{7pt}
\subsubsection{Factorization with Column-Pivoting}
\vspace{7pt}
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Column with largest norm is pivoted to the current column \(i\) to be reduced, 
		and current row \(i\) is completed too.}\\
	&\bullet\ \text{\scriptsize Choose the next pivoting column based on norms of the smaller columns from remaining 
		uncompleted submatrix.}\\
	&\bullet\ \text{\scriptsize Repeat until the end (rank might be \(n\)) or if the max norm is smaller than 
			some tolerance (rank might be \(k < n\))}\\
	&\bullet\ \text{\scriptsize Pivoting avoids working with 0's on the diag.}
\end{aligned}\)

%-----------------------------------------------------------
%-----------------------------------------------------------
% Rank-Deficiency
\vspace{7pt}
\subsubsection{Rank Deficiency (or Other) Case}
\vspace{10pt}
\(\begin{aligned}
	&\underline{\text{If rank}(A) = k < n} : \ \ \boxed{ \arraycolsep=0pt \begin{array}{c c c l}
			(Q^T AP) & (P^Tx) &=& Q^Tb\\[5pt]
			\text{\scriptsize\(\left(\begin{matrix}
					R\ & S\\
					0\ & 0'
				\end{matrix}\right)\)} 
				& \text{\scriptsize\(\left(\begin{matrix}
					z\\
					0
				\end{matrix}\right)\)} &\ \ =\ \ 
				& \text{\scriptsize\(\left(\begin{matrix}
					b'_1\\[3pt]
					b'_2
				\end{matrix}\right)\)}
			\end{array} }
			\hspace{30pt}
			\begin{aligned}
				&\text{\scriptsize\(\bullet\ 0'\) is approx. 0 since the remaining norms are too small.}\\[-5pt]
				&\text{\scriptsize\(\bullet\ R = R_{k \times k}\)}\\[-5pt]
				&\text{\scriptsize\(\bullet\ S\) is the remaining columns after \(R\) is completed.}\\[-5pt]
				&\text{\scriptsize\(\bullet\) \fbox{There are multiple solutions for \(x\).}}
			\end{aligned}\\[10pt]
	&\bullet\ \text{\scriptsize For a quick solution, }\ \boxed{ Rz = b'_1\ , \ \ x = P\text{\scriptsize\(\left(\begin{matrix}
			z\\
			0
		\end{matrix}\right)\)} }\\[5pt]
	&\bullet\ \text{\scriptsize For the minimized-norm solution with the smallest \(\Vert x \Vert\), \(S\) must be annihilated.}\\
	&\bullet\ \boxed{ \text{\scriptsize For another method or if underdetermined \((m < n)\), 
		something like SVD Decomp. can be used.} }
\end{aligned}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
\newpage
% Singular Value Decomposition
\subsection{Singular Value Decomposition (SVD)}
\vspace{5pt}
\[ \boldsymbol{ \arraycolsep=3pt
	\begin{array}{c c c c l}
		A &=& \boxed{ U\Sigma V^T }
			&=& \text{\scriptsize\(\left(\arraycolsep=3pt \begin{array}{c c c c c}
				\rule[-20pt]{.05pt}{38pt} &\hspace{20pt}& \rule[-20pt]{.05pt}{38pt} & & \rule[-20pt]{.05pt}{38pt}\\
				u_1 & \dots & u_k & .. & u_m\\
				\rule[-20pt]{.05pt}{38pt} & &\rule[-20pt]{.05pt}{38pt}& & \rule[-20pt]{.05pt}{38pt}
			\end{array}\right)
			\left(\arraycolsep=1pt \begin{array}{c c c c}
				\sigma_1 & 0 & \dots & 0\\
				0 & \sigma_2 & \ddots & \vdots\\[-4pt]
				0 & 0 & \ddots & 0\\[-3pt]
				\vdots & \vdots & \ddots & \parbox{14pt}{\(\sigma_n\) \vspace{7pt}} \\
				0 & 0 & \dots & 0\\[-4pt]
				\vdots & \vdots & & \vdots\\
				0 & 0 & \dots & 0
			\end{array}\right)
			\left(\begin{array}{c}
				\rule[2pt]{20pt}{.1pt} v_1 \rule[2pt]{20pt}{.1pt}\\[-3pt]
				\hspace{5pt}\\[-2pt]
				\vdots\\[-2pt]
				\hspace{5pt}\\[-2pt]
				\rule[2pt]{20pt}{.1pt} v_k \rule[2pt]{20pt}{.1pt}\\[-4pt]
				\vdots\\[-4pt]
				\rule[2pt]{20pt}{.1pt} v_n \rule[2pt]{20pt}{.1pt}
			\end{array}\right)\)}
			\\[1.75cm]
		&=& \boxed{ U_\parallel \Sigma_1 V^T }
			&=& \left(\begin{matrix}
				U_\parallel & U_\perp
			\end{matrix}\right)
			\left(\begin{matrix}
				\Sigma_1 \\
				0
			\end{matrix}\right)
			\text{\scriptsize\(
				\left(\begin{matrix}
					V_{0\perp}^T \\[5pt]
					V_{0\parallel}^T
				\end{matrix}\right)
			\)}
			\ =\ \displaystyle \boxed{ \sum_i^n \sigma_i \cdot u_i v_i^T }
	\end{array}
	\hspace{10pt} \vline \hspace{10pt}
	\begin{aligned}
		AA^T &= U \mathcal{D} U^T\\
		A^TA &= VD V^T\\[3pt]
		\mathcal{D} &= \Sigma \Sigma^T\\
		D &= \Sigma^T \Sigma\\ 
		\sigma_i &= \sqrt{d_i}\\
	\end{aligned}
} \]

% Notes
\(\begin{aligned}
	&\bullet\ \text{Underdetermined, } m < n \text{ is possible too.}\\
	&\bullet\ \text{Analagous to Gaussian-Jordan Diagonalization method.}\\
	&\bullet\ U \text{ and } V \text{ are orthogonal; } u_i \text{ and } v_i 
		\text{ are the respective ``left'' and ``right'' singular vectors.}\\
	&\bullet\ \text{Usually, the singular values are ordered such that } \sigma_1 \geq \sigma_2 \geq \dots\\
	&\bullet\ \forall(k < i),\ \sigma_i = 0 \ \ \Rightarrow\ \ \text{rank}(A) = k < n
	\\[5pt]
	&\bullet\ U_\parallel = U_{m \times k} : \ \  
		\text{span}(U_\parallel) = \text{span}(A)\ , \ \ \text{span}(U_\perp) = \text{span}(A)^{\perp}\\
	&\bullet\ V_{0\perp} = V_{n \times k} : \ \ 
		\text{span}(V_{0\parallel}) = \text{null}(A)\ , \ \ \text{span}(V_{0\perp}) = \text{null}(A)^{\perp}
		\hspace{30pt} \text{\scriptsize null\((A) = \{x : Ax = 0\}\)}
\end{aligned}\)

% Pseudoinverse
\vspace{5pt}
\underline{Pseudoinverse}: \ \ \(\boxed{ \begin{aligned}[t]
	&A^+ \ \equiv \ V \Sigma^+ U^T\\
	&\Sigma^+ \ \equiv\ \Big[ \Sigma^T \text{\ \ and\ \ } \sigma_i \rightarrow 1/\sigma_i 
		\hspace{10pt} \text{\scriptsize\(\forall(\sigma_i \neq 0)\)} \Big]
\end{aligned} }\)

% Solution for x
\(\begin{aligned}
	&\bullet\ \begin{aligned}[t]
		&Ax + r = b \ \rightarrow \\
		&\boxed{ x = A^+ b = \big(V \Sigma^+ U^T \big) b }
	\end{aligned}\\
	&\bullet\ \boxed{ x_\text{min} = \sum_{\sigma_i \neq 0}\frac{u_i \cdot b}{\sigma_i}\ v_i } 
		\hspace{20pt} \text{\scriptsize useful for ill-conditioned or rank deficient since small \(\sigma\) can be dropped.}
\end{aligned}\)

%---------------------------------------------------------
%---------------------------------------------------------
% Other uses
\vspace{5pt}
\subsubsection{Other uses}
\vspace{5pt}
\(\begin{array}{r l}
	\text{{Euclidean 2-norm}}: 
		& \displaystyle \Vert A \Vert_2 = \max_{x\neq 0} \tfrac{\Vert Ax \Vert_2}{\Vert x \Vert_2} = \sigma_\text{max}
		\\[15pt]
	\text{{Euclid. Cond. Num.}}:
		& \text{cond}_2(A) = \sigma_\text{max} / \sigma_\text{min}
		\\[5pt]
	\text{{Lower Rank Approx.}}:
		& A \ \approx\ A_k = \sum_i^k \sigma_i \left( u_i v_i^T \right)
		\hspace{20pt}
		\begin{aligned}
			&\text{\scriptsize\(\bullet\) Closest rank\(=k\) matrix to \(A\) in the Frobinius norm.}\\[-3pt]
			&\text{\scriptsize\(\bullet\) Frobinius Norm \(=\) Euclid. Norm for a ``vector'' in \(\mathbb{R}^{mn}\).}
		\end{aligned}
		\\[10pt]
	\text{Total Least Squares}:
		& \text{\scriptsize\(\begin{aligned}[t]
			&\left[A\ |\ y\right]_{m\times(n+1)} = U\Sigma V_{(n+1)\times(n+1)}^T\\[5pt]
			&\text{rank}\big( \big[\widehat{A}\ |\ y \big] \big) \leq n
				\ \rightarrow \ \sigma_{n+1} = 0 
				\ \rightarrow \ \widehat{A} \cdot v_{n+1} = 0\\[5pt]
			&\left[\widehat{A}\ |\ y\right] \cdot \left[\begin{matrix} x \\ -1 \end{matrix}\right] = 0
				\ \rightarrow \ \left[\begin{matrix} x \\ -1 \end{matrix}\right] \propto v_{n+1} 
				= \left[\begin{matrix} \vec{\nu_n} \\ \nu_{n+1} \end{matrix}\right]
				\ \rightarrow \ \boxed{ x = \frac{\vec{\nu_n}}{- \nu_{n+1}} }
		\end{aligned}\)}
		\hspace{15pt} \begin{aligned}[t]
			&\\[-10pt]
			&\text{-}\parbox[t]{3.7cm}{\scriptsize\(\widehat{A}\) is an \(A\) with uncertainty, like how \(y\) normally is.}
		\end{aligned}
\end{array}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
\newpage
% Complexity
\subsection{Complexity}

% Cholesky
\vspace{10pt}
\(\begin{aligned}
	&\text{Normal, Cholesky}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ A^TA = A' \text{\ \ costs\ \ } \tfrac{mn^2}{2}\\[2pt]
		&\bullet\ A' = LL^T \text{\ \ costs\ \ } \tfrac{n^3}{6}\\[2pt]
		&\bullet\ \text{Rel. Err. } \propto\ [\text{cond}(A)]^2\\[4pt]
		&\bullet\ \text{Bad if \ cond}(A) \approx 1/\sqrt{\epsilon_\text{mach}}
	\end{aligned}
\end{aligned}\)

% Householder
\vspace{10pt}
\(\begin{aligned}
	&\text{Householder}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ Q^TA = R \text{\ \ costs\ \ } mn^2 - \tfrac{n^3}{3} \\[2pt]
		&\bullet\ \text{Rel. Err. } \propto\ [\text{cond}(A)]^2 \Vert r \Vert_2 + \text{cond}(A)\\[4pt]
		&\bullet\ \text{Bad if \ cond}(A) \approx 1/\epsilon_\text{mach}\\[2pt]
		&\bullet\ \text{More accurate than Cholesky and broadly applicable}\\[2pt]
		&\bullet\ \text{Usable for rank deficient or nearly rank-deficient}
	\end{aligned}
\end{aligned}\)

% Givens
\vspace{10pt}
\(\begin{aligned}
	&\text{Givens}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ \text{The normal implementation needs 50\% more work than Householder.}\\[2pt]
		&\bullet\ \text{A more complex implementation makes it comparable to Householder.}\\[2pt]
		&\bullet\ \text{Useful if matrix is sparse or zeros need to be maintained.}
	\end{aligned}
\end{aligned}\)

% SVD
\vspace{10pt}
\(\begin{aligned}
	&\text{SVD}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ \text{Most expensive cost at } \propto\ mn^2 + n^3 \text{ , perhaps 4-10 times or more.}\\[2pt]
		&\bullet\ \text{Robust and reliable.}
	\end{aligned}
\end{aligned}\)


%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Matrix Information
\newpage
\section{Matrix Information}
\vspace{10pt}
\begin{minipage}[t]{.49\textwidth}
	\(\hspace{10pt} \begin{array}[t]{r l}
		\text{Symmetric}:& {S = S^T}\\[5pt]
		\text{Hermitian}:& {H = H^\dagger}
		\\[10pt]
		\text{Orthogonal}:& {QQ^T = Q^T Q = I}\\[5pt]
		\text{Unitary}:& {UU^\dagger = U^\dagger U = I}\\[5pt]
		\text{Normal}:& AA^\dagger = A^\dagger A
		\\[10pt]
		\text{D}
	\end{array}\)
\end{minipage}
\begin{minipage}[t]{.49\textwidth}
	\(\begin{aligned}[t]
		&\bullet\ H = UDU^{-1} \hspace{10pt} \text{\scriptsize(\(D\) is real)}\\[5pt]
		&\bullet\ U = e^{iH} = U_{H} e^{iD} (U_{H})^\dagger\\[5pt]
		&\bullet\ Ax = \lambda x \ , \ \ \text{det}(A) \neq 0 
			\ \Rightarrow \ A^{-1}x = (1/\lambda)x\\[5pt]
		&\bullet\ ATx = \lambda Ty \ \rightarrow \ A \sim B = T^{-1}AT 
	\end{aligned}\)
\end{minipage}

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\vspace{5pt}
\subsection{Error Bound and Conditioning}

% Error Bound for Eigenvalue
\vspace{10pt}\noindent
\(\begin{aligned}[t]
	&A + \Delta A = Q(D + \Delta D)Q^{-1}\\[5pt]
	&\hspace{10pt} \begin{aligned}
			&\bullet\ v 			
				= (\Delta\lambda I - D)^{-1} (\Delta D) v\\[5pt]
			&\bullet\ \begin{aligned}[t]
				\Vert (\Delta\lambda I - D)^{-1} \Vert_2^{-1} &\leq \Vert \Delta D \Vert_2\\[5pt]
				\vert \Delta\lambda - \lambda_i \vert &\leq \Vert Q (\Delta A) Q^{-1} \Vert_2   
			\end{aligned}
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \vert \Delta\lambda - \lambda_i \vert
		\leq \text{cond}(Q)\ \Vert \Delta A \Vert_2 }
\end{aligned}
\hspace{1.25cm}
% Error for Simple Eigenvalues
\begin{aligned}[t]
	&(A + \Delta A)(x + \Delta x) = (\lambda + \Delta\lambda)(x + \Delta x)\\[5pt]
	&\hspace{10pt} \begin{aligned}[t]
			&\bullet\ Ax = \lambda x\ ,\ \ y^H A = \lambda y^H \\
			&\bullet\ \underline{\lambda \text{\scriptsize\ is simple}} \ \Rightarrow \ y^H x \neq 0\ \ (?)\\
			&\bullet\ \text{\scriptsize\(\begin{aligned}[t]
				&y^H \bcancel{Ax} + \cancel{y^H A\Delta x} + y^H (\Delta A)x + y^H \xcancel{(\Delta A) \Delta x} \\
				&\hspace{10pt} \approx y^H \bcancel{\lambda x} + \cancel{y^H \lambda \Delta x} 
					+ y^H (\Delta\lambda) x + y^H \xcancel{(\Delta\lambda) \Delta x}
			\end{aligned}\)}
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \vert \Delta\lambda \vert
		\ \lessapprox\ \frac{\Vert y \Vert_2 \cdot \Vert x \Vert_2}{\vert y^H x \vert}\ \Vert \Delta A \Vert_2
		\ =\ \frac{1}{\cos\theta}\ \Vert \Delta A \Vert_2 }
\end{aligned}\)

% Notes
\vspace{10pt}
\(\begin{aligned}
	&\bullet\ AA^\dagger = A^\dagger A \ \rightarrow \ \text{cond}(A) = 1\\
	&\bullet\ \text{Non-simple (multiple) eigenvalue is complicated:}\\
	&\bullet\ \text{allows } y^Hx = 0 \text{, depends on eigenvalue spacings, vector angles, etc.}\\
	&\bullet\ \text{Balancing can improve conditioning - diagonal rescaling}
\end{aligned}\)

\end{document}