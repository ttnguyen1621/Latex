\documentclass[12pt]{article}
\usepackage[left=.75in, right=.75in, top=1in, bottom = 1in]{geometry}
\usepackage{enumitem, amssymb, amsmath, amsfonts, mathtools, parskip}
\usepackage{cancel}
\usepackage{setspace}
\usepackage[normalem]{ulem}

\newcommand{\rdiag}{\raisebox{0.5ex}{\scalebox{0.7}{$\diagdown$}}}

% \newcommand\italicmath{\mathversion{italic}}
% \DeclareMathVersion{italic}
% \SetSymbolFont{operators}{italic}{OT1}{cmr} {m}{it}
% \SetSymbolFont{letters}  {italic}{OML}{cmm} {m}{it}
% \SetSymbolFont{symbols}  {italic}{OMS}{cmsy}{m}{n}
% \SetMathAlphabet\mathsf{italic}{OT1}{cmss}{m}{sl}
% \SetMathAlphabet\mathit{italic}{OT1}{cmr}{m}{it}

% \newcommand\bitalicmath{\mathversion{bitalic}}
% \DeclareMathVersion{bitalic}
% \SetSymbolFont{operators}{bitalic}{OT1}{cmr} {bx}{it}
% \SetSymbolFont{letters}  {bitalic}{OML}{cmm} {b}{it}
% \SetSymbolFont{symbols}  {bitalic}{OMS}{cmsy}{b}{n}
% \SetMathAlphabet\mathsf{bitalic}{OT1}{cmss}{bx}{sl}
% \SetMathAlphabet\mathit{bitalic}{OT1}{cmr}{bx}{it}

\begin{document}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
\section{System of Linear Equations, \(Ax=b\)}

% P Norm and Condition Number
\subsection{\(p\)-Norm and Condition Number}
\vspace{10pt}\noindent
% Vectors
\begin{minipage}[t]{.49\textwidth}
	% Vector P Norm
	\underline{Vector \(p\)-Norm}: \ \ 
	\fbox{\(\Vert \vec{x} \Vert_p \ =\ \sqrt[\leftroot{-1}\uproot{5}{ p }]{\ \displaystyle \sum_i |x_i|^p\ }\)}

	% 1 and infty norms
	\vspace{10pt}\hspace{10pt}\(\begin{array}{r l}
		{\text{\(1\)-Norm}}: &
			\Vert \vec{x} \Vert_1 \ =\ \displaystyle \sum_i |x_i|\\[20pt]
		{\text{\(\infty\)-Norm}}: &
			\Vert \vec{x} \Vert_\infty \ =\ \max |x_i|
	\end{array}\)

	% Vector Norm Extra Info
	\vspace{20pt}
	\(\setlength{\arraycolsep}{4pt}\begin{array}{l r c c c c}
		\bullet & \Vert x \Vert_1 &\geq&  \Vert x \Vert_2 			&\geq& \Vert x \Vert_\infty\\[15pt]
		\bullet & \Vert x \Vert_1 &\leq&  \sqrt{n}\ \Vert x \Vert_2 	&\leq& \sqrt{n}\ \Vert x \Vert_\infty
	\end{array}\)
\end{minipage}
\begin{minipage}[t]{.49\textwidth}
	\setlength{\parindent}{.5cm}
	% Func/Vector Condition Number
	\noindent
	\underline{Function/Vector Condition Number}:\\[15pt]
	\indent\(\begin{aligned}
		\text{cond}\Big( f(x) \Big) 
		&= \left\vert \frac{[f(\hat{x}) - f(x)] / f(x)}{[\hat{x} - x] / x} \right\vert\\[5pt]
		&= \left\vert \frac{\Delta y / y}{\Delta x / x}\right\vert 
			= \left\vert \frac{y' \cdot \Delta x / y}{\Delta x / x}\right\vert\\[5pt]
		&= \left\vert \frac{x f'(x)}{f(x)}\right\vert
	\end{aligned}\)
\end{minipage}

% Matrices
\vspace{20pt}\noindent
\begin{minipage}[t]{.49\textwidth}
	% Matrix P Norm
	\underline{Matrix \(p\)-Norm}: \ \ 
	\fbox{\(\displaystyle \Vert A \Vert_p \ =\ \max_{x \neq 0} \frac{\Vert Ax \Vert}{\Vert x \Vert}\)}

	% 1 and infty norms
	\vspace{10pt}\hspace{10pt}\(\begin{array}{r l}
		{\text{\(1\)-Norm}}: &
			\displaystyle \Vert A \Vert_1 \ =\ \max_{j}\ \sum_i \vert a_{ij} \vert\\[20pt]
		{\text{\(\infty\)-Norm}}: &
			\displaystyle \Vert A \Vert_\infty \ =\ \max_{i}\ \sum_j \vert a_{ij} \vert
	\end{array}\)

	% Matrix Norm Extra Info
	\vspace{10pt}
	\(\left.\setlength{\arraycolsep}{3pt}\begin{array}{l r c l}
		\bullet& \Vert AB \Vert &\leq& \Vert A \Vert \cdot \Vert B \Vert\\[15pt]
		\bullet& \Vert Ax \Vert &\leq& \Vert A \Vert \cdot \Vert x \Vert
	\end{array}\right\} \begin{gathered}
		\scriptstyle \text{For \(p\)-norms (not} \\[-5pt]
		\scriptstyle \text{necessarily in general)}
	\end{gathered}\)
\end{minipage}
\begin{minipage}[t]{.49\textwidth}
	\setlength{\parindent}{.5cm}
	% Matrix Condition Number
	\noindent
	\underline{Matrix Condition Number}:\\[15pt]
	\indent\(\begin{aligned}
		&\boxed{ \text{cond}_p(A) = \Vert A \Vert_p \cdot \Vert A^{-1} \Vert_p }
					\hspace{20pt} {\scriptstyle(\text{\(\infty\) if singular})}\\[5pt]
		&\hspace{20pt}= \frac{{\displaystyle \max_{x \neq 0}}\ \Vert Ax \Vert_p / \Vert x \Vert_p}
			{{\displaystyle \min_{x \neq 0}}\ \Vert Ax \Vert_p / \Vert x \Vert_p}
			\ =\ \text{cond}_p(\gamma A)
			\ \geq\ 1\\[10pt]
		&\bullet\ \text{Diagonal, } D: \ \text{cond}(D) = \tfrac{\max \vert d_i \vert}{\min \vert d_i \vert}\\[3pt]
		&\bullet\ \begin{aligned}[t]
			&\Vert z \Vert = \Vert A^{-1} y \Vert \ \leq\ \Vert A^{-1} \Vert \cdot \Vert y \Vert\\[3pt]
			&\rightarrow \tfrac{\Vert z \Vert}{\Vert y \Vert} 
				\ \leq\ \max \tfrac{\Vert z \Vert}{\Vert y \Vert} 
				\stackrel{?}{=} \Vert A^{-1} \Vert \hspace{15pt} \text{\scriptsize(optimize)}
		\end{aligned}
	\end{aligned}\)
\end{minipage}

%---------------------------------------------------------------
%---------------------------------------------------------------
%---------------------------------------------------------------
%---------------------------------------------------------------
% Residual, and Error Bounds when Solving Ax = b
\newpage
\subsection{Error Bounds and Residuals}
% Error Bound
\vspace{10pt}\noindent
\underline{Error Bound}:\ \ \(\boxed{ \displaystyle \frac{\Vert \hat{x} - x \Vert}{\Vert x \Vert} 
	\ \lessapprox\ \text{cond}(A)\ \epsilon_\text{mach} } 
	\ \rightarrow \ \begin{minipage}{8cm}
		\scriptsize
		A computed solution is expected to lose about \(\log_{10}(\text{cond}(A))\) digits, so
		the input data must be more accurate to these digits and 
		the working precision must carry more than these digits.
	\end{minipage}\)

% Error Bound for Change in Vector b
\vspace{15pt}\noindent
\(\begin{aligned}[t]
	&A\hat{x} = b + \Delta b = Ax + A\Delta x\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{array}{l c c c}
			\bullet&\Vert b \Vert 			&\leq& \Vert A \Vert \cdot \Vert x \Vert\\[10pt]
			\bullet\ &\Vert \Delta x \Vert 	&\leq& \Vert A^{-1} \Vert \cdot \Vert \Delta b \Vert
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}}
\end{aligned}
\hspace{2cm}
% Residual
\begin{aligned}[t]
	&A\hat{x} + r = b\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{aligned}
			\bullet\ \Vert \Delta x \Vert 
				&= \Vert A^{-1}(A\hat{x} - b) \Vert
				= \Vert -A^{-1} r \Vert \\[5pt]
			&\leq \Vert A^{-1} \Vert \cdot \Vert r \Vert
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert r \Vert}{\Vert A \Vert \cdot \Vert \hat{x} \Vert}}
\end{aligned}\)

% Error Bound for Change in Matrix A
\vspace{20pt}\noindent
\(\begin{aligned}[t]
	&(A + \Delta A)\hat{x} = b\\[5pt]
	&\hspace{10pt} \begin{aligned}[t]
			\bullet\ \Vert \Delta x \Vert &= \Vert - A^{-1} (\Delta A) \hat{x} \Vert\\[10pt]
			&\leq \Vert A^{-1} \Vert \cdot \Vert \Delta A \Vert \cdot \Vert \hat{x} \Vert
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}}
\end{aligned}
\hspace{1.5cm}
% Residual
\begin{aligned}[t]
	&(A + \Delta A)\hat{x} = b\\[5pt]
	&\hspace{10pt} \begin{aligned}
		\bullet\ \Vert r \Vert 
			&= \Vert b - A \hat{x} \Vert
			= \Vert \Delta A \cdot \hat{x} \Vert \\[5pt]
		&\leq \Vert \Delta A \Vert \cdot \Vert \hat{x} \Vert
	\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert r \Vert}{\Vert A \Vert \cdot \Vert \hat{x} \Vert} 
		\leq \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}}
		\ , \ \ \tfrac{\Vert \Delta x \Vert}{\Vert x \Vert} 
		\leq \tfrac{\Vert A^{-1} \Vert \cdot \Vert r \Vert}{\Vert \hat{x} \Vert} 
		\leq \text{cond}(A)\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
\end{aligned}\)

% Calculus Error Bounds
\vspace{20pt}\noindent
\(\begin{aligned}[t]
	&\Big[ A(t) x(t)\ =\ b(t) \Big] 
		= \Big[ \big(A_0 + \Delta A \cdot t \big) x(t)\ =\ b_0 + \Delta b \cdot t \Big]\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{2pt}\begin{array}{l r c l}
			\bullet&x'(t) &=& \frac{b'(t) - A'(t)x(t)}{A(t)} = A^{-1}(t) \Big[ \Delta b - \Delta A\cdot x(t) \Big]\\[10pt]
			\bullet&x(t) &=& x_0 + x'(0)t + \mathcal{O}(t^2)
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \tfrac{\Vert x(t) - x_0 \Vert}{\Vert x_0 \Vert} 
		\leq \text{cond}(A) \left( \tfrac{\Vert \Delta b \Vert}{\Vert b \Vert} 
		+ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert} \right) \vert t \vert 
		+ \mathcal{O}(t^2)}
\end{aligned}\)

%-----------------------------------------------------------------
%-----------------------------------------------------------------
%-----------------------------------------------------------------
%-----------------------------------------------------------------
% LU Decomposition to Solve Linear System of Eq
\newpage
\subsection{Gaussian Elimination with LU/PLU/PLDUQ Decomposition}
% Elementary Elimination Matrices
\vspace{10pt}\noindent
\underline{Elementary Elimination Matrices, \(L_k\)}\\[10pt]
\(\begin{aligned}[t]
	&\\[-20pt]
	&\text{\scriptsize\(\left(\begin{matrix}
		1 & \dots & 0 & 0 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & 1 & 0 & \dots & 0\\
		0 & \dots & \tfrac{-a_{k+1}}{a_k} & 1 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & \tfrac{-a_n}{a_k} & 0 & \dots & 1\\
	\end{matrix}\right) 
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_k\\
		a_{k+1}\\
		\vdots\\
		a_n
	\end{matrix}\right) 
	=
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_k\\
		0\\
		\vdots\\
		0
	\end{matrix}\right)\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\bullet\ a_k\ \text{is the ``pivot''}\\
	&\bullet\ \text{is lower triangular}\\
	&\bullet\ \scriptstyle \forall i \neq j\ \ (L_k^{-1})_{ij}\ =\ - (L_k)_{ij} 
\end{aligned}
\hspace{30pt}
\begin{aligned}[t]
	&\text{Ex}:\\
	&\text{\scriptsize\(\left(\begin{matrix}
			1 & 0 & \dots\\
			-a_1/a_2 & 1 & \dots\\
			\vdots & \vdots & \ddots
		\end{matrix}\right) 
		\left(\begin{matrix}
			a_1\\
			a_2\\
			\vdots
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			a_1\\ 
			0\\
			\vdots
		\end{matrix}\right)\)}\\
\end{aligned}
\) 

% LU/PLU Factorization
\vspace{20pt}\noindent
\underline{LU/PLU Factorization (w/ partial pivoting)}

\vspace{5pt}
\(\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= LU \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(L\) is gen. triang.)}\\[-7pt]
					&\text{\scriptsize(\(U\) is upp. triang.)}
				\end{aligned}\\[5pt]
			L &= (\dots L_2 P_2 L_1 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\} b &= (\dots L_2 P_2 L_1 P_1) Ax \\[5pt]
			L^{-1}b &= (P_1^T L_1^{-1} P_2^T L_2^{-1} \dots)^{-1} Ax \\[5pt]
			&= L^{-1} (L U)x = y
		\end{aligned}\\[5pt]
	&\boxed{\begin{gathered}
			b = Ly\\[-5pt]
			\text{\scriptsize(forw.-sub.)}
		\end{gathered} 
		\ ,\ \ 
		\begin{gathered}
			y = Ux\\[-5pt]
			\text{\scriptsize(back.-sub.)}
		\end{gathered}}
\end{aligned}
\hspace{25pt}
\begin{aligned}[t]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize Permutation matrix, \\ 
			\(P_i\), rowswaps s.t. \\
			\(a_k \neq 0\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3.5cm}
			\scriptsize \(P_i\) rowswaps s.t. \(a_k\) is \\
			largest s.t. \(a_{k+i}/a_k \leq 1\)\\
			for numerical stability/\\
			minimize errors
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3.5cm}
		\scriptsize Pivoting isn't needed if \\
		\(A\) is diag. dom.\\
		\((a_{jj} > \sum_{i, i \neq j} a_{ij})\)
	\end{minipage}\\[5pt]
	&\bullet\ \text{\scriptsize \(A\) can be singular}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= PLU \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(P\) is rowswap permu.)}\\[-7pt]
					&\text{\scriptsize(\(L\) is unit low. triang.)}\\[-7pt]
					&\text{\scriptsize(\(U\) is upp. triang.)}
				\end{aligned}\\[5pt]
			P &= (\dots P_2 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\}b &= (\dots P_2 P_1)Ax\\[5pt]
			P^T b &= (P_1^T P_2^T \dots)^{-1} Ax\\[5pt]
			&= P^T (PLU)x = Ly
		\end{aligned}\\[10pt]
	&\boxed{P^T b = Ly\ ,\ \ y = Ux}
\end{aligned}\)

% LDU Diagonal Uniquness
\vspace{20pt}
\(\begin{aligned}[t]
	&\boxed{P^T A = LDU \hspace{20pt} \text{\scriptsize(D is diag.)}}\\[10pt]
	&\bullet\ \scriptstyle LDU\ \text{is unique up to } D\\
	&\bullet\ \scriptstyle LDU\ \text{is unique if } L/U \text{ are unit low./upp. diag., resp.}
\end{aligned}
% PAQ Full pivoting
\hspace{2cm}
\begin{aligned}[t]
	&\boxed{P^T A Q^T = LDU \hspace{20pt} \begin{aligned}
			&\text{\scriptsize(P is permu. for rows)}\\[-7pt]
			&\text{\scriptsize(Q is permu. for cols.)}
		\end{aligned}}\\[5pt]
	&\bullet\ \text{\scriptsize ``Complete pivoting'' search for largest \(a_k\)}\\[-3pt]
	&\bullet\ \text{\scriptsize Would be most numerically stable}\\[-3pt]
	&\bullet\ \text{\scriptsize Expensive, so not really used}
\end{aligned}\)

% Erro Bounds
\vspace{20pt}\noindent
\underline{Error Bound}:\ \(\tfrac{\Vert r \Vert}{\Vert A \Vert \Vert x \Vert} 
	\ \leq\ \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
	\ \leq\ \begin{gathered}[t]
		\rho\ n^2 \epsilon_\text{mach}\\[-5pt]
		\text{\scriptsize(Wilkinson)}
	\end{gathered}
	\ \sim\ \begin{gathered}[t]
		n \epsilon_\text{mach}\\[-5pt]
		\text{\scriptsize (usually)}
	\end{gathered}
	\hspace{40pt} \begin{minipage}{6cm}
		\scriptsize
		(growth factor, \(\rho\), is the largest entry at any point during factorization - usually at \(U\) - \\ 
		divided by the largest entry of A)
	\end{minipage}\)

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
% Gaussian-Jordan
\newpage
\subsection{Gaussian-Jordan with MD Decomposition}
% Elementary Elimination Matrices
\vspace{10pt}\noindent
\underline{Elementary Elimination Matrices, \(M_k\)}\\[10pt]
\(\begin{aligned}[t]
	&\\[-20pt]
	&\text{\scriptsize\(\left(\begin{matrix}
		1 & \dots & \tfrac{-a_1}{a_k} & 0 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & 1 & 0 & \dots & 0\\
		0 & \dots & \tfrac{-a_{k+1}}{a_k} & 1 & \dots & 0\\
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
		0 & \dots & \tfrac{-a_n}{a_k} & 0 & \dots & 1\\
	\end{matrix}\right) 
	\left(\begin{matrix}
		a_1\\
		\vdots\\
		a_{k-1}\\
		a_k\\
		a_{k+1}\\
		\vdots\\
		a_n
	\end{matrix}\right) 
	=
	\left(\begin{matrix}
		0\\
		\vdots\\
		0\\
		a_k\\
		0\\
		\vdots\\
		0
	\end{matrix}\right)\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}[t]
	&\bullet\ a_k\ \text{is the ``pivot''}\\
	&\bullet\ \scriptstyle \forall i \neq j\ \ (M_k^{-1})_{ij}\ =\ - (M_k)_{ij} 
\end{aligned}
\) 

% MD Factorization
\vspace{20pt}\noindent
\underline{MD Factorization (w/ partial pivoting)}

\vspace{5pt}
\(\begin{aligned}[t]
	&\boxed{\begin{aligned}[t]
			A &= MD \hspace{20pt} \begin{aligned}
					&\text{\scriptsize(\(M\) is elem. elim.)}\\[-7pt]
					&\text{\scriptsize(\(D\) is diag.)}
				\end{aligned}\\[5pt]
			M &= (\dots M_2 P_2 M_1 P_1)^{-1}
		\end{aligned}}\\[10pt]
	&\begin{aligned}[t]
			\{\dots\} b &= (\dots M_2 P_2 M_1 P_1) Ax \\[5pt]
			M^{-1}b &= (P_1^T M_1^{-1} P_2^T M_2^{-1} \dots)^{-1} Ax \\[5pt]
			&= M^{-1} (MD)x = y
		\end{aligned}\\[5pt]
	&\boxed{\begin{gathered}
			M^{-1}b=y
		\end{gathered} 
		\ ,\ \ 
		\begin{gathered}[t]
			y = Dx\\[-5pt]
			\text{\scriptsize(division)}
		\end{gathered}}
\end{aligned}
\hspace{25pt}
\begin{aligned}[t]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize Permutation matrix, \\ 
			\(P_i\), rowswaps s.t. \\
			\(a_k \neq 0\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize \(P_i\) rowswaps cannot \\
			ensure numerical \\
			stability \((\leq 1)\)
		\end{minipage}\\[5pt]
	&\bullet\ \begin{minipage}[t]{3cm}
			\scriptsize 
			Division is \(\mathcal{O}(n)\), \\
			so may be useful for\\
			parallel comps.
		\end{minipage}\\[5pt]
	&\bullet\ \text{\scriptsize Can also find \(A^{-1}\)}
\end{aligned}
\hspace{30pt}
\text{\scriptsize\(
	\begin{aligned}[t]
		&\underline{\text{Finding } A^{-1}}\\[5pt]
		&D^{-1} M^{-1} (A | I) = (I | A^{-1})\\[5pt]
		&\hspace{10pt} \begin{aligned}[t]
			&= D^{-1} M^{-1} 
				\left[\left.\begin{matrix}
					a_{11} & \dots \\
					\vdots & a_{nn}	
				\end{matrix}\right|
				\begin{matrix}
					1 & 0\\
					0 & 1
				\end{matrix}\right]\\[5pt]
			&= \left[\begin{matrix}
					1 & 0\\
					0 & 1
				\end{matrix}
				\left|\begin{matrix}
					a'_{11} & \dots \\
					\vdots & a'_{nn}	
				\end{matrix}\right.\right]
		\end{aligned}
	\end{aligned}\)}
\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Symmetric
\vspace{15pt}
\subsection{Symmetric Matrices}
% Positive Definite Def
\vspace{5pt}
\underline{Positive Definite}:\ \ \(\boxed{x^T Ax \geq 0}\) \\[15pt]
% Cholesky Factorization
\underline{Cholesky Factorization for Sym., Pos. Def.}:\ \ \(\boxed{A = LL^T = LDL^T}\)\\[15pt]
\text{\scriptsize\(\begin{aligned}
	&\left(\begin{matrix}
			a_{11} & a_{21} & a_{31} & \dots\\
			a_{21} & a_{22} & a_{32} & \dots\\
			a_{31} & a_{32} & a_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			l_{11} & 0 		& 0 	 & \dots\\
			l_{21} & l_{22} & 0 	 & \dots\\
			l_{31} & l_{32} & l_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)
		\left(\begin{matrix}
			l_{11} 	& l_{21} 	& l_{31} & \dots\\
			0 		& l_{22} 	& l_{32} & \dots\\
			0 		& 0 		& l_{33} & \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)  
		= 
		\left(\begin{matrix}
			l_{11}^2 		& \dots 						& \dots 							& \dots\\
			l_{21} l_{11} 	& l_{21}^2 + l_{22}^2 			& \dots 							& \dots\\
			l_{31} l_{11} 	& l_{31}l_{21} + l_{32}l_{22} 	& l_{31}^2 + l_{32}^2 + l_{33}^2 	& \dots\\
			\vdots & \vdots & \vdots & \ddots
		\end{matrix}\right)\\[5pt]
	% Notes
	&\begin{aligned}[t]
			&\bullet\ \text{Pivoting not needed}\\
			&\bullet\ \text{Well defined (always works)}\\
		\end{aligned}
		\hspace{2cm}
		\begin{aligned}[t]
			&\bullet\ \text{Only lower triangle needed for storage}\\
			&\bullet\ A = LDL^T \text{ is sometimes useful, where \(D\) is diag.}
		\end{aligned}
\end{aligned}\)}

% Symmetrix Indefinite Matrices
\vspace{10pt}
\underline{Symmetric Indefinite Matrices}\\[5pt]
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{Pivoting Needed}:\ \boxed{PAP^T = LDL^T}\\
	&\bullet\ \text{Ideally, \(D\) is diag., but if not possible, 
		then \(D\) is tridiag. (Aasen) or 1x1/2x2 block diag. (Bunch, Parlett, Kaufmann, etc.)}
\end{aligned}\)}

%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
\newpage
% Banded
\subsection{Banded Matrices}
\vspace{5pt}
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{Similar to normal Gaussian Elim., but less work since more zeroes}\\
	&\bullet\ \text{Pivoting means bandwidth will expand no more than double}\\
	&\bullet\ \text{Only \(\mathcal{O}(\beta n)\) storage needed} 
\end{aligned}\)}

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Rank 1 Update with Sherman-Morrison
\vspace{10pt}
\subsection{Rank-1 Update with Sherman-Morrison}
\vspace{5pt}
\(
	\begin{aligned}[t]
		\Tilde{A} \tilde{x} = b &= (A - uv^T) \tilde{x} \\[10pt]
		\ \rightarrow \ \tilde{x} &= \Tilde{A}^{-1}b 
	\end{aligned}
	\hspace{10pt}
	\rule[-62pt]{.5pt}{80pt}
	\hspace{10pt}
	\boxed{
	\setlength{\arraycolsep}{3pt}\begin{array}[t]{r c r c r c l}
		\Tilde{A}^{-1} 	&=& (A-uv^T)^{-1} 	&=& A^{-1} 		&+& \tfrac{A^{-1} u}{1 - v^T (A^{-1}u)}\ v^T A^{-1}\\[10pt]
		\Tilde{A}^{-1}b &=& \tilde{x} 				&=& (A^{-1} b) 	&+& \tfrac{A^{-1} u}{1 - v^T (A^{-1}u)}\ v^T (A^{-1} b)\\[10pt]
		& & & & 										x 			&+& \tfrac{y}{1 - v^T y}\ v^T x
	\end{array} }
\)

% General Woodbury Formula
\vspace{10pt}
\underline{General Woodbury Formula}: \ \ 
	\(\boxed{ (A-UV^T)^{-1} = A^{-1} + (A^{-1}U)(I - V^T A^{-1} U)^{-1}\ v^T A^{-1} }\) \\[10pt]
% Notes
\text{\scriptsize\(\begin{aligned}
	&\bullet\ \text{\(U\) and \(V\) are general \(n \times k\) matrices}\\
	&\bullet\ \text{No guarantee of numerical stability, so caution is needed}
\end{aligned}\)}

%------------------------------------------------------------------
%------------------------------------------------------------------
% Complexity
\vspace{10pt}
\subsection{Complexity}
\vspace{5pt}
\(\setlength{\arraycolsep}{3pt}\begin{array}{r c c c c c c l}
	\text{Explicit Inversion}: 
		&\begin{aligned}
				\scriptstyle LUA^{-1} &\scriptstyle\ =\ I\\[-5pt]
				\scriptstyle D^{-1}M^{-1} I &\scriptstyle\ =\ A^{-1}
			\end{aligned}
		&\rightarrow
		&\mathcal{O}(n^3)
		&\ ,\ 
		&A^{-1}b = x
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[15pt]
	\text{Gaussian Elimination}: 
		&A = LU
		&\rightarrow
		&\mathcal{O}(n^3/3)
		&\ ,\ 
		&LUx = b 
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[5pt]
	\text{Gaussian-Jordan}: 
		&A = MD
		&\rightarrow
		&\mathcal{O}(n^3/2)
		&\ ,\ 
		&MDx = b
		&\rightarrow
		&\mathcal{O}(n)
		\\[5pt]
	\text{Symmetric}:
		&\begin{aligned}
				\scriptstyle A &\scriptstyle\ =\ LL^T\\[-5pt]
				\scriptstyle PAP^T &\scriptstyle\ =\ LDL^T
			\end{aligned}
		&\rightarrow
		&\mathcal{O}(n^3/6)
		&\ ,\ 
		&LL^Tx = b
		&\rightarrow
		&\mathcal{O}(n^2)
		\\[15pt]		
	\text{Banded}:
		&A_\beta = LU
		&\rightarrow
		&\mathcal{O}(\beta^2 n)
		&\ ,\ 
		&LUx = b
		&\rightarrow
		&\mathcal{O}(\beta n)
		\\[10pt]	
	\text{Sherman-Woodbury}:
		&\Tilde{A} = A-uv^T
		&\rightarrow
		&\mathcal{O}(n^2)
		&\ ,\ 
		&\tilde{x} = \Tilde{A}b
		&\rightarrow
		&\mathcal{O}(n^2)
\end{array}\)

%------------------------------------------------------------------
%-----------------------------------------------------------------
\vspace{20pt}
% Diagonal Scaling
\begin{minipage}[t]{.45\textwidth}
	\subsection{Diagonal Scaling}
	% Ill conditioned
	\vspace{5pt}
	Ill-conditioned\\[5pt]
	\text{\scriptsize\(\begin{aligned}[t]
		\left(\begin{matrix}
			1 & 0 \\
			0 & \epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			x_1 \\ x_2
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			1 \\ \epsilon
		\end{matrix}\right)
	\end{aligned}\)}\\[15pt]	
	% Well conditioned
	Well-conditioned\\[5pt]
	\text{\scriptsize\(\begin{aligned}[t]
		\left(\begin{matrix}
			1 & 0 \\
			0 & 1/\epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			1 & 0 \\
			0 & \epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			x_1 \\ x_2
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			1 & 0 \\
			0 & 1/\epsilon
		\end{matrix}\right)
		\left(\begin{matrix}
			1 \\ \epsilon
		\end{matrix}\right)
	\end{aligned}\)}	
	
	% Notes
	\vspace{10pt}
	\(\begin{aligned}
		\text{\scriptsize \(\bullet\ \) No general way to correct poor scaling}
	\end{aligned}\)
\end{minipage}
%------------------------------------------------------------------
%-----------------------------------------------------------------
% Iterative Refinement
\begin{minipage}[t]{.53\textwidth}
	\subsection{Iterative Refinement}
	\vspace{5pt}
	\text{\scriptsize\(\arraycolsep=3pt \begin{array}{r c l c l}
		r_0 &=& b - Ax_0 = A \Delta x_0\\
		r_1 &=& b - A(x_0 + \Delta x_0) &=& b - Ax_1 = A \Delta x_1\\
		r_2 &=& b - A(x_1 + \Delta x_1) &=& b - Ax_2 = A \Delta x_2\\[5pt]
		\cline{1-3}
		\multicolumn{1}{|r}{\rule[-7pt]{0pt}{20pt} x} &=& \multicolumn{1}{l|}{\displaystyle x_0\ +\ \lim_{n=0}^{\infty} \Delta x_n}
			& & \text{\scriptsize(terminate when \(r_n\) is small enough)}\\
		\cline{1-3}
	\end{array}\)}

	% Notes
	\vspace{10pt}
	\text{\scriptsize\(\begin{aligned}
		&\bullet\ \text{Double storage needed to hold original matrix}\\
		&\bullet\ \text{\(r_n\) usually must be computed with higher precision than \(x_n\)}\\
		&\bullet\ \text{Useful for badly scaled systmes, or making unstable systems stable}\\
		&\bullet\ \text{If \(x_n\) is not accurate, \(r_n\) might not need better accuracy}
	\end{aligned}\)}
\end{minipage}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Least r Linear Regression/Fit
\newpage
\section{Least \(\Vert r \Vert\) Linear Regression/Fit, \(Ax + r = b\)}

% Assumptions
\(\begin{aligned}[t]
	&\bullet\ A = A_{m \times n} \hspace{10pt} \boxed{ \text{\scriptsize\((m > n\),\ \ underdetermined\()\)} }\\[5pt]
	&\bullet\ \Vert r\text{\scriptsize\((y=Ax)\)} \Vert \text{ is cont. \& coer.} 
	\ \rightarrow \ \exists \Vert r\text{\scriptsize\((y)\)} \Vert_\text{min}
\end{aligned}
\hfill
\begin{aligned}[t]
	&\bullet \ r\text{\scriptsize\((y)\)} \text{ is strictly convex} \ \rightarrow \ y=Ax \text{ is unique}\\[5pt]
	&\bullet\ \boxed{ \begin{gathered}[t]
			\text{rank}(A) = n\\[-3pt]
			\text{\scriptsize(full column rank)}
		\end{gathered} }
		\ \Rightarrow
		\arraycolsep=3pt \begin{array}[t]{r c c c c}
			A(x_1 - x_2) &=& 0 & &			\text{\scriptsize(unique \(x\))}\\
			(x_1 - x_2) &=& 0 &\rightarrow& x_1 = x_2 
		\end{array}
\end{aligned}\)

% Example: Vandermonde Matrix 
\vspace{5pt}
\underline{Example - Vandermonde Matrix, \(A\)}:\\[10pt]
\(
	Ax = \text{\scriptsize \(
		\left(\begin{matrix}
			- \vec{f}(t_1) -\\ 
			\vdots\\ 
			- \vec{f}(t_m) -
		\end{matrix}\right) 
		\left(\begin{matrix}
			|\\
			\vec{x}\\
			| 
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			y(t_1)\\ 
			\vdots\\ 
			y(t_m)
		\end{matrix}\right) 
		=
		\left(\begin{matrix}
			|\\ 
			\vec{y}\\ 
			|
		\end{matrix}\right) 
	\)} 
	= (x^T A^T)^T 
	\hspace{7pt} , \hspace{15pt}
	y(t) = \sum_{i=1}^n x_i f_i(t) = \vec{x} \cdot \vec{f}
\)

\vspace{15pt}
\(
	% Vector b decomp.
	\begin{aligned}
		&\text{\underline{Decompose \(b\)}}:\\[5pt]
		&{\arraycolsep=3pt \begin{array}{r c c c c}
				b &=& Ax &+& r \\[3pt]
				&=& y 	&+& r \\[3pt]
				&=& Pb 	&+& P_\perp b
			\end{array}}
	\end{aligned}
	\hspace{.5cm}
	\rule[-35pt]{.5pt}{75pt}
	\hspace{.5cm}
	% Orthogonal Projector Matrix
	\begin{aligned}
		&\underline{\text{Projector of A, \(P\)}}\\[10pt]
		&\begin{array}{r l}
			% Projector Def
			\text{Projector}: & \begin{gathered}[t]
					P^2 = P\\[-3pt]
					\text{\scriptsize(Idempotent)}
				\end{gathered}
				\ \rightarrow\ 
				\begin{gathered}[t]
					PA = A\\[-3pt]
					\text{\scriptsize(Projector of A)}
				\end{gathered}\\[15pt]
			% Ortho. Proj.
			\underline{\text{Orthogonal Projector}}: & P^T = P \ \rightarrow\ P_\perp A = (I - P)A = 0
		\end{array}
	\end{aligned}
\)

% Minimize residual, r
\vspace{15pt}
\underline{Minimize residual, \(r\)}:\\[10pt]
\(\begin{aligned}[t]
	\nabla \Vert r \Vert_2^2 &= 0 \hspace{20pt} 
		\text{\scriptsize\(\left(\tfrac{\partial r^2}{\partial x_i} = 0 \right)\)}\\[5pt]
	&= \nabla\left[ (b-Ax)^T(b-Ax) \right]\\[5pt]
	&= \nabla\left( b^T b - 2 x^T A^T b + x^T A^T Ax \right)\\[5pt]
	0 &= 2 A^T Ax - 2 A^T b\\
	&\downarrow\\
	A^TAx &= A^T b \hspace{10pt} \text{\scriptsize(Solvable with Cholesky)}
\end{aligned}
\hspace{2cm}
\begin{aligned}[t]
	\Vert r \Vert_2^2 &= \Vert Pr + P_\perp r \Vert_2^2 = \Vert b - Ax \Vert^2\\
	&= \Vert Pr \Vert^2 + \Vert P_\perp r \Vert^2\\
	&= \cancel{ \Vert Pb - Ax \Vert_2^2 } + \Vert P_\perp b \Vert_2^2 \\
	&\downarrow\\
	Ax &= Pb\\
	A^T Ax &= A^T P b = (P^T A)^T b \\
	A^T Ax &= A^T b \hspace{10pt} \text{\scriptsize(System of Normal Equations)}
\end{aligned}\)

% Cross Product Matrix
\vspace{10pt}
\begin{minipage}[t]{.49\textwidth}
	\underline{Cross-Product Matrix of \(A\)}:\ \ \(\boxed{A^T A}\)\\[10pt]
	\(\begin{array}{r l}
		\text{Symmetric}: & (A^T A)^T = A^T A\\[10pt]
		\text{Pos. Def.}: & \begin{aligned}[t]
				&\text{rank}(A) = n\\
				&\rightarrow \ \begin{aligned}[t]
						&\big\langle x \big| A^T Ax \big\rangle = x^T A^T A x\\
						&= (Ax)^T (Ax) \\
						&= \Vert Ax \Vert^2 \geq 0
					\end{aligned} \\
			\end{aligned} \\[2.2cm]
		\text{Nonsingular}: & \begin{aligned}[t]
				&A^T A x = 0 \\
				&\rightarrow\ \Vert Ax \Vert^2 = 0 = Ax \\
				&\rightarrow\ (x = 0)
			\end{aligned} 
	\end{array}\)	
\end{minipage}
% System of Normal Equations
\hfill
\begin{minipage}[t]{.49\textwidth}
	\underline{System of Normal Equations}: \ \ \(\boxed{A^T Ax = A^T b}\)\\[20pt]
	\underline{Pseudoinverse, \(A^+\)}\\[10pt]
	\(
		\boxed{ \begin{aligned}
			x &= (A^T A)^{-1} A^T b \\[3pt]
			&\equiv A^+ b
		\end{aligned} }
		\ \ \ \rightarrow \ \ \
		\boxed{ \begin{aligned}
			A^+ &\equiv (A^T A)^{-1} A^T \\
			A^+ A &= I
		\end{aligned} }
	\)\\[20pt]
	\underline{Ortho. Proj., \(P\)}\\[10pt]
	\(
		\boxed{ \begin{aligned}[t]
			Ax &= A (A^T A)^{-1} A^T b \\
			&= Pb
		\end{aligned} }
		\ \ \ \rightarrow \ \ \
		\boxed{ \begin{aligned}[t]
			P &= A (A^T A)^{-1} A^T \\
			&= A A^+ 
		\end{aligned} }
	\)
\end{minipage}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
% Issues with System of Normal Equations
\newpage
\underline{System of Normal Equations Issues}:\\[10pt]
\(\begin{aligned}
	&\bullet\ \text{Info can be lost forming \(A^TA\), e.g, \ } 
		A = \text{\scriptsize\(\left(\begin{matrix}
			1 & 0 \\ 
			\epsilon & 0 \\ 
			0 & \epsilon
		\end{matrix}\right)\)} 
		\ \rightarrow \ 
		A^T A = \text{\scriptsize\(\left(\begin{matrix}
			1 + \epsilon^2 & 1 \\ 
			1 & 1 + \epsilon^2
		\end{matrix}\right)\)} 
		\ \approx \ \text{\scriptsize\(\left(\begin{matrix}
			1 & 1 \\ 
			1 & 1
		\end{matrix}\right)\)} 
		\hspace{10pt} \text{\scriptsize(singular)} \\[.3cm]
	&\bullet\ \text{System of Normal Equations:}\ \ \boxed{ \text{cond}(A^T A) = [\text{cond}(A)]^2 }
\end{aligned}\)

%------------------------------------------------------------------------
%------------------------------------------------------------------------
% Residual, and Error Bounds when Solving Ax = b
\vspace{15pt}
\subsection{Error Bounds and Residuals}
% Error Bound
\vspace{10pt}\noindent
\underline{Error Bound}:\ \ \(\boxed{ \displaystyle \frac{\Vert \Delta x \Vert}{\Vert x \Vert} 
	\ \lessapprox\ \text{cond}(A)\ \epsilon_\text{mach} } 
	\ \rightarrow \ \begin{minipage}{8cm}
		\scriptsize
		A computed solution is expected to lose about \(\log_{10}(\text{cond}(A))\) digits, so
		the input data must be more accurate to these digits and 
		the working precision must carry more than these digits.
	\end{minipage}\)

% Norm and Conditioning
\vspace{10pt}
\underline{Norm and Conditioning}:\\[10pt]
% Norm
\begin{minipage}[t]{.49\textwidth}
	\(\boxed{ \Vert A \Vert = \max_{x \neq 0} \left( \frac{\Vert Ax \Vert}{\Vert x \Vert} 
	= \frac{\Vert AA^+ b \Vert}{\Vert A^+ b \Vert} \right) }\)
\end{minipage}
% Cond(A)
\begin{minipage}[t]{.49\textwidth}
	\(\boxed{ \text{cond}(A) = \begin{cases}
		\Vert A \Vert_2 \cdot \Vert A^+ \Vert_2 & \hspace{10pt} \text{rank}(A) = n\\
		\infty & \hspace{10pt} \text{rank}(A) < n
	\end{cases} }\)
\end{minipage}

% Error Bound for Change in Vector b
\vspace{15pt}\noindent
\(\begin{aligned}[t]
	&A^T A(x + \Delta x) = A^T A(b + \Delta b)\\[5pt]
	&\hspace{10pt} \setlength{\arraycolsep}{3pt}\begin{array}{l c c c}
			\bullet\ &\Vert \Delta x \Vert 	&\leq& \Vert A^+ \Vert \cdot \Vert \Delta b \Vert
		\end{array}\\[10pt]
	&\rightarrow\ \boxed{ \begin{aligned}[t]
			\tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
				&\leq \left( \text{cond}(A) \tfrac{\Vert b \Vert}{\Vert Ax \Vert} \right) 
				\tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}\\[5pt]
			&= \left( \text{cond}(A) \tfrac{1}{\cos \theta} \right) 
				\tfrac{\Vert \Delta b \Vert}{\Vert b \Vert}
		\end{aligned} }\\[20pt]
	% Other notes
	&\begin{aligned}
			&\bullet\ \text{\scriptsize Cond. number is a func. of cond\((A)\) and \(b\)}\\
			&\bullet\ \text{\scriptsize \(Pb \approx 0\) or \(\theta\approx 90^\circ\) is highly sensitive}
		\end{aligned}
\end{aligned}
% Residual
\hfill
\begin{aligned}[t]
	&(A + \Delta A)^T (A + \Delta A)(x + \Delta x) = (A + \Delta A)^T b\\[10pt]
	&\hspace{10pt} \begin{aligned}
			&\bullet\ \text{\scriptsize\(\begin{aligned}[t]
					&\cancel{A^T A x} + A^T \Delta A x + (\Delta A)^T A x + \bcancel{ (\Delta A)^T \Delta A x }\\
					&\ \ + A^T A \Delta x 
						+ \bcancel{ A^T \Delta A \Delta x} 
						+ \bcancel{ (\Delta A)^T A \Delta x } 
						+ \bcancel{ (\Delta A)^T \Delta A \Delta x }
				\end{aligned}\ \ =\ \ \cancel{A^T b} + (\Delta A)^T b \)}\\[10pt]
			&\bullet\ \begin{aligned}[t]
					\Vert \Delta x \Vert 
						&= \Vert (A^T A)^{-1}(\Delta A)^T r - A^+ \Delta A x \Vert\\[5pt]
					&\leq \Vert (A^T A)^{-1} \Vert \cdot \Vert \Delta A \Vert \cdot \Vert r \Vert 
						+ \Vert A^+ \Vert \cdot \Vert \Delta A \Vert \cdot \Vert x \Vert
				\end{aligned}
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \begin{aligned}[t]
			\tfrac{\Vert \Delta x \Vert}{\Vert \hat{x} \Vert} 
				&\leq \left( \scriptstyle [\text{cond}(A)]^2 \tfrac{\Vert r \Vert}{\Vert Ax \Vert} 
				+ \text{cond}(A) \right) \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}\\[5pt]
			&= \big( \scriptstyle [\text{cond}(A)]^2 \tan \theta
				+ \text{cond}(A) \big) \tfrac{\Vert \Delta A \Vert}{\Vert A \Vert}
		\end{aligned} }
\end{aligned}\)

%--------------------------------------------------------------
%--------------------------------------------------------------
% Augmented Matrix
\vspace{15pt}
\subsection{Solving \(A^TAx = A^Tb\) with an Augmented Matrix}
\vspace{10pt}
% Equations
\[
	\arraycolsep=1pt
	\begin{array}{r c c c c}
		r &+& Ax &\ =\ & b\\[5pt]
		A^Tr & & &\ =\ & 0
	\end{array}
	\ \ \Rightarrow \ \
	\left(\begin{matrix}
		I & A\\
		A^T & 0
	\end{matrix}\right)
	\left(\begin{matrix}
		r \\
		x
	\end{matrix}\right)
	= 
	\left(\begin{matrix}
		b\\
		0
	\end{matrix}\right)
	\ \ \Rightarrow \ \
	\left(\begin{matrix}
		\alpha I & A\\
		A^T & 0
	\end{matrix}\right)
	\left(\begin{matrix}
		r/\alpha \\
		x
	\end{matrix}\right)
	= 
	\left(\begin{matrix}
		b\\
		0
	\end{matrix}\right)
\]

% Notes
\vspace{5pt}
\(\begin{aligned}
	&\bullet\ \text{Solvable with \(LU\) Decomp or Symm. Pos. Def. Methods} \\
	&\bullet\ \alpha \text{ "controls the relative weights of the two subsystems in chooing pivots from either"}\\
	&\bullet\ \alpha = \max a_{ij}/1000 \hspace{20pt} \text{\scriptsize(rule of thumb)}\\
	&\bullet\ \text{MATLAB uses it for large, sparse systems}
\end{aligned}\)

%------------------------------------------------------------
%------------------------------------------------------------
%------------------------------------------------------------
%------------------------------------------------------------
% QR Decomposition
\newpage
\subsection{QR Decomposition}
% Motivation
\vspace{5pt}
\underline{Motivation}: \ \(
	\begin{gathered}
		Q^T A = \text{\scriptsize\(\left(\begin{matrix}
				R \\ 
				0
			\end{matrix}\right)\)}\\[5pt]
		\text{\scriptsize(\(R\) is upp. triag.)}
	\end{gathered}
	\ \ \rightarrow \ \
	\arraycolsep=2pt
	\begin{array}{r c c c l}
		Q^T Ax &+& Q^T r &=& Q^T b \\[5pt]
		\text{\scriptsize\(\left(\begin{matrix}
				Rx \\ 
				0
			\end{matrix}\right)\)}
			&+& 
			\text{\scriptsize\(\left(\begin{matrix}
				r_1' \\[2pt]
				r_2'
			\end{matrix}\right)\)}
			&=&
			\text{\scriptsize\(\left(\begin{matrix}
				b_1' \\[2pt]
				b_2'
			\end{matrix}\right)\)}
	\end{array}
	\ \ \rightarrow \ \
	\begin{aligned}
		\Vert r' \Vert^2 &= \cancel{ \Vert b_1' - Rx \Vert^2 } + \Vert b_2' \Vert^2 \\
		&\downarrow\\
		\Aboxed{Rx &= b_1'} \ , \ r' = \text{\scriptsize\(\left(\begin{matrix}
				0 \\ 
				b_2'
			\end{matrix}\right)\)} 
			\hspace{12pt} \text{\scriptsize(solve with back-sub)}
	\end{aligned}
\)

% QR Decomp
\vspace{5pt}
\(\hspace{20pt} \begin{aligned}[t]
	&\underline{\text{Orthogonal Matrix, \(Q\)}}\\[2pt]
	&\boxed{ Q^T Q = QQ^T = I }
\end{aligned} 
\hfill
\begin{aligned}[t]
	&\underline{\text{QR Factorization}}\\[2pt]
	&\boxed{ A = Q \left(\begin{matrix}
			R\\ 0
		\end{matrix}\right) }
\end{aligned}
\hfill
\begin{aligned}[t]
	&\underline{\text{Reduced QR Factorization}}\\[2pt]
	&\boxed{ A = Q \left(\begin{matrix}
			R\\ 0
		\end{matrix}\right)
		= 
		\bigg(\begin{matrix}
			Q_\parallel & Q_\perp
		\end{matrix}\bigg)
		\left(\begin{matrix}
			R\\ 0
		\end{matrix}\right)
		= Q_\parallel R }
\end{aligned}\)

% Interpretations
\vspace{15pt}
% 1. Q^T is a rotation
\begin{minipage}[t]{.49\textwidth}
	\textbf{1.} \hspace{5pt} \begin{minipage}[t]{7cm}
		\textbf{\(Q^T\) is a span\((A)\) Plane Rotation}\\[3pt]
		\textbf{through \(\mathbb{R}^m\) to span\(\big( [R\ \ 0]^T \big)\)}
	\end{minipage}

	% Orthogonal Matrix Rotates/Reflects and preserves the norm
	\vspace{10pt}
	\fbox{
		\(\begin{aligned}
			&\text{2-norm Preserved {\scriptsize(\(Q\) is a rotation/reflection)}}\\[5pt]
			&\bullet\ \begin{aligned}[t]
					\Vert Q v \Vert^2 &= \langle v | Q^TQv \rangle = \Vert v \Vert^2\\
					\Vert Q^T v \Vert^2 &= \langle v | QQ^Tv \rangle = \Vert v \Vert^2
				\end{aligned}\\[5pt]
		\end{aligned}\)
	}

	% Describe how Q^T transforms A to R
	\vspace{10pt}
	\(\begin{gathered}
		\begin{array}{l c l}
			\bullet\ Q^T = H_n\dots H_1 &\ &\bullet\ H_i^T H_i = H_i H_i^T = I \\[3pt]
			\bullet\ A = [a_1\ \dots\ a_n] &\ &\bullet\ I_n = [e_1\ \dots\ e_n]
		\end{array}\\[10pt]
		\arraycolsep=3pt \begin{array}{r c l}
				H_1 a_1 &=& \alpha_1 e_1 
					\hspace{15pt}
					\text{\scriptsize\(\left( \Vert a_1 \Vert = \vert \alpha_1 \vert \right)\)}
					\\[10pt]
				H_i\dots H_1 a_i &=& \begin{gathered}[t]
					{\scriptstyle \sum}^i_j\ c_j e_j \ =\ H_n\dots H_1 a_i\\[5pt]
					\text{\scriptsize\(\left( \Vert a_i \Vert^2 = \vert \alpha_1 \vert^2 
						= {\scriptstyle \sum}_j^i\ c_j^2 \right)\)}
				\end{gathered}
					\\[27pt]
				\langle r | a_i \rangle &=& 0
					\hspace{27pt}
					\text{\scriptsize\((1 \leq i \leq n)\)}
					\\[7pt]
				\langle H_i \dots H_1r | e_j \rangle &=& 0 
					\hspace{27pt}
					\text{\scriptsize\((1 \leq j \leq i)\)}
			\end{array}
	\end{gathered}\)

	% Conclusion
	\vspace{15pt}
	\(\rightarrow\) \fbox{ 
		\begin{minipage}[t]{7.5cm}
			\(Q^TA\) rotates \(A\) until the column vectors are aligned with certain axes described above
		\end{minipage}
	}
\end{minipage}
\vline
% 2. A is a linear sum of Q
\hfill
\begin{minipage}[t]{.48\textwidth}
	\textbf{2.} \hspace{5pt} \begin{minipage}[t]{7cm}
		\textbf{\(A\) is a Lin. Sum of \(Q_\parallel\)'s Orthogonal}\\[3pt]
		\textbf{Column Vectors Given by \(R\)}
	\end{minipage}

	% System of Orthogonal Equations
	\vspace{15pt}
	\(\begin{aligned}
		&\Big\{ Q_\parallel = Q_{m \times n}\ \Big|\ \text{span}(Q_\parallel) = \text{span}(A) \Big\} \\[5pt]
		&\rightarrow\ Q^+ = (Q^T Q)^{-1} Q^T = Q^T\\[5pt]
		&\rightarrow\ P = Q_\parallel Q_\parallel^T\\[5pt]
		&\rightarrow\ \begin{aligned}[t]
				Q_\parallel^T Ax &= Q_\parallel^T Pb = \cancel{Q_\parallel^T Q_\parallel} Q_\parallel^T b \\[4pt]
				&= Q_\parallel^T b \hspace{10pt} \text{\scriptsize(System of Orthogonal Equations?)}
			\end{aligned}
	\end{aligned}\)

	% Describe how R changes Q to A
	\vspace{20pt}
	\(\begin{aligned}
		&A = Q_\parallel R = \text{\scriptsize\(
			\left(\begin{matrix}
				| & \hspace{-7pt} |	& \hspace{-7pt} | \\
				\vec{q_1} &\hspace{-7pt}\dots &\hspace{-7pt}\vec{q_n}\\
				| & \hspace{-7pt} |	& \hspace{-7pt} | \\
			\end{matrix}\right) 
			\left(\begin{matrix}
				r_{11} & \hspace{-5pt} \dots \hspace{-5pt} & r_{1n} \\[-5pt]
				0 & \hspace{-5pt} \ddots \hspace{-5pt} & \vdots \\
				0 & \hspace{-5pt} 0 \hspace{-5pt} & r_{nn}
			\end{matrix}\right)\)}
			= \text{\scriptsize\(
				\left(\begin{matrix}
					| & \hspace{-7pt} |	& \hspace{-7pt} | \\
					\vec{a_1} &\hspace{-7pt}\dots &\hspace{-7pt}\vec{a_n}\\
					| & \hspace{-7pt} |	& \hspace{-7pt} | \\
				\end{matrix}\right)\)} \\
		&\bullet\ \vec{a_j} = \sum_i^j r_{ij}\cdot \vec{q_i}
	\end{aligned}\)

	% Conclusion
	\vspace{15pt}
	\(\rightarrow\) \fbox{ 
		\begin{minipage}[t]{7.5cm}
			\(R\) transforms the \(Q_\parallel\) column vectors about \\[5pt]
			span\((A)\), an \(\mathbb{R}^n\) plane, until they equal the\\[5pt]
			column vectors of \(A\)
		\end{minipage}
	}
\end{minipage}

%---------------------------------------------------------------		
%---------------------------------------------------------------		
%---------------------------------------------------------------		
%---------------------------------------------------------------		
% Householder Transformations
\newpage
\subsubsection{Householder Transformation/Elementary Reflector, \(H\)}
% Define H
\[
	\begin{aligned}
		H\vec{a_1} &\ =\ \alpha_1 \vec{e_1} 
			\hspace{20pt} \begin{gathered}
				\text{\scriptsize\( \Vert a_1 \Vert = \vert \alpha_1 \vert \)}\\[-7pt]
				\text{\scriptsize(rotation)}
			\end{gathered}\\[5pt]
		&\ =\ \begin{gathered}[t]
				\boxed{ \vec{a_1} - 2\hat{v} (\hat{v} \cdot \vec{a_1}) }\\[-1pt]
				\text{\scriptsize\(\big[ v_\perp \text{ bisects } \theta(a_1, e_1) \big]\)}
			\end{gathered}
	\end{aligned}
	\ \ \ \rightarrow \ \ \
	\boxed{ H = I - \hat{v} \hat{v}^T = I - \frac{2vv^T}{v^Tv} }
	\hspace{20pt}
	\begin{aligned}
		&\bullet\ H = H^T = H^{-1}\\[-5pt]
		&\text{\scriptsize(symmetric and orthogonal)}
	\end{aligned}
\]

% Solve for v
\vspace{5pt}
\(\begin{aligned}
	&\bullet\ 
		\begin{aligned}[t]
			\alpha_1 e_1 = a_1 - (2v_1)\frac{v_1 \cdot a_1}{v_1 \cdot v_1} 
				&\ \ \Rightarrow\ \ v_1 = (a_1 - \alpha e_1) \cancel{ \frac{v_1 \cdot v_1}{2v_1 \cdot a_1} } 
				\hspace{15pt} \text{\scriptsize(magnitude doesn't matter)}
				\\[5pt]
			&\ \ \rightarrow\ \ \boxed{ v_1 = (a_1 - \alpha e_1) }
				\\[10pt]
			\alpha_1 = \pm \Vert a_1 \Vert
				& \ \ \rightarrow\ \ \boxed{ \alpha_i = - \text{sign}(a_i) \Vert a_i \Vert }
				\hspace{15pt} \text{\scriptsize(avoid "cancellation" in finite-calc. of \(v\) above)}
				\\[5pt]
			H_j\dots H_1 a_i = a_i^j 
				& \ \ \rightarrow\ \ \boxed{ v_{j+1} = \text{\scriptsize\(\left(\begin{matrix}
					0\\
					\vdots\\
					(a_i^j)_i\\
					\vdots\\
					(a_i^j)_m
				\end{matrix}\right)\)} - \alpha_i e_i }
				% Extra notes
				\hspace{17pt}
				\begin{aligned}
					&\bullet\ \text{\scriptsize Store \(v_i\) and \(R\) into \(A\) and an extra \(n\)-vector.}\\
					&\bullet\ \text{\scriptsize \(Q\) and \(H\) can be computed if needed.}\\
					&\bullet\ \text{\scriptsize When column \(i\) is completed, row \(i\) is too.}
				\end{aligned}
		\end{aligned}
\end{aligned}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Givens Rotation
\vspace{10pt}
\subsubsection{Givens Rotation, \(G\)}
\vspace{5pt}
\(\begin{aligned}
	\boxed{ G = 
		\left(\begin{matrix}
			c & s\\
			-s& c
		\end{matrix}\right) }
		&\ \rightarrow \ Gx = 
		G \left(\begin{matrix} 
			a_1\\
			a_2 
		\end{matrix}\right) 
		= \pm 
		\left(\begin{matrix} 
			\Vert a \Vert \\
			0 
		\end{matrix}\right)\\
	&\ \rightarrow \ \boxed{ c = \frac{a_1}{\sqrt{a_1^2 + a_2^2}}\ , \ \ s = \frac{a_2}{\sqrt{a_1^2 + a_2^2}} }
\end{aligned}
\hspace{25pt}
% Extra Notes
\begin{aligned}
	&\bullet\ \parbox[t]{6cm}{\scriptsize creates 0's one at a time, rotating all column vectors CLOCKWISE}\\[-3pt]
	&\bullet\ \text{\scriptsize Doesn't necessarily preserve 0's.}\\[-5pt]
	&\bullet\ \parbox[t]{6cm}{\scriptsize Create zeros columns-wise, starting from the bottom of the vector and moving up.}\\[-3pt]
	&\bullet\ \text{\scriptsize useful for spare matrices}\\[-5pt]
	&\bullet\ \text{\scriptsize When column \(i\) is completed, row \(i\) is too.}\\[-5pt]
\end{aligned}\\[10pt]\)

% Example of matrix
\(\begin{aligned}
	\text{Ex: }\ G_5(2,4) = \text{\scriptsize\(
		\left(\begin{matrix}
			1 & 0 & 0 & 0 & 0\\
			0 & c & 0 & s & 0\\
			0 & 0 & 1 & 0 & 0\\
			0 & -s & 0 & c & 0\\
			0 & 0 & 0 & 0 & 1\\
		\end{matrix}\right)\)}
\end{aligned}\)

% Minor change to reduce chance of overflow/underflow
\vspace{5pt}
\(\begin{aligned}
	&\text{Avoid squaring any number \(\gg 1\) to prevent overflow/underflow}\\[5pt]
	&\hspace{15pt} \begin{array}{c r c l c l}
			\bullet & t = \tfrac{a_2}{a_1} < 1 &\rightarrow& c = \frac{1}{\sqrt{1+t^2}} &,& s = c \cdot t\\[5pt]
			\bullet & \tau = \tfrac{a_1}{a_2} < 1 &\rightarrow& s = \frac{1}{\sqrt{1+\tau^2}} &,& c = s \cdot \tau\\
		\end{array}
\end{aligned}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
% Gram-Schmidt Orthogonalization
\newpage
\subsubsection{Gram-Schmidt Orthogonalization}

\vspace{5pt}
% Classical, Column-Oriented Gram Schmidt
\(\begin{aligned}
	&\parbox{10cm}{\setstretch{1.1} \underline{Column-Oriented}:
		Build the \(\widehat{q_i}\) column vector first, then solve for \(\widehat{q}_{i+1}\), 
		and continue up to \(\widehat{q_n}\).}\\[5pt]
	&\parbox{10cm}{\setstretch{1.1} \underline{Classical}:
		Each sucessive term in the sum for \(q_i\) depends on the original vector, \(a_i\).}\\
	% Notes
	&\bullet\ \text{\scriptsize Extra storage is needed for \(a_i\).}\\	
	&\bullet\ \parbox[t]{9cm}{\scriptsize Cancellation that causes loss of orthogonality occurs more when ill-conditioned.
		As a result, performing \(Q^T_\parallel b = b'_1\) isn't always best.}\\
	&\bullet\ \text{\scriptsize Can't column-pivot, since that depends on rows being completed first.}
\end{aligned}\)
\hfill
% QR Matrices
\text{\scriptsize\(
	\begin{aligned}
		Q_\parallel^T &= 
			\left(\begin{aligned}
				&\widehat{q_1} : q_1 = a_1\\
				&\widehat{q_2} : q_2 = a_2 - \widehat{q_1} (\widehat{q_1} \cdot a_2)\\
				&\widehat{q_3} : q_3 = a_3 - \widehat{q_1} (\widehat{q_1} \cdot a_3) - \widehat{q_2} (\widehat{q_2} \cdot a_3)\\
				&\hspace{3pt} \vdots\\
				&\widehat{q_n} : q_n = a_n - {\scriptstyle\sum}_j^n\ \widehat{q_j} (\widehat{q_j} \cdot a_n)
			\end{aligned}\right)
			\\[5pt]
		R &= 
			\left(\begin{matrix}
				\Vert q_1 \Vert & \widehat{q_1} \cdot a_2 & \widehat{q_1} \cdot a_3 & \dots & \widehat{q_1} \cdot a_n\\
				0 & \Vert q_2 \Vert & \widehat{q_2} \cdot a_3 & \dots & \widehat{q_2} \cdot a_n\\
				0 & 0 & \Vert q_3 \Vert & \dots & \widehat{q_3} \cdot a_n\\
				\vdots & \vdots & \ddots & \ddots & \vdots\\
				0 & 0 & \dots & 0 & \Vert q_n \Vert
			\end{matrix}\right)
	\end{aligned}
\)}

% Modified Gram-Schmidt
\vspace{20pt}
\(\begin{aligned}[t]
	&\parbox{9.25cm}{\setstretch{1.1} \underline{Modified}:
		Each sucessive term in the \(q_i\) sum, \(q_i^{[k+1]}\), depends on its previous term, \(q_i^{[k]}\).}\\[5pt]
	&\boxed{ 
		\begin{aligned}
			q_{i}^{[k+1]} \ &\equiv\ q_{i}^{[k]} - \widehat{q}_{k} \langle \widehat{q}_{k} | \widehat{q}_{i}^{\ [k]} \rangle\\[5pt]
			q_{i}^{[k]} \ &\equiv\ \widehat{q}_{k} \langle \widehat{q}_{k} | \widehat{q}_{i}^{\ [k]} \rangle + q_{i}^{[k+1]}\\
				&\equiv\ \text{\scriptsize\(\sum_{k'=k}^i \ \)} 
				\widehat{q}_{k'} \langle \widehat{q}_{k'} | \widehat{q}_{i}^{\ [k']} \rangle 
		\end{aligned} 
		}
		\ \left\{ \ 
		\boxed{
		\begin{aligned}
			q_{i} &\equiv q_{i}^{[i]}\\[2pt]
			q_{i}^{[1]} &\equiv a_i \\
			&= \text{\scriptsize\(\sum_{k=1}^i \ \)} \widehat{q}_{k} \langle \widehat{q}_{k} | \widehat{q}_{i}^{\ [k]} \rangle\\
			q_1 &= x_0
		\end{aligned} 
		} 
		\right.
		\\[5pt]
	% Notes
	&\bullet\ \text{\scriptsize No storage for \(a_i\) is needed.}\\[-5pt]
	&\bullet\ \text{\scriptsize Less error in orthog. with finite-precision arithmatic.}
\end{aligned}\)
\hfill 
% QR matrices
\text{\scriptsize\(\begin{aligned}[t]
	&\\[-25pt]
	Q_n^T &=
		\left(\arraycolsep=2pt \begin{array}{c c l c l}
			\widehat{q_1} &:& q_1 = a_1\\[-6pt]
			\widehat{q_2} &:& q_2 = q_2^{[2]} &=& \overbrace{ a_2 }^{q_2^{[1]}}
				- \ \widehat{q_1} (\widehat{q_1} \cdot \widehat{q_2}^{[1]})\\[5pt]
			\widehat{q_3} &:& q_3 = q_3^{[3]} &=& \underbrace{ a_3 
				- \widehat{q_1} (r_{13}) }_{q_3^{[2]} \hspace{20pt}} 
				- \ \widehat{q_2} \langle \widehat{q_2} | \widehat{q_3}^{[2]} \rangle \\[-15pt]
			\vdots &\\[-5pt]
			\widehat{q}_{n} &:& q_{n} = q_n^{[n]} &=& a_n
				- \text{\scriptsize\(\displaystyle \sum_{m=1}^{n-1}\)} \ \widehat{q}_m (r_{mn})
		\end{array}\right)
		\\
	R_n &=
		\left(\arraycolsep=3pt \begin{array}{c c c c c}
			\Vert q_1 \Vert 
				& \langle \widehat{q_1}|\widehat{q_2}^{[1]}\rangle 
				& \langle\widehat{q_1}|\widehat{q_3}^{[1]}\rangle
				& \dots 
				& \langle\widehat{q_1}|\widehat{q_n}^{[1]}\rangle\\[2pt]
			0 
				& \Vert q_2 \Vert 
				& \langle\widehat{q_2}|\widehat{q_3}^{[2]}\rangle 
				& \dots 
				& \langle\widehat{q_2}|\widehat{q_n}^{[2]}\rangle\\[2pt]
			0 
				& 0 
				& \Vert q_3 \Vert 
				& \dots & \langle\widehat{q_3}|\widehat{q_n}^{[3]}\rangle\\
			\vdots 
				& \vdots 
				& \ddots 
				& \ddots 
				& \vdots\\
			0 
				& 0 
				& \dots 
				& 0 
				& \Vert q_n \Vert
		\end{array}\right)
\end{aligned}\)}

% Row Oriented Gram Schidt
\vspace{10pt}
\underline{Row Oriented}:\ \ For all
	\(1\leq i \leq n\), solve for \(q_i^\text{\scriptsize\([k]\)}\) starting first at \(k=1\), then continue 
	until \(k=n\).
	\\[5pt]
% Notes
\(\hspace{10pt} \begin{aligned}
	&\bullet\ \text{\scriptsize Allows for column pivoting since rows are completed first.}\\[-5pt]
	&\bullet\ \text{\scriptsize Cancellation, though still present, is less severe.}
\end{aligned}\)

% Augmented Gram Schmidt matrices
\vspace{10pt}
\underline{Augmented Matrix}:\\[10pt]
\(\begin{aligned}
	\left(\begin{matrix}
			A
		\end{matrix}\right.
		|
		\left.\begin{matrix}
			b
		\end{matrix}\right)
		&= 
		\left(\begin{matrix}
			Q_\parallel
		\end{matrix}\right.
		|
		\left.\begin{matrix}
			q_{n+1}
		\end{matrix}\right)
		\text{\scriptsize\(
			\left(\begin{matrix}
				R & b'_1\\
				0 & \rho
			\end{matrix}\right)
		\)}\\[5pt]
	\left(\arraycolsep=2pt \begin{array}{c c c c}
			| & | & | & |\\
			a_1 & .. & a_n & b\\
			| & | & | & |
		\end{array}\right)
		&=
		\left(\arraycolsep=1pt \begin{array}{c c c c}
			| & | & | & |\\
			\widehat{q_1} & \dots & \widehat{q_n} & q_{n+1}\\
			| & | & | & |
		\end{array}\right)
		\text{\scriptsize\(\arraycolsep=1pt
			\left(\begin{array}{c c c c}
				r_{11} & \dots & r_{1n} & |\\
				0 & \ddots & \vdots & b'_1\\[-5pt]
				\vdots & \ddots & r_{nn} & |\\[5pt]
				0 & \dots & 0 & \rho\\
			\end{array}\right)
		\)}
\end{aligned}
\hspace{20pt}
\begin{aligned}
	&\bullet\ \begin{minipage}[t]{7cm}
			\scriptsize Use Gram-Schmidt QR on this, then solve \(Rx = b'_1\)
		\end{minipage}\\
	&\bullet\ \begin{minipage}[t]{7cm}
			\scriptsize This method is prefered numerically to reduce \\
			cancelling effects
		\end{minipage}\\
	&\bullet\ \begin{minipage}[t]{7cm}
			\scriptsize Text didn't recommend what \(q_{+1}\) or \(\rho\) should be.
		\end{minipage}\\ 
	&\bullet\ \text{\scriptsize \(\rho\) or \((q_{n+1})_i\) looks like it should be 0.}\\
	&\bullet\ \text{\scriptsize Idk, not much explained.}
\end{aligned}\)

% Extra
\vspace{10pt}
\underline{Reorthogonalizing}:\ \ Repeating procedure to straighten vectors (usually not needed)

%-----------------------------------------------------------
%-----------------------------------------------------------
% Column-Pivoting
\newpage
% \vspace{7pt}
\subsubsection{Factorization with Column-Pivoting}
\vspace{7pt}
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Column with largest norm is pivoted to the current column \(i\) to be reduced, 
		and current row \(i\) is completed too.}\\
	&\bullet\ \text{\scriptsize Choose the next pivoting column based on norms of the smaller columns from remaining 
		uncompleted submatrix.}\\
	&\bullet\ \text{\scriptsize Repeat until the end (rank might be \(n\)) or if the max norm is smaller than 
			some tolerance (rank might be \(k < n\))}\\
	&\bullet\ \text{\scriptsize Pivoting avoids working with 0's on the diag.}
\end{aligned}\)

%-----------------------------------------------------------
%-----------------------------------------------------------
% Rank-Deficiency
\vspace{7pt}
\subsubsection{Rank Deficiency (or Other) Case}
\vspace{10pt}
\(\begin{aligned}
	&\underline{\text{If rank}(A) = k < n} : \ \ \boxed{ \arraycolsep=0pt \begin{array}{c c c l}
			(Q^T AP) & (P^Tx) &=& Q^Tb\\[5pt]
			\text{\scriptsize\(\left(\begin{matrix}
					R\ & S\\
					0\ & 0'
				\end{matrix}\right)\)} 
				& \text{\scriptsize\(\left(\begin{matrix}
					z\\
					0
				\end{matrix}\right)\)} &\ \ =\ \ 
				& \text{\scriptsize\(\left(\begin{matrix}
					b'_1\\[3pt]
					b'_2
				\end{matrix}\right)\)}
			\end{array} }
			\hspace{30pt}
			\begin{aligned}
				&\text{\scriptsize\(\bullet\ 0'\) is approx. 0 since the remaining norms are too small.}\\[-5pt]
				&\text{\scriptsize\(\bullet\ R = R_{k \times k}\)}\\[-5pt]
				&\text{\scriptsize\(\bullet\ S\) is the remaining columns after \(R\) is completed.}\\[-5pt]
				&\text{\scriptsize\(\bullet\) \fbox{There are multiple solutions for \(x\).}}
			\end{aligned}\\[10pt]
	&\bullet\ \text{\scriptsize For a quick solution, }\ \boxed{ Rz = b'_1\ , \ \ x = P\text{\scriptsize\(\left(\begin{matrix}
			z\\
			0
		\end{matrix}\right)\)} }\\[5pt]
	&\bullet\ \text{\scriptsize For the minimized-norm solution with the smallest \(\Vert x \Vert\), \(S\) must be annihilated.}\\
	&\bullet\ \boxed{ \text{\scriptsize For another method or if underdetermined \((m < n)\), 
		something like SVD Decomp. can be used.} }
\end{aligned}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
% Singular Value Decomposition
\newpage
\subsection{Singular Value Decomposition (SVD)}
\vspace{5pt}
\[ \boldsymbol{ \arraycolsep=3pt
	\begin{array}{c c c c l}
		A &=& \boxed{ U\Sigma V^T }
			&=& \text{\scriptsize\(\left(\arraycolsep=3pt \begin{array}{c c c c c}
				\rule[-20pt]{.05pt}{38pt} &\hspace{20pt}& \rule[-20pt]{.05pt}{38pt} & & \rule[-20pt]{.05pt}{38pt}\\
				u_1 & \dots & u_k & .. & u_m\\
				\rule[-20pt]{.05pt}{38pt} & &\rule[-20pt]{.05pt}{38pt}& & \rule[-20pt]{.05pt}{38pt}
			\end{array}\right)
			\left(\arraycolsep=1pt \begin{array}{c c c c}
				\sigma_1 & 0 & \dots & 0\\
				0 & \sigma_2 & \ddots & \vdots\\[-4pt]
				0 & 0 & \ddots & 0\\[-3pt]
				\vdots & \vdots & \ddots & \parbox{14pt}{\(\sigma_n\) \vspace{7pt}} \\
				0 & 0 & \dots & 0\\[-4pt]
				\vdots & \vdots & & \vdots\\
				0 & 0 & \dots & 0
			\end{array}\right)
			\left(\begin{array}{c}
				\rule[2pt]{20pt}{.1pt} v_1 \rule[2pt]{20pt}{.1pt}\\[-3pt]
				\hspace{5pt}\\[-2pt]
				\vdots\\[-2pt]
				\hspace{5pt}\\[-2pt]
				\rule[2pt]{20pt}{.1pt} v_k \rule[2pt]{20pt}{.1pt}\\[-4pt]
				\vdots\\[-4pt]
				\rule[2pt]{20pt}{.1pt} v_n \rule[2pt]{20pt}{.1pt}
			\end{array}\right)\)}
			\\[1.75cm]
		&=& \boxed{ U_\parallel \Sigma_1 V^T }
			&=& \left(\begin{matrix}
				U_\parallel & U_\perp
			\end{matrix}\right)
			\left(\begin{matrix}
				\Sigma_1 \\
				0
			\end{matrix}\right)
			\text{\scriptsize\(
				\left(\begin{matrix}
					V_{0\perp}^T \\[5pt]
					V_{0\parallel}^T
				\end{matrix}\right)
			\)}
			\ =\ \displaystyle \boxed{ \sum_i^n \sigma_i \cdot u_i v_i^T }
	\end{array}
	\hspace{10pt} \vline \hspace{10pt}
	\begin{aligned}
		AA^T &= U \mathcal{D} U^T\\
		A^TA &= VD V^T\\[3pt]
		\mathcal{D} &= \Sigma \Sigma^T\\
		D &= \Sigma^T \Sigma\\ 
		\sigma_i &= \sqrt{d_i}\\
	\end{aligned}
} \]

% Notes
\(\begin{aligned}
	&\bullet\ \text{Underdetermined, } m < n \text{ is possible too.}\\
	&\bullet\ \text{Analagous to Gaussian-Jordan Diagonalization method.}\\
	&\bullet\ U \text{ and } V \text{ are orthogonal; } u_i \text{ and } v_i 
		\text{ are the respective ``left'' and ``right'' singular vectors.}\\
	&\bullet\ \text{Usually, the singular values are ordered such that } \sigma_1 \geq \sigma_2 \geq \dots\\
	&\bullet\ \forall(k < i),\ \sigma_i = 0 \ \ \Rightarrow\ \ \text{rank}(A) = k < n
	\\[5pt]
	&\bullet\ U_\parallel = U_{m \times k} : \ \  
		\text{span}(U_\parallel) = \text{span}(A)\ , \ \ \text{span}(U_\perp) = \text{span}(A)^{\perp}\\
	&\bullet\ V_{0\perp} = V_{n \times k} : \ \ 
		\text{span}(V_{0\parallel}) = \text{null}(A)\ , \ \ \text{span}(V_{0\perp}) = \text{null}(A)^{\perp}
		\hspace{30pt} \text{\scriptsize null\((A) = \{x : Ax = 0\}\)}
\end{aligned}\)

% Pseudoinverse
\vspace{5pt}
\underline{Pseudoinverse}: \ \ \(\boxed{ \begin{aligned}[t]
	&A^+ \ \equiv \ V \Sigma^+ U^T\\
	&\Sigma^+ \ \equiv\ \Big[ \Sigma^T \text{\ \ and\ \ } \sigma_i \rightarrow 1/\sigma_i 
		\hspace{10pt} \text{\scriptsize\(\forall(\sigma_i \neq 0)\)} \Big]
\end{aligned} }\)

% Solution for x
\(\begin{aligned}
	&\bullet\ \begin{aligned}[t]
		&Ax + r = b \ \rightarrow \\
		&\boxed{ x = A^+ b = \big(V \Sigma^+ U^T \big) b }
	\end{aligned}\\
	&\bullet\ \boxed{ x_\text{min} = \sum_{\sigma_i \neq 0}\frac{u_i \cdot b}{\sigma_i}\ v_i } 
		\hspace{20pt} \text{\scriptsize useful for ill-conditioned or rank deficient since small \(\sigma\) can be dropped.}
\end{aligned}\)

%---------------------------------------------------------
%---------------------------------------------------------
% Other uses
\vspace{5pt}
\subsubsection{Other uses}
\vspace{5pt}
\(\begin{array}{r l}
	\text{{Euclidean 2-norm}}: 
		& \displaystyle \Vert A \Vert_2 = \max_{x\neq 0} \tfrac{\Vert Ax \Vert_2}{\Vert x \Vert_2} = \sigma_\text{max}
		\\[15pt]
	\text{{Euclid. Cond. Num.}}:
		& \text{cond}_2(A) = \sigma_\text{max} / \sigma_\text{min}
		\\[5pt]
	\text{{Lower Rank Approx.}}:
		& A \ \approx\ A_k = \sum_i^k \sigma_i \left( u_i v_i^T \right)
		\hspace{20pt}
		\begin{aligned}
			&\text{\scriptsize\(\bullet\) Closest rank\(=k\) matrix to \(A\) in the Frobinius norm.}\\[-3pt]
			&\text{\scriptsize\(\bullet\) Frobinius Norm \(=\) Euclid. Norm for a ``vector'' in \(\mathbb{R}^{mn}\).}
		\end{aligned}
		\\[10pt]
	\text{Total Least Squares}:
		& \text{\scriptsize\(\begin{aligned}[t]
			&\left[A\ |\ y\right]_{m\times(n+1)} = U\Sigma V_{(n+1)\times(n+1)}^T\\[5pt]
			&\text{rank}\big( \big[\widehat{A}\ |\ y \big] \big) \leq n
				\ \rightarrow \ \sigma_{n+1} = 0 
				\ \rightarrow \ \widehat{A} \cdot v_{n+1} = 0\\[5pt]
			&\left[\widehat{A}\ |\ y\right] \cdot \left[\begin{matrix} x \\ -1 \end{matrix}\right] = 0
				\ \rightarrow \ \left[\begin{matrix} x \\ -1 \end{matrix}\right] \propto v_{n+1} 
				= \left[\begin{matrix} \vec{\nu_n} \\ \nu_{n+1} \end{matrix}\right]
				\ \rightarrow \ \boxed{ x = \frac{\vec{\nu_n}}{- \nu_{n+1}} }
		\end{aligned}\)}
		\hspace{15pt} \begin{aligned}[t]
			&\\[-10pt]
			&\text{-}\parbox[t]{3.7cm}{\scriptsize\(\widehat{A}\) is an \(A\) with uncertainty, like how \(y\) normally is.}
		\end{aligned}
\end{array}\)

%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
%----------------------------------------------------------------
\newpage
% Complexity
\subsection{Complexity}

% Cholesky
\vspace{10pt}
\(\begin{aligned}
	&\text{Normal, Cholesky}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ A^TA = A' \text{\ \ costs\ \ } \tfrac{mn^2}{2}\\[2pt]
		&\bullet\ A' = LL^T \text{\ \ costs\ \ } \tfrac{n^3}{6}\\[2pt]
		&\bullet\ \text{Rel. Err. } \propto\ [\text{cond}(A)]^2\\[4pt]
		&\bullet\ \text{Bad if \ cond}(A) \approx 1/\sqrt{\epsilon_\text{mach}}
	\end{aligned}
\end{aligned}\)

% Householder
\vspace{10pt}
\(\begin{aligned}
	&\text{Householder}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ Q^TA = R \text{\ \ costs\ \ } mn^2 - \tfrac{n^3}{3} \\[2pt]
		&\bullet\ \text{Rel. Err. } \propto\ [\text{cond}(A)]^2 \Vert r \Vert_2 + \text{cond}(A)\\[4pt]
		&\bullet\ \text{Bad if \ cond}(A) \approx 1/\epsilon_\text{mach}\\[2pt]
		&\bullet\ \text{More accurate than Cholesky and broadly applicable}\\[2pt]
		&\bullet\ \text{Usable for rank deficient or nearly rank-deficient}
	\end{aligned}
\end{aligned}\)

% Givens
\vspace{10pt}
\(\begin{aligned}
	&\text{Givens}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ \text{The normal implementation needs 50\% more work than Householder.}\\[2pt]
		&\bullet\ \text{A more complex implementation makes it comparable to Householder.}\\[2pt]
		&\bullet\ \text{Useful if matrix is sparse or zeros need to be maintained.}
	\end{aligned}
\end{aligned}\)

% SVD
\vspace{10pt}
\(\begin{aligned}
	&\text{SVD}\\[5pt]
	&\hspace{10pt}\begin{aligned}[t]
		&\bullet\ \text{Most expensive cost at } \propto\ mn^2 + n^3 \text{ , perhaps 4-10 times or more.}\\[2pt]
		&\bullet\ \text{Robust and reliable.}
	\end{aligned}
\end{aligned}\)


%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Matrix Information
\newpage
\section{Matrix Information}
\vspace{10pt}
% Types
\begin{minipage}[t]{.49\textwidth}
	\(\begin{array}[t]{r l}
		\text{Orthogonal}:& {QQ^T = Q^T Q = I}\\[5pt]
		\text{Unitary}:& \begin{aligned}[t]
				&UU^\dagger = U^\dagger U = I \\
				&\Leftrightarrow\ U = e^{iH} = (U_{h}) e^{iD_h} (U_{h})^\dagger
			\end{aligned}\\[23pt]
		\text{Normal}:& AA^\dagger = A^\dagger A \ \Leftrightarrow\ A = UDU^\dagger
		\\[5pt]
		\text{Symmetric}:& S = S^T = QDQ^T \hspace{10pt} \text{\scriptsize(\(D\) is real)}\\[5pt]
		\text{Hermitian}:& H = H^\dagger = UDU^\dagger \hspace{10pt} \text{\scriptsize(\(D\) is real)}
	\end{array}\)
\end{minipage}
\hfill
\begin{minipage}[t]{.49\textwidth}
	\(\begin{array}[t]{r l}
		\text{Similar}:& \begin{aligned}[t]
				&A(Ty) = \lambda (Ty) \\
				&\rightarrow \ A \sim B = T^{-1}AT
			\end{aligned}\\[22pt]
		\text{Diagonalize}:& T^{-1}AT = D 
			\hspace{15pt} \text{\scriptsize\(\left(\begin{gathered}
				\text{\(A\) is nondefective}\\[-4pt]
				\text{\(T\) is nonsingular}\\[-4pt]
				\text{\(D\) is diag.}
			\end{gathered}\right)\)}\\[10pt]
		\text{Hessenberg}:& \text{\scriptsize Triang. but from any diag.}\\[5pt]
		\text{Jordan Form}:& \parbox[t]{4cm}{\scriptsize Nonsing. trans. into a near diag. w/ entries in diag. above}
	\end{array}\)
\end{minipage}

% notes
\vspace{5pt}
\begin{minipage}[t]{.49\textwidth}
	\(\begin{aligned}[t]
		&\bullet\ Ax = \lambda x \ , \ \ \text{det}(A) \neq 0 
			\ \Rightarrow \ A^{-1}x = (1/\lambda)x\\[5pt]
		&\bullet\ \text{Shifting}:\ (A-\sigma I)x = (\lambda-\sigma)x
	\end{aligned}\)
\end{minipage}
\hfill
\begin{minipage}[t]{.48\textwidth}
	\(\begin{aligned}[t]
		&\bullet\ \text{Simple: Normal [Algebraic] Mult. of 1}\\[5pt]
		&\bullet\ \text{Defective: \(\begin{gathered}[t]
				\text{Geo. Mult.}\\[-7pt]
				\text{\scriptsize(eig. vec. \#)}
			\end{gathered}\) \(<\) Alg. Mult.}
	\end{aligned}\)
\end{minipage}

% Invariant Subspace
\underline{Invariant Subspace}: \ \ 
	\(\{ \mathcal{S} :\ (A\mathcal{S} \subseteq \mathcal{S})
	\ \equiv\ (\forall x \in \mathcal{S} \Rightarrow Ax \in \mathcal{S}) \}\)

%------------------------------------------------------------
%------------------------------------------------------------
% Schur Form
\vspace{5pt}
\subsection{Schur Form}
\vspace{5pt}
% Unitary Form
\begin{minipage}[t]{.49\textwidth}
	% Definition
	\(
		\begin{gathered}
			\text{Unitary}\\[-7pt]
			\text{\scriptsize(for all)}
		\end{gathered}\): \ \(\boxed{T^\dagger A T = R} \hspace{20pt} \begin{gathered}
			\text{\scriptsize(\(T\) is unitary)}\\[-7pt]
			\text{\scriptsize(\(R\) is upp. triang.)}
		\end{gathered}
	\)
	
	% Notes
	\vspace{10pt}
	\(
		\begin{aligned}
			&\bullet\ A\vec{x_i} = r_{ii} \vec{x_i} \\[5pt]
			&\bullet\ \begin{gathered}[t]
					\begin{aligned}[t]
							0 &= (R - r_{ii} I)\ \vec{x_i}\\[5pt]
							&= \text{\scriptsize\(\left(\arraycolsep=3pt \begin{array}{c c c}
									R_{11} - r_{ii} I & \vec{u} & R_{13} \\[3pt]
									0 & 0 & \vec{v}^{\ T} \\
									\mathcal{O} & 0 & R_{33} - r_{ii} I
								\end{array}\right)\)}
								\vec{x_i}
						\end{aligned}\\[5pt]
					\Downarrow\\
					\vec{x_i} = \text{\scriptsize\(\left(\begin{matrix}
							\vec{y}\\ -1\\ 0
						\end{matrix}\right)\)} \ :\ \begin{gathered}[t]
							(R_{11} - r_{ii} I)\\[-5pt]
							\text{\scriptsize(nonsingular)}
						\end{gathered}\ \vec{y} = \vec{u}
				\end{gathered}\\[5pt]
			&\bullet\ \parbox[t]{.8\textwidth}{\scriptsize``Schur form of a real matrix will have complex
				entries if the matrix has any complex eigenvalues''}
		\end{aligned}
	\)
\end{minipage}
% Real Form
\begin{minipage}[t]{.49\textwidth}
	% Definition
	\(
		\begin{gathered}[t]
			\text{Real}\\[-7pt]
			\text{\scriptsize(only for }\\[-7pt]
			\text{\scriptsize real matrix)}
		\end{gathered}: \ \boxed{Q^T AQ = R} 
		\hspace{20pt} 
		\begin{gathered}
			\text{\scriptsize(\(Q\) is ortho.)}\\[-7pt]
			\text{\scriptsize(\(R\) is block upp. triang.)}
		\end{gathered}
	\)
	
	% Notes
	\vspace{10pt}
	\(
		\begin{aligned}
			&\bullet\ R = \left(\begin{matrix}
					R_{11} & \dots & R_{1p}\\
					\mathcal{O} & \ddots & \vdots\\
					\mathcal{O} & \mathcal{O} & R_{pp}
				\end{matrix}\right) \\[5pt]
			&\bullet\ \lambda_i(A) = \lambda(R_{ii}) \\[5pt]
			&\bullet\ R_{ii} = \begin{cases} 
					R_{1\times1} & \text{\scriptsize(real eigenvalue of \(A\))}\\
					R_{2\times 2} & \text{\scriptsize(complex eigenvalue pairs of \(A\))}
				\end{cases}\\[5pt]
			&\bullet\ \text{\scriptsize All other entries are real}\\[5pt]
			&\bullet\ \text{Reducible: If } PAP^{T} = R \hspace{10pt} \text{\scriptsize(\(P\) is permu.)}
		\end{aligned}
	\)
\end{minipage}

% Blocked Upper Triangular
\vspace{10pt}
\underline{Block Upper Triangular Transformation}\\[10pt]
\(\begin{aligned}
	&\bullet\ \begin{aligned}[t]
			&X_\parallel = \left(\vec{x_1} \ \dots \ \vec{x_p}\right) 
				\ , \ \ Ax_i \in \text{span}(x_i) = \mathcal{S}\\[5pt]
			&\Rightarrow \ AX_\parallel = X_\parallel B
		\end{aligned}\\[5pt]
	&\bullet\ I_n = X^{-1} X = \left(\begin{matrix}
			Y_\parallel \\
			Y_\perp
		\end{matrix}\right) 
		\left(\begin{matrix}
			X_\parallel & X_\perp
		\end{matrix}\right)
		= 
		\left(\begin{matrix}
			Y_\parallel X_\parallel & Y_\parallel X_\perp\\
			Y_\perp X_\parallel & Y_\perp X_\perp
		\end{matrix}\right)
		= 
		\left(\begin{matrix}
			I_{p} & \mathcal{O}\\
			\mathcal{O} & I_{n-p}
		\end{matrix}\right)\\[10pt]
	&\bullet\ X^{-1}AX = \left(\begin{matrix}
			Y_\parallel A X_\parallel & Y_\parallel A X_\perp\\
			Y_\perp A X_\parallel & Y_\perp A X_\perp
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			Y_\parallel X_\parallel B & Y_\parallel A X_\perp\\
			Y_\perp X_\parallel B & Y_\perp A X_\perp
		\end{matrix}\right)
		=
		\left(\begin{matrix}
			B & Y_\parallel A X_\perp\\
			\mathcal{O} & Y_\perp A X_\perp
		\end{matrix}\right)
		= R
\end{aligned}\)

%---------------------------------------------------------------
%---------------------------------------------------------------
%---------------------------------------------------------------
%---------------------------------------------------------------
\newpage
% Upper Hessenberg Transformation
\subsection{Upper Hessenberg Transformation (see Householder)}
\(\begin{aligned}
	&\rightarrow\ H_j^{(k)} = I - \hat{v} \hat{v}^T\\
	&\rightarrow\ v_{j+1} = \text{\scriptsize\(\left[\arraycolsep=2pt \begin{array}{c c c c c c c}
			0 &\dots& 0 & (a_i^j)_k & \dots & (a_i^j)_m
		\end{array}\right]\)}^T - \alpha_i e_i
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Extra 0s are added to \(v\) to start del. from a diff. row, \(k\).}
\end{aligned}\)

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Solving Eigenvalue Equation
\vspace{15pt}
\section{Eigenvalue Equation, \(Ax = \lambda x\)}

% Error Bounds and Conditioning
\subsection{Error Bound and Conditioning}

% Error Bound for Eigenvalue
\vspace{10pt}\noindent
\(\begin{aligned}[t]
	&A + \Delta A = Q(D + \Delta D)Q^{-1}\\[5pt]
	&\hspace{10pt} \begin{aligned}
			&\bullet\ v 			
				= (\Delta\lambda I - D)^{-1} (\Delta D) v\\[5pt]
			&\bullet\ \begin{aligned}[t]
				\Vert (\Delta\lambda I - D)^{-1} \Vert_2^{-1} &\leq \Vert \Delta D \Vert_2\\[5pt]
				\vert \Delta\lambda - \lambda_i \vert &\leq \Vert Q (\Delta A) Q^{-1} \Vert_2   
			\end{aligned}
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \vert \Delta\lambda - \lambda_i \vert
		\leq \text{cond}(Q)\ \Vert \Delta A \Vert_2 }
\end{aligned}
\hspace{1.25cm}
% Error for Simple Eigenvalues
\begin{aligned}[t]
	&(A + \Delta A)(x + \Delta x) = (\lambda + \Delta\lambda)(x + \Delta x)\\[5pt]
	&\hspace{10pt} \begin{aligned}[t]
			&\bullet\ Ax = \lambda x\ ,\ \ y^H A = \lambda y^H \\
			&\bullet\ \underline{\lambda \text{\scriptsize\ is simple}} \ \Rightarrow \ y^H x \neq 0\ \ (?)\\
			&\bullet\ \text{\scriptsize\(\begin{aligned}[t]
				&y^H \bcancel{Ax} + \cancel{y^H A\Delta x} + y^H (\Delta A)x + y^H \xcancel{(\Delta A) \Delta x} \\
				&\hspace{10pt} \approx y^H \bcancel{\lambda x} + \cancel{y^H \lambda \Delta x} 
					+ y^H (\Delta\lambda) x + y^H \xcancel{(\Delta\lambda) \Delta x}
			\end{aligned}\)}
		\end{aligned}\\[10pt]
	&\rightarrow\ \boxed{ \vert \Delta\lambda \vert
		\ \lessapprox\ \frac{\Vert y \Vert_2 \cdot \Vert x \Vert_2}{\vert y^H x \vert}\ \Vert \Delta A \Vert_2
		\ =\ \frac{1}{\cos\theta}\ \Vert \Delta A \Vert_2 }
\end{aligned}\)

% Notes
\vspace{10pt}
\(\begin{aligned}
	&\bullet\ AA^\dagger = A^\dagger A \ \rightarrow \ \text{cond}(A) = 1\\
	&\bullet\ \text{Non-simple (multiple) eigenvalue is complicated:}\\
	&\bullet\ \text{allows } y^Hx = 0 \text{, depends on eigenvalue spacings, vector angles, etc.}\\
	&\bullet\ \text{Balancing (diagonal rescaling) can improve conditioning}
\end{aligned}\)

%--------------------------------------------------------------
%--------------------------------------------------------------
% QR Iteration
\vspace{10pt}
\subsection{QR Iteration}
% QR Iteration Intro

% Power Iteration
\vspace{5pt}
\underline{Power Iteration}:\\[10pt]
\(\begin{aligned}[t]
	&\begin{aligned}[t]
			A^k x_0 \ &=\ A^k \sum_{i=1}^n c_i v_i
				\ =\ \sum_{i=1}^n c_i \left( A^k v_i \right)
				\ =\ \sum_{i=1}^n c_i \left( \lambda_i^k v_i \right) 
				\\[10pt]
			&=\ \lambda_1^k \left[ \sum_{i=1}^j c_i v_i 
				+ \sum_{i=j+1}^n c_i \cancel{ \left( \frac{\lambda_i}{\lambda_1} \right)^k } v_i  \right]
				\hspace{10pt} \begin{gathered}
					\text{\scriptsize\((\lambda_1 = ... = \lambda_j)\)} \\[-5pt]
					\text{\scriptsize\((\lambda_1 \geq ... \geq \lambda_n)\)} 
				\end{gathered}
		\end{aligned}\\[10pt]
	&\boxed{ \begin{aligned}[t]
			A^k x_0 &= A x_{k-1} = x_k \\
			&\approx\ \lambda_1^k \ \sum_i^j c_i v_i 
			\ \ =\ \lambda_1^k (c_1 v_1) \hspace{5pt} \text{ if \ } j=1
		\end{aligned} }
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}[t]
	&\bullet\ \text{\scriptsize Eigenvalue converges to \(\lambda_i\) of largest modulus.}\\
	&\bullet\ \text{\scriptsize \(v_i\) converges to lin. com. if mult. max \(\lambda_i\).}\\
	&\bullet\ \text{\scriptsize Normalize to norm\(_\infty = 1\) to prevent over/underflow.}\\
	&\bullet\ \text{\scriptsize Fails if \(\langle v_i | x_0 \rangle = 0\) (unlikely w/ round. error).}\\
	&\bullet\ \text{\scriptsize Real \(A\) and \(x_0\) won't ever converge a complex.}\\
	&\bullet\ \text{\scriptsize Convergence Rate: \(C = \vert \lambda_2 / \lambda_1 \vert\)}
\end{aligned}\)

%--------------------------------------------------------------
%
%
%--------------------------------------------------------------
\newpage
% Power Iteration w/ Shifts
\underline{Power Iteration w/ Shifts}:\\[15pt]
\(\begin{aligned}[t]
	&\boxed{ \Big[ A' x_{k-1} = ({\scriptstyle A-\sigma I}) x_{k-1} = x_k \Big]
		\ \Rightarrow\ \Big[ {\scriptstyle (\lambda'_1 + \sigma)} \rightarrow \tfrac{1}{\lambda'_1 + \sigma} \Big] }
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}[t]
	&\bullet\ \text{\scriptsize Convergence Rate: 
		\(C = \left\vert \frac{\lambda_2 - \sigma}{\lambda_1 - \sigma} \right\vert < \vert \lambda_2 / \lambda_1 \vert\)}\\[3pt]
	&\bullet\ \text{\scriptsize\(x_k \rightarrow v_1 \ : \ \sigma = (\lambda_2 + \lambda_n)/2\)}\\
	&\bullet\ \text{\scriptsize\(x_k \rightarrow v_n \ : \ \sigma = (\lambda_1 + \lambda_{n-1})/2\)}\\
\end{aligned}\)

% Inverse Iteration
\underline{Inverse [Power] Iteration}:\\[15pt]
\(\begin{aligned}
	&\left[ \left( A^{-1} \right)^k x_0 = x_{k} \right]
		\ \Rightarrow\ \boxed{ \big( A x_{k+1} = x_k \big)
		\rightarrow \left( \lambda_1 \rightarrow \tfrac{1}{\lambda_1} \right) }
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Use LU Decomp. or Cholesky to solve for \(x_{k+1}\)}\\
	&\bullet\ \text{\scriptsize Eigenvalue converges to \(\lambda_i\) of smallest modulus.}\\
\end{aligned}\)

% Inverse Iteration w/ Shifts
\vspace{15pt}
\underline{Inverse Iteration w/ Shifts}:\\[10pt]
\(\begin{aligned}
	&\boxed{ \Big[ A' x_{k+1} = ({\scriptstyle A-\sigma I}) x_{k+1} = x_k \Big]
		\ \Rightarrow\ \Big[ {\scriptstyle (\lambda'_1 + \sigma)} \rightarrow \tfrac{1}{\lambda'_1 + \sigma} \Big] }
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Eigenvalues converges to \(\lambda_i\) closest to \(\sigma\).}\\
	&\bullet\ \parbox[t]{6cm}{\scriptsize Using a new shift each iteration requires refactoring each time.}
\end{aligned}\)

% Rayleigh Quotient
\vspace{15pt}
\underline{Rayleigh Quotient (for shifts)}:\\[10pt]
\(\begin{aligned}[t]
	\Big[ x \lambda + r = Ax \Big]
	\ \Rightarrow \ \Big[ x^H x \lambda = x^H A x \Big] 
	\ \Rightarrow \ \boxed{ \lambda = \frac{\langle x | Ax \rangle}{\langle x | x \rangle} }
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Derived with Normal eq. / Least Lin. Regression}\\
	&\bullet\ \text{\scriptsize Use \(\lambda\) as \(\sigma\) for shifts}
\end{aligned}\)

% Deflation
\vspace{10pt}
\underline{Deflation (concept)}:\\[10pt]
\(\begin{aligned}
	T^{-1}A_kT = 
		\left( \begin{matrix}
			\lambda_1 & b^T\\
			0 & A_{k+1}
		\end{matrix} \right)
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \parbox[t]{9cm}{\scriptsize Use \(A_{k+1}\) when \(\lambda_1\) is found to 
		good accuracy, then repeat with remaining \(\lambda_i\).}
\end{aligned}\)

% Simultaneous Iteration
\vspace{15pt}
\underline{Simultaneous/Subspace [Power] Iteration}:\\[10pt]
\(\begin{aligned}[t]
	&X_0 = X_{n \times p} 
		\ \ , \ \ \text{rank}(X_0) = p 
		\\[10pt]
	&\rightarrow \ \boxed{ X_{k+1} = AX_k = A^{k+1} X_0 }\\
	&\rightarrow \ \boxed{ \lim_{k \rightarrow \infty} \text{span} \big( X_k \big) = \text{span}(A_p) }
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}[t]
	&\bullet\ \text{\scriptsize All are found at the same time.}\\
	&\bullet\ \text{\scriptsize span\((A_p)\) is the span of the first \(p\) eigenvectors of largest magnitude.}\\
	&\bullet\ \parbox[t]{9cm}{\scriptsize Ill-conditioned since all columns of \(X_k\) converge to \(v_1\) 
		(though at different rates so they're still ortho.).}\\
	&\bullet\ \text{\scriptsize Normalize to norm\(_\infty = 1\) to prevent over/underflow.}
\end{aligned}\)

% Orthogonal Iteration
\vspace{15pt}
\underline{Orthogonal [Subspace] Iteration}:\\[10pt]
\(\begin{aligned}[t]
	&X = X_{n \times p} 
		\ \ , \ \ \text{rank}(X) = p 
		\\[10pt]
	&\rightarrow \ \boxed{ X_k = Q_k R_k 
		\hspace{15pt} \text{\scriptsize\((Q = Q_{n\times p})\)} }\\
	&\rightarrow \ \boxed{ X_{k+1} = A Q_k } \ = Q_k B_{p \times p}
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}[t]
	&\bullet\ \text{\scriptsize span\((Q) = \) span\((X)\) \ \ (see QR Decomp.).}\\
	&\bullet\ \text{\scriptsize\(B_{p\times p}\) is triag. if 
		\(\vert \lambda_i \vert > \vert \lambda_{i+1} \vert\); else it's block triag. \ \ (see Schur Form).}\\
	&\bullet\ \text{\scriptsize Orthogonalization is expensive, and convergence may be slow.}
\end{aligned}\)

%--------------------------------------------------------------
%--------------------------------------------------------------
%--------------------------------------------------------------
%--------------------------------------------------------------
\newpage
% QR Iteration
\subsubsection{QR Iteration}
\vspace{5pt}
\(\begin{aligned}
	&\begin{aligned}
			X_{k+1} \ &\equiv\ A Q_{k} 
				\hspace{15pt} (\text{\scriptsize use \(X_0 = I \ \rightarrow\ X_1 = A\)})\\
			Q_{k+1} R_{k+1}\ &=\ Q_{k} \big( Q_{k}^H A Q_{k} \big)
		\end{aligned}\\[5pt]
	&\rightarrow\ \begin{aligned}[t]
			&\boxed{ A_k \ \equiv\ Q_k^H X_{k+1} \ = \ Q_k^H A Q_k }\\[5pt]
			&= \left\{ \begin{array}{r c c}
					\big( Q_{k}^H Q_{k+1} \big) R_{k+1} &\equiv& \boxed{ Q_{k+1}^{(A)} R_{k+1} }\\[10pt]
					Q_k^H ( Q_k R_k Q_{k-1}^H ) Q_k &=&  R_{k} Q_{k}^{(A)}
				\end{array} \right.
		\end{aligned}\\[5pt]
	&\rightarrow\ \boxed{ A_{k+1} = R_{k+1} Q_{k+1}^{(A)} }
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize \(A_k = B_{p \times p}\) \ (from above), so diag. entries are \(\lambda\).}\\
	&\bullet\ \parbox[t]{7.5cm}{\scriptsize If \((p=n)\) and \((X_0 = I)\), then 
		\(\boxed{ Q_k = Q_1^{(A)} \dots Q_k^{(A)} }\) and \(\boxed{ R^{(k)} = R_k \dots R_1 }\).}\\
	&\bullet\ \text{\scriptsize Since \((A = Q_1 R_1)\), then by induction, \(\boxed{ A^k = Q_k R^{(k)} }\)}\\
	&\bullet\ \parbox[t]{7.5cm}{\scriptsize Still, orthogonalization is \(\mathcal{O}(n^3)\) expensive
		and convergence may be slow.}
\end{aligned}\)

% Inverse QR Iteration
\vspace{15pt}
\underline{Inverse QR Iteration}\\[10pt]
\(\begin{aligned}
	&\left( A_{k}^{-H} \ =\ Q_{k+1}^{(A)} R_{k+1}^{-H} \right)
		\ , \
		\left( A_{k+1}^{-H} \ =\ R_{k+1}^{-H} Q_{k+1}^{(A)} \right) \\[10pt]
	&\rightarrow\ \left( A^{-H} \right)^k = Q_{k}^{(A)} R_{k}^{-H}
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize\(R^H\) is low. triang.; \(Q^{(A)}\) is built backwards from \(v_n\) to \(v_1\).}\\
	&\bullet\ \parbox[t]{7cm}{\scriptsize Columns of \(Q^{(A)}\) for inv. QR iter. 
		of \(A^{H}\) are the same as QR iter. of \(A\).}\\
	&\bullet\ \parbox[t]{7cm}{\scriptsize Means QR iter. of A is an implicit inv. iter., so shifts are recommended 
		from \(v_n\) to \(v_1\) (see below).}
\end{aligned}\)

% QR Iteration with shifts
\vspace{15pt}
\underline{QR Iteration w/ Shifts \& Deflation}\\[10pt]
\(\begin{aligned}
	&\text{\scriptsize Rayleigh Quotient: \ } \sigma^{(n)} 
		= \tfrac{\langle q_n | A | q_n \rangle}{\langle q_n | q_n \rangle} 
		= {\scriptstyle \langle q_n | A | q_n \rangle}
		= (A_k)_{nn}\\[5pt]
	&\rightarrow\ \boxed{ Q_{k+1}^{(A)} R_{k+1} = A_k - \sigma_k^{(n)} I }\\[5pt]
	&\rightarrow\ \boxed{ A_{k+1} = R_{k+1} Q_{k+1}^{(A)} + \sigma_k^{(n)} I }\\[5pt]
	&\rightarrow\ \text{\scriptsize Deflation: } \lim_{k\rightarrow\infty} A_{k} = 
		\left( \begin{matrix}
			A'_{k+1} & b^T\\
			0 & \sigma_k^{(n)}
		\end{matrix} \right)
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Diag. entries of \(A_k\) are automatically Rayleigh Quotients.}\\
	&\bullet\ \text{\scriptsize \(\sigma\) is the last diag. entry, which corresponds to \(v_n\) (see above).}\\
	&\bullet\ \parbox[t]{8cm}{\scriptsize If \(\sigma_k = \lambda_n \rightarrow\) the last row of 
		\(Q_{k+1}^{(A)} R_{k+1}\) and \(R_{k+1}\) is 0 
		\(\rightarrow\) the last row of \(R_{k+1} Q_{k+1}^{(A)}\) is 0 \(\rightarrow\) Deflation (see right).}\\
	&\bullet\ \parbox[t]{8cm}{\scriptsize Fails if \(\sigma\) is halfway between two \(\lambda_i\) and favors neither.
		Or cancellation occurs (rare). Also might require complex arithmatic. Other more robust shifts are available.
		Convergence is only a few iterations, but cost to factor is still \(\mathcal{O}(n^3)\).}
\end{aligned}\)

% Hessenberg QR Iteration
\vspace{15pt}
\underline{Hessenberg QR Iteration [w/ Shifts]}\\
\(\begin{aligned}
	&\text{\scriptsize(see Hessenberg Transformation, \(H\))}\\[5pt]
	&\text{\sout{\( H = H^{(1)} \ \Rightarrow\ H^H A H \)}} 
		\ \ \text{\scriptsize isn't Hessenberg at all}\\[5pt]
	&H = H^{(2)} \ \Rightarrow\ H^H A H \ \ \text{\scriptsize is Hessenberg at subdiag.}\\[5pt]
	&\rightarrow\ \boxed{ A_1 = H^H A H = H A H }\\[5pt]
	&\rightarrow\ \begin{aligned}[t]
			&\boxed{ A_{k} \ [{\scriptstyle -\ \sigma I}] = R_k Q_k^{(A)} 
				\hspace{15pt} \text{\scriptsize\((k>1)\)} }
				\\[5pt]
			&\hspace{10pt} = R_k \Big( A_{k-1} \ [{\scriptstyle -\ \sigma I}] \Big) R_k^{-1}
				\ \ \ \parbox{50pt}{\scriptsize is Hessenberg at subdiag.}
		\end{aligned}
\end{aligned}\)
% Notes
\hfill
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Transformation to Hessenberg is done once and costs \(\mathcal{O}(n^3)\).}\\
	&\bullet\ \parbox[t]{7.5cm}{\scriptsize Givens Rotations to factorize a Hessenberg matrix at each 
		iter. costs \(\mathcal{O}(n^2)\).}\\
	&\bullet\ \text{\scriptsize Less iter. needed since already near triagular.}\\
	&\bullet\ \parbox[t]{7.5cm}{\scriptsize If \(A=A^H\), then the transform is tri-diag that costs 
		\(\mathcal{O}(n)\) and \(A_k\) approaches diag.}\\
	&\bullet\ \text{\scriptsize Still expensive if \(n\) is large.}\\
	&\bullet\ \text{\scriptsize Excessive storage if large and sparse.}\\
	&\bullet\ \text{\scriptsize No advantage if only a few \(\lambda_i\) are needed.}
\end{aligned}\)

%-----------------------------------------------------
%-----------------------------------------------------
%-----------------------------------------------------
%-----------------------------------------------------
% Krylov Subspace
\newpage
\subsection{Krylov Subspace}

% Characteristic Polynomial
\underline{Characteristic Polynomial}: \ \ 
\(\begin{aligned}[t]
	&\Big[ Ax = \lambda x \Big] \ \Rightarrow\ \Big[ p_n(\lambda) = 0 \Big]\\[5pt]
	&\boxed{ p_n(\lambda) \ =\ \left( \text{\scriptsize\(\sum_{i=0}^{n-1}\)}\ c_i \lambda^i \right) + \lambda^n 
		\ =\ c_0 + c_1 \lambda + \dots + c_{n-1}\lambda^{n-1} + \lambda^n }
	\\[5pt]
\end{aligned} \)

% Companion Matrix
\underline{Companion Matrix}: \ \ 
\(\begin{aligned}
	&\Big[ p_n(\lambda) = 0 \Big] \Rightarrow \ C_n x = \lambda x \ ,
		\hspace{5pt} C_n = \text{\scriptsize\(
			\left(\begin{matrix}
				0 & 0 & \dots & 0 & -c_0\\
				1 & 0 & \dots & 0 & -c_1\\
				0 & 1 & \dots & 0 & -c_2\\
				\vdots & \vdots & \ddots & \vdots & \vdots\\
				0 & 0 & \dots & 1 & -c_{n-1}\\
			\end{matrix}\right)
		\)}
		= \text{\scriptsize\(
			\left(\arraycolsep=2pt \begin{array}{c c c c c}
				e_2 & e_3 & \dots & e_n & -\vec{c}\
			\end{array}\right)
		\)}
\end{aligned}\)

% Krylov Matrix
\vspace{5pt}
\underline{Krylov Matrix}: \ \ \(\boxed{ 
	K_k = K_{n\times k}
	= \left(\begin{matrix}
		x_0 & x_1 & \dots & x_{k-1} 
	\end{matrix}\right)
	= \left(\begin{matrix}
		x_0 & A x_0 & \dots & A^{k-1} x_0
	\end{matrix}\right) 
} \hspace{15pt} \text{\scriptsize(\(x_0 \neq 0\) vector)}\)

% Krylov Subspace Def
\vspace{5pt}
\underline{Krylov Subspace}: \ \ \(
	\boxed{ \mathcal{K}_k = \text{span}(K_k) } 
	\ \rightarrow \ 
	\boxed{ \mathcal{K}_n = \text{span}(K_n) = \text{span}(A)}
\)

\vspace{10pt}
\(
	\hspace{10pt} 
	\left. \begin{aligned}
		AK_n &= \left(\begin{matrix}
				x_1 & x_2 & \dots & x_n
			\end{matrix}\right)\\[5pt]
		&= K_n \left(\begin{matrix}
				e_2 & e_3 & \dots & \vec{a}\
			\end{matrix}\right)
			\hspace{15pt} \text{\scriptsize\((\vec{a} = K_n^{-1}x_n)\)}\\[5pt]
		&= K_n C_n
	\end{aligned} \right\}
	\ \Rightarrow\ \boxed{ K_n^{-1} A K_n = C_n }
	\hspace{15pt}
	\begin{aligned}
		&\text{\scriptsize \(\bullet\) Assume \(\exists K^{-1}\)}\\[-5pt]
		&\text{\scriptsize \(\bullet\) \(C_n\) is Hessenberg}\\[-5pt]
		&\text{\scriptsize\(\bullet\ \)} \parbox[t]{5cm}{\scriptsize Sucessive columns of 
			\(K_n\) converge to \(v_1\), so problem is ill-conditioned}
	\end{aligned}
\)

%----------------------------------------------------
%----------------------------------------------------
% Arnoldi Iteration
\subsubsection{Arnoldi Iteration }

% Arnoldi Iteration is Modified, Column-Oriented Gram-Schmidt
\vspace{5pt}
\(\begin{aligned}
	&\boxed{ K_n = Q_n R_n } \\
	&\parbox[t]{9cm}{\scriptsize Arnoldi iteration is simply modified, column-oriented Gram-Schmidt, 
		the benefit being that \(Q_n\) is built by column-vector. \(R_n\) is also built by column when building \(Q_n\).}\\
	&\boxed{ 
		\begin{aligned}
			q_{n}^{[i+1]} \ &\equiv\ q_{n}^{[i]} - \widehat{q}_{i} \langle \widehat{q}_{i} | \widehat{q}_{n}^{\ [i]} \rangle\\[5pt]
			q_{n}^{[i]} \ &\equiv\ \widehat{q}_{i} \langle \widehat{q}_{i} | \widehat{q}_{n}^{\ [i]} \rangle + q_{n}^{[i+1]}\\
				&\equiv\ \text{\scriptsize\(\sum_{i'=i}^n \ \)} 
				\widehat{q}_{i'} \langle \widehat{q}_{i'} | \widehat{q}_{n}^{\ [i']} \rangle 
		\end{aligned} 
		}
		\ \left\{ \ 
		\boxed{
		\begin{aligned}
			q_{n} &\equiv q_{n}^{[n]}\\[2pt]
			q_{n}^{[1]} &\equiv A \widehat{q}_{n-1} \\
			&= \text{\scriptsize\(\sum_{i=1}^n \ \)} \widehat{q}_{i} \langle \widehat{q}_{i} | \widehat{q}_{n}^{\ [i]} \rangle\\
			q_1 &= x_0
		\end{aligned} 
		} 
		\right.
\end{aligned}\)
\hfill 
% K=QR Matrices
\text{\scriptsize\(\begin{aligned}
	Q_n^T &=
		\left(\arraycolsep=2pt \begin{array}{c c l c l}
			\widehat{q_1} &:& q_1 = x_0\\[-7pt]
			\widehat{q_2} &:& q_2 = q_2^{[2]} &=& \overbrace{ A \widehat{q_1} }^{q_2^{[1]}}
				- \ \widehat{q_1} (\widehat{q_1}^H \widehat{q_2}^{[1]})\\[5pt]
			\widehat{q_3} &:& q_3 = q_3^{[3]} &=& \underbrace{ A \widehat{q_2} 
				- \widehat{q_1} (r_{13}) }_{q_3^{[2]}} 
				- \ \widehat{q_2} \langle \widehat{q_2} | \widehat{q_3}^{[2]} \rangle \\[-15pt]
			\vdots &\\[-5pt]
			\widehat{q}_{k} &:& q_{k} = q_k^{[k]} &=& A \widehat{q}_{k-1} 
				- \text{\scriptsize\(\displaystyle \sum_{j=1}^{k-1}\)} \ \widehat{q_j} (r_{jk})
		\end{array}\right)
		\\
	R_n &=
		\left(\arraycolsep=3pt \begin{array}{c c c c c}
			\Vert q_1 \Vert 
				& \langle \widehat{q_1}|\widehat{q_2}^{[1]}\rangle 
				& \langle\widehat{q_1}|\widehat{q_3}^{[1]}\rangle
				& \dots 
				& \langle\widehat{q_1}|\widehat{q_n}^{[1]}\rangle\\[2pt]
			0 
				& \Vert q_2 \Vert 
				& \langle\widehat{q_2}|\widehat{q_3}^{[2]}\rangle 
				& \dots 
				& \langle\widehat{q_2}|\widehat{q_n}^{[2]}\rangle\\[2pt]
			0 
				& 0 
				& \Vert q_3 \Vert 
				& \dots & \langle\widehat{q_3}|\widehat{q_n}^{[3]}\rangle\\
			\vdots 
				& \vdots 
				& \ddots 
				& \ddots 
				& \vdots\\
			0 
				& 0 
				& \dots 
				& 0 
				& \Vert q_n \Vert
		\end{array}\right)
\end{aligned}\)}

% Building Hessenberg matrix, H 
\vspace{15pt}
\(\begin{aligned}
	&\begin{aligned}
			\Aboxed{ Q_n^{-1} A Q_n &= R_n C_n R_n^{-1} \ \equiv\ H } \hspace{10pt} \Big( \begin{gathered}
					\text{\scriptsize Still}\\[-7pt]
					\text{\scriptsize Hessenberg}
				\end{gathered} \Big) \\
			\Aboxed{ A Q_n &= Q_n H }
		\end{aligned}
		\\[5pt]
	&\parbox{8cm}{\scriptsize Instead of building \(R_n\) from \(K_n = Q_n R_n\), build \(H\) from \(AQ_n = Q_n H\) 
		(also by column as \(Q_n\) is built).}
		\\[5pt]
	&\begin{aligned}
		A \widehat{q_j} &= \text{\scriptsize\(\sum_{i=1}^n\)}\ \widehat{q_{i}} H_{ij}
			= \text{\scriptsize\(\sum_{i=1}^n\)}\ \widehat{q_{i}} \langle \widehat{q_i} | \widehat{q}_{j+1}^{\ [i]} \rangle \\
		\langle \widehat{q_i} | A | \widehat{q_j} \rangle &= H_{ij}  
			= \langle \widehat{q_i} | \widehat{q}_{j+1}^{\ [i]} \rangle 
	\end{aligned}
\end{aligned}\)
\hfill 
% H Matrix
\text{\scriptsize\(\begin{aligned}
	H &= \text{\scriptsize\(
		\left(\arraycolsep=2pt \begin{array}{c c c c c}
			\langle \widehat{q_1}|A|\widehat{q_1}\rangle 
				& \langle \widehat{q_1}|A|\widehat{q_2}\rangle
				& \langle \widehat{q_1}|A|\widehat{q_3}\rangle
				& \dots 
				& \langle \widehat{q_1}|\widehat{q_n}^{[1]}\rangle\\[2pt]
			\Vert q_2 \Vert 
				& \langle \widehat{q_2}|\widehat{q_3}^{[2]}\rangle
				& \langle \widehat{q_2}|\widehat{q_4}^{[2]}\rangle
				& \dots 
				& \langle \widehat{q_2}|\widehat{q_n}^{[2]}\rangle\\[2pt]
			0 
				& \Vert q_3 \Vert 
				& \langle \widehat{q_3}|\widehat{q_4}^{[3]}\rangle 
				& \dots 
				& \langle \widehat{q_3}|\widehat{q_n}^{[3]}\rangle\\[2pt]
			\vdots 
				& \ddots 
				& \ddots 
				& \ddots 
				& \vdots\\[2pt]
			0 
				& \dots 
				& 0 
				& \Vert q_{n} \Vert 
				& \langle \widehat{q}_{n}|\widehat{q}_{n+1}^{\ [n]}\rangle
		\end{array}\right)\)}
\end{aligned}\)}

%----------------------------------------------------
%
%
%----------------------------------------------------
\newpage
% Rayleigh-Ritz Procedure
\subsubsection{Rayleigh-Ritz Procedure}
\(\boxed{ 
	% Defines Q_k
	\begin{aligned}
		Q_n &= \left[\begin{matrix}
				Q_k & U_{k}
			\end{matrix}\right]
			\\[7pt]
		Q_k &= \text{\scriptsize\(\left[\arraycolsep=2pt\begin{array}{c c c c}
				\widehat{q_1} & \widehat{q_2} & ... & \widehat{q_k}
			\end{array}\right]\)}
			\\
		U_k &= \text{\scriptsize\(\left[\arraycolsep=2pt\begin{array}{c c c c}
				\widehat{q}_{k+1} & ... & \widehat{q}_n
			\end{array}\right]\)}
	\end{aligned} 
}
\hspace{5pt}
\Rightarrow
\hspace{5pt}
% Converts to "2x2" matrix
\begin{aligned}
	Q_n^H A Q_n &=  
		\left[\begin{matrix}
			Q_k^H \\[3pt]
			U_{k}^H
		\end{matrix}\right]
		A 
		\left[\begin{matrix}
			Q_k & U_{k}
		\end{matrix}\right]
		\\
	&= \left[\begin{matrix}
			Q_k^H A Q_k & Q_k^H A U_{k}\\[3pt]
			U_k^H A Q_k & U_k^H A U_{k}
		\end{matrix}\right]
		= H =
		\left[\begin{matrix}
			H_k & | | |\\[3pt]
			\tilde{H}_k & | | |
		\end{matrix}\right]
\end{aligned}
\hfill
% Define H_k
\begin{aligned}
	&\bullet\ \begin{gathered}[t]
			\boxed{H_k = Q_k^H A Q_k}\\[-5pt]
			\text{\scriptsize(Is hessenberg)}\\[-7pt]
			\text{\scriptsize(\(\sim\) Rayleigh Quot.)}
		\end{gathered}\\[3pt]
	&\bullet\ \boxed{ \tilde{H}_k =
		\text{\scriptsize\(\left[\arraycolsep=2pt\begin{array}{c c}
			\boldsymbol{0}^T & \Vert q_{k+1} \Vert \\[5pt]
			\mathcal{O} & \boldsymbol{0}
		\end{array}\right]\)} }
\end{aligned}\)

% Notes
\vspace{5pt}
\(\begin{aligned}
	&\bullet\ \text{\scriptsize \(Q_k\) are the first \(k\) vectors of \(Q_n\) 
		found through Arnoldi Iteration, called Arnoldi vectors.}\\
	&\bullet\ \text{\scriptsize The eigenvalues of \(H_k\) are called Ritz values, 
		and its eigenvectors premultiplied by \(Q_k\) are called Ritz vectors.}\\
	&\bullet\ \text{\scriptsize At iteration \(H_{k=n} = H\), then the Ritz values/vectors are 
		equal to the eigenvalues/vectors of \(A\).}\\
	&\bullet\ \text{\scriptsize The Ritz values/vectors of \(H_k\) converge to 
		\(A\)'s eigenvalues/vectors as \(k \rightarrow n\), and may be found from QR/Orthogonal iteration.}\\
	&\bullet\ \text{\scriptsize Extreme eigenvalues are found quickest; shifting + inverting 
		can be used to find interior eigenvalues.}\\
	&\bullet\ \text{\scriptsize \(\big( \Vert q_{k+1} \Vert = 0 \rightarrow \tilde{H}_k = 0 \big) \rightarrow H\) is 
		block triag., \(\mathcal{K}_k\) is an invar. subspace, and the Ritz values/vectors ARE \(A\)'s eigenvalues/vectors.}\\
	&\bullet\ \parbox[t]{17cm}{\scriptsize For each \(k\)th iteration, multiplying \(Aq\) costs \(\mathcal{O}(n^2)\), 
		orthogonalizing costs \(\mathcal{O}(kn)\), and computing the Ritz values/vectors costs \(\mathcal{O}(k^3)\).
		Storage is also needed for \(Q_k\) and \(H_k\).
		As this increases with \(k\), it's better to do a few iterations, then restart the Arnoldi process with a better
		\(x_0\) based on already computed info.}
\end{aligned}\)

%----------------------------------------------------
%----------------------------------------------------
% Lanczoz Iteration (when A is Symmetric)
\subsubsection{Lanczoz Iteration (when \(A=A^H\))}

% Explanation
\vspace{5pt}
\(\begin{aligned}
	&\boxed{ A = A^H \ \Rightarrow \ \begin{aligned}[t]
			Q_n^H A Q_n &= H_n \ \ \text{\scriptsize hessen. and symm. \(=\) tridiag.} \\[5pt]
			T_k &\equiv H_k \ \ \text{\scriptsize(common notation in practice)}
		\end{aligned} }\\[5pt]
	&\parbox[t]{9cm}{\scriptsize Similar to Arnoldi Iteration in building \(Q_n\) and \(H\) by column and using the 
		Rayleigh-Ritz procedure to estimate the eigenvalues/vectors, but much more simplified with an only three-term 
		reoccurance for \(Q_n\), and a symm. tridiag. \(H\) for storage and computation of the Ritz values/vectors.}
\end{aligned}\)
\hfill 
% AQ=QH Matrices
\text{\scriptsize\(\begin{aligned}
	Q_n^T &=
		\left(\arraycolsep=3pt \begin{array}{c c r c l}
			\widehat{q_1} &:& q_1 &=& x_0
			\\[-5pt]
			\widehat{q_2} &:& q_2 &=& 
				A \widehat{q_1}
				- \widehat{q_1} \langle \widehat{q_1} | A \widehat{q_1} \rangle
				- \overbrace{ \cancel{ \widehat{q_0} \langle q_1 | q_1 \rangle } }^{\equiv\ 0}
				\\[5pt]
			\widehat{q_3} &:& q_3 &=&  
				A \widehat{q_2} 
				- \widehat{q_2} \langle \widehat{q_2} | A \widehat{q_2} \rangle
				- \widehat{q_1} \langle q_2 | q_2 \rangle QR
				\\
			\vdots &\\
			\widehat{q}_{i+1} &:& q_{i+1} &=& 
				A \widehat{q}_{i} 
				- \widehat{q}_{i} \langle \widehat{q}_{i}  | A \widehat{q}_{i} \rangle
				- \widehat{q}_{i-1} \langle q_{i} | q_{i}  \rangle
			\end{array}\right)
		\\[10pt]
	H &= T = \text{\scriptsize\(
		\left(\arraycolsep=2pt \begin{array}{c c c c c}
			\langle \widehat{q_1}|A|\widehat{q_1}\rangle 
				& \langle \widehat{q_2}|\widehat{q_2}\rangle
				& 0
				& 0
				\\[2pt]
			\Vert q_2 \Vert 
				& \langle \widehat{q_2}|A|\widehat{q_2}\rangle 
				& \ddots
				& 0 
				\\[2pt]
			0 
				& \ddots
				& \ddots
				& \Vert q_{n} \Vert
				\\[5pt]
			0 
				& 0
				& \Vert q_{n} \Vert
				& \langle q_n | A| q_n \rangle
		\end{array}\right)\)}
\end{aligned}\)}

% Notes
\(\begin{aligned}
	&\bullet\ \text{\scriptsize Only the diagonal and subdiagonal of \(T\) need to be stored.}\\
	&\bullet\ \text{\scriptsize Extreme eigenvalues are found quickest; shifting + inverting 
		can be used to find interior eigenvalues.}\\
	&\bullet\ \text{\scriptsize \(\big( \Vert q_{k+1} \Vert = 0 \rightarrow \tilde{T}_k = 0 \big) \rightarrow T\) is 
		block triag., \(\mathcal{K}_k\) is an invar. subspace, and the Ritz values/vectors ARE \(A\)'s eigenvalues/vectors.}\\
	&\bullet\ \parbox[t]{17cm}{\scriptsize Finite-arithmatic means the vectors are more likely to lose orthogonality.
		Reorthogonalizing them might be expensive, so sometimes it is ignored if the Ritz values are close enough. Not 
		reorthogonalizing might lead to repeated eigenvalues.}
	\end{aligned}\)
\end{document}