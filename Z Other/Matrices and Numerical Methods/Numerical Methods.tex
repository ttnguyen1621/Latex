\documentclass[12pt]{article}
\usepackage[left=.75in, right=.75in, top=1in, bottom = 1in]{geometry}
\usepackage{enumitem, amssymb, amsmath, amsfonts, mathtools, parskip}
\usepackage{relsize}
\usepackage{cancel}
\usepackage{dashbox}

\newcommand{\hs}{\hspace{1pt}}
\newcommand{\rdiag}{\raisebox{0.5ex}{\scalebox{0.7}{$\diagdown$}}}
\newcommand{\hsvec}[1]{\vec{\hs #1}}
\newcommand{\dboxed}[1]{\dbox{\ensuremath{#1}}}
\newcommand{\mathscriptsize}[1]{\text{\scriptsize\(#1\)}}

\begin{document}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Solving Nonlinear Equation (1-D)
\section{Solving Nonlinear Equations [by Root Finding \(y=0\)]}

% Root multiplicity
\underline{Root Multiplicity, \(m\)}: \ \(0 = f(\bar{x}) = f'(\bar{x}) = ... = f^{(m-1)}(\bar{x}) 
    \hspace{15pt} \text{\scriptsize(Simple Root: \(m=1\))}\) 

% K-th iteration error
\vspace{10pt}
\underline{\(k\)-th Iteration Error}: \ \(\boxed{ e_k = x_k - \bar{x} }\)
% Convergence rate
\hfill
\underline{Convergence Rate, \(r\)}: \ \(\displaystyle 
    \boxed{ \lim_{k \rightarrow \infty} \frac{ \Vert e_{k+1} \Vert }{ \Vert e_k \Vert^r } = C }
    \hspace{15pt} \text{\scriptsize(\(0 < C < 1\) if \(r = 1\))} \)

%-------------------------------------------------------------
%-------------------------------------------------------------
% 1-Dimension
\vspace{5pt}
\subsection{One Dimension/Equation \hspace{15pt} {\scriptsize skipped a lot}}

% Interval Bisection
\vspace{10pt}
\underline{Iterval Bisection (Finding \(y=0\))}: \ \(
    \boxed{ [f(a) < 0] \ ,\  [f(b) > 0] \ , \ [f \text{ is cont.}] 
    \ \Rightarrow\ \exists m \text{ s.t. } f(m) = 0 }
\)

% Fixed-Point Iteration
\vspace{10pt}
\underline{Fixed-Point Iteration (Finding \(y=x\))}: \ \(\boxed{ \text{cont. } f(x) = 0 
    \ \Rightarrow\ \text{Find } g(x) = x } \rightarrow \boxed{ x_{k+1} = g(x_k) }\)

% ~ Banach FP Theorem conditions
\vspace{2pt}
\hspace{5pt} \(\begin{aligned}
    &\sim \text{Banach-Fixed Point Theorem (there are many FP theorems)}\\[5pt]
    &\bullet\ \text{\(g\) is Contractive (over a domain): \ \ dist}(g(x), g(y)) \leq q \cdot \text{dist}(x,y)
        \hspace{15pt} \text{\scriptsize\(q \in [0,1)\)}\\[5pt]
    &\bullet\ e_{k+1} = [x_{k+1} - \bar{x}] = [g(x_k) - g(\bar{x})] = g'(\xi_k) (x_k - \bar{x}) = g'(\xi_k) e_k \\[5pt]
    &\bullet\ \forall \vert g'(\xi_k) \vert < G < 1 
        \ \Rightarrow\ \Big( \vert e_{k+1} \vert \leq G \vert e_k \vert \leq ... \leq G^k \vert e_0 \vert \Big)
        \ \Rightarrow\ \lim_{k\rightarrow\infty} e_{k} = 0
        \hspace{15pt} \text{\scriptsize(\(G = \max g'\) over domain)} \\[5pt]
    &\bullet\ \lim_{k\rightarrow\infty} \vert g'(\xi_k) \vert = \boxed{ 
        \begin{gathered}[t]
            \Big( 0 < \vert g'(\bar{x}) \vert < 1 \Big) \\[-3pt]
            \text{\scriptsize(one contractive condition)}
        \end{gathered}
         = C } \hspace{15pt} \text{\scriptsize(\(r=1\))}\\[5pt]
    &\bullet\ \boxed{g'(\bar{x}) = 0} \ \Rightarrow\ \big[ g(x_k) - g(\bar{x}) \big] = \tfrac{g''(\xi_k)}{2}(x_k - \bar{x})^2 
        \ \Rightarrow\ \boxed{ \left\vert \tfrac{g''(\bar{x})}{2} \right\vert = C } 
        \hspace{15pt} \text{\scriptsize(\(r=2\) if \(\bar{x}\) is an \(m=2\) root of g)}
\end{aligned}\)

% Newton's Method
\vspace{10pt}
\underline{Newton's Method (Finding \(y=0\))}:

\hspace{5pt} \(\begin{aligned}
    &f(\bar{x}) = 0 = f(x_k + h_k) \ \approx\ f(x_k) + f'(x_k) h_k 
        \ \Rightarrow\ \boxed{ x_{k+1} = x_k + h_k = x_k - \tfrac{f(x_k)}{f'(x_k)} }\\[5pt]
    &\bullet\ \boxed{ g(x) \equiv x - \tfrac{f(x)}{f'(x)}}\ \Rightarrow \ 
        g(\bar{x}) = \bar{x} \ , \ \boxed{ g'(\bar{x}) = \tfrac{f(\bar{x}) f''(\bar{x})}{f'(\bar{x})^2} = 0 } \ , \ \boxed{ r = 2 } 
        \hspace{15pt} \text{\scriptsize(if \(\bar{x}\) is a simple root of \(f\))}\\[5pt]
    &\bullet\ \bar{x} \text{ is an } m > 1 \text{ root of } f \ \Rightarrow \ \boxed{ r = 1 \ , \ C = 1 - 1/m }
        \hspace{15pt} \text{\scriptsize(proof not given)}
\end{aligned}\)

% Secant Method
\vspace{10pt}
\underline{Secant Method/Linear Interpolation (Finding \(y=0\))}:

\hspace{5pt} \(\begin{aligned}
    &f'(x_k) \ \approx\ \tfrac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}} 
        \hspace{15pt} \parbox{3cm}{\scriptsize Approx. \(f'(x_k)\) with a secant line's slope} 
        \hspace{5pt} \ \Rightarrow \ 
        \boxed{ x_{k+1} = x_k + h_k = x_k - \tfrac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} f(x_k) }
        \\[5pt]
    &\bullet\ \boxed{ r = r_+ \approx 1.618 }: \ r_+^2 - r_+ - 1 = 0 \hspace{15pt} \text{\scriptsize(proof hard)}\\[5pt]
    &\bullet\ \text{\scriptsize Lower cost of iter. offsets the larger number of iter. compared to Newton's Method with derivatives}
\end{aligned}\)

%-------------------------------------------------------------------------------------------------------------------------------------
%
%
%
\newpage
% Inverse Parabolic Interpolation
\underline{Inverse Parabolic Interpolation}: \ Use 3 pts to approx. an inverse [sideways] parabola

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% m-dimension/equations
\vspace{15pt}
\subsection{\(m\) Dimensions/System of Equations \hspace{15pt} {\scriptsize stuff skipped}}

% Newton's Method
\vspace{10pt}
\underline{Newton's Method (Solving \(\vec{y}=0\))}:

\hspace{5pt} \(\begin{aligned}
    &\boxed{ \left\{ J_f(\vec{x}) \right\}_{ij} = \tfrac{\partial f_i(\vec{x})}{\partial x_j} }: \ \
        \boxed{ J_f(\vec{x}_k) \vec{h}_k = - \vec{f}(x_k) }
        \ \Rightarrow\ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k = \vec{x}_k - J_f(\vec{x}_k)^{-1} \vec{f}(\vec{x}_k) }
        \\[5pt]
    &\bullet\ \boxed{ \vec{g}(\vec{x}) \equiv \vec{x} - J_f(\vec{x})^{-1} \vec{f}(\vec{x}) }\ \Rightarrow \ 
        \begin{aligned}
            J_g(\bar{x}) &= \begin{gathered}[t]
                    \cancel{ I - J_f(\bar{x})^{-1} J_f(\bar{x}) }\\[-3pt]
                    \text{\scriptsize(if \(J_f(\bar{x})\) is nonsingular)}
                \end{gathered} 
                + \sum_{i=1}^n H_i(\bar{x}) f_i(\bar{x})
                \hspace{20pt} \parbox[t]{2.2cm}{\scriptsize \vspace{-10pt} 
                \(H_i =\) component matrix of the \\ tensor, \(D_x J_f(\bar{x})\)}
                \\[5pt]
            &= \mathcal{O} \ \Rightarrow \ \boxed{ r = 2 }
                \hspace{15pt} \text{\scriptsize(uh... idk)}
        \end{aligned}
        \\
    &\bullet\ \text{\scriptsize \(LU\) fact. of the Jacobian costs \(\mathcal{O}(n^3)\)}
\end{aligned}\)

% Secant Method
\vspace{10pt}
\underline{Broyden's [Secant Updating] Method (Solving \(\vec{y}=0\))}:

\hspace{5pt} \(\begin{aligned}
    &\boxed{ B_k \vec{h}_k = - \vec{f}(x_k) }
        \ \Rightarrow \ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
        \ , \ \boxed{ B_{k+1} = B_k + \tfrac{f(x_{k+1}) h_k^T}{h_k^T h_k} }
        \hspace{15pt} \text{\scriptsize(cost is \(\mathcal{O}(n^3)\))}
        \\[5pt]
    &\bullet\ B_{k+1} (\vec{x}_{k+1} - \vec{x}_k) \ =\ B_{k+1} \vec{h}_k \ =\ f(\vec{x}_{k+1}) - f(\vec{x}_k)\\[5pt]
    &\bullet\ B_{k} \text{\scriptsize \ factorization is updated to factorization of } 
        B_{k+1} \text{\scriptsize \ at cost } \mathcal{O}(n^2) \text{\scriptsize \ instead of directly from the above eq.}\\[5pt]
    &\bullet\ \text{\scriptsize Lower cost of iter. offsets the larger number of iter. compared to Newton's Method with derivatives}
\end{aligned}\)

%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
% Optimization/Minimum Finding
\vspace{10pt}
\section{Optimizing [By Finding \(\min f(\vec{x}) = f(\bar{x})\)]}

\subsection{Function Shape and Convexity}

\vspace{5pt}
% Coercivity
\parbox{.37\textwidth}{
    \underline{Coercive}: \ \(\boxed{ \lim_{x \rightarrow \pm \infty} f(x) = \infty }\)
}
% Unimodal
\parbox{.62\textwidth}{
    \underline{Unimodal}: \ \(
        \begin{gathered}
            a \leq \bar{x} \leq b \\[-5pt]
            x_1 < x_2    
        \end{gathered}
        \ : \ 
        \arraycolsep=2pt \boxed{ \begin{array}{c c l}
            x_2 < \bar{x} &\ \rightarrow\ & f(x_1) > f(x_2)\\ 
            \bar{x} < x_1 &\ \rightarrow\ & f(x_1) < f(x_2)
        \end{array} }
    \)
}

% Global Minimum
\vspace{5pt}
\(\begin{aligned}
    &\underline{ \exists \text{ global } \min f \text{ if} }\\[3pt]
    &\hspace{10pt} \begin{aligned}
            &\bullet\ \text{\scriptsize cont. \(f\) on a closed and bounded set}\\
            &\bullet\ \text{\scriptsize cont. \(f\) is coercive on a closed, unbounded set}\\
            &\bullet\ \text{\scriptsize cont. \(f\) on a set and has a nonempty, closed, and bounded sublevel set}\\
            &\bullet\ \text{\scriptsize domain set is unbounded: cont. \(f\) is 
                coercive \(\Leftrightarrow\) all sublevel sets are bounded}
        \end{aligned}
\end{aligned}\)

% Convexity
\vspace{8pt}
\(\begin{aligned}
    &\underline{ \text{\(f\) is convex [on a convex set]} }: \\[3pt]
    &\hspace{10pt} \begin{aligned}
            &\bullet\ \text{\scriptsize any sublevel set is convex}\\
            &\bullet\ \text{\scriptsize any local min. is a global min}
        \end{aligned}
\end{aligned}\)
\hfill
\(\begin{aligned}
    &\underline{ \text{\(f\) is strictly convex [on a convex set]} }:\\[3pt]
    &\hspace{10pt} \begin{aligned}
            &\bullet\ \text{\scriptsize any local min. is a unique global min.}\\
            &\bullet\ \text{\scriptsize if set is unbounded: \(f\) is 
                coercive \(\Leftrightarrow\) \(f\) has a unique global min.} 
        \end{aligned}
\end{aligned}\)

%-----------------------------------------------------------------------------------------------------------------------------------
%
%
%-----------------------------------------------------------------------------------------------------------------------------------
\newpage

% Derivative Tests of Un/constrained Optimization
\subsection{Derivative Tests (Gradient, Jacobian, Hessian) and Lagrangians}

% Function requirements
\vspace{5pt}
\underline{Req.} : \ \( \boxed{ 
    \text{\scriptsize cont. } f(\bar{x}) = \min f 
    \ ,\ \text{\scriptsize cont. } \vec{\nabla} f(\bar{x})
    \ ,\ \text{\scriptsize cont. }  H_f(\bar{x})
} \)

% Taylor's Theorem
\vspace{5pt}
\underline{Taylor's Theorem}: \( \boxed{
    \arraycolsep=2pt \begin{array}{c c c c l c}
        f{\scriptstyle(\bar{x} + \vec{s}\hspace{1pt})} - f{\scriptstyle(\bar{x})} &=
            &\vec{\nabla} \begin{gathered}[b]
                    {\scriptstyle \alpha_1 \hspace{1pt} \in\hspace{1pt} (0,1)}
                        \\[-5pt]
                    f{\scriptstyle(\bar{x} + \alpha_1 \vec{s}\hspace{1pt})}
                \end{gathered} \cdot \vec{s}
            &=
            &\vec{\nabla} f{\scriptstyle(\bar{x})} \cdot \vec{s} 
                + \tfrac{1}{2} 
                \langle \vec{s}\hspace{1pt} | 
                \begin{gathered}[b]
                    {\scriptstyle \alpha_2 \hspace{1pt}\in\hspace{1pt} (0,1)}
                        \\[-5pt]
                    H_f{\scriptstyle(\bar{x} + \alpha_2 \vec{s} \hspace{1pt})}
                \end{gathered} 
                | \vec{s} \hspace{1pt} \rangle
            &\geq\ 0
            \\[5pt]
        f{\scriptstyle(\vec{x} + s \hat{u})} - f{\scriptstyle(\vec{x})} &=
            &\vec{\nabla} f{\scriptstyle(\vec{x} + \alpha_1 s \hat{u})} \cdot s \hat{u}
            &=
            &\vec{\nabla} f{\scriptstyle(\vec{x})} \cdot \vec{s} 
                + \tfrac{s^2}{2} 
                \langle \hat{u} | 
                H_f{\scriptstyle(\vec{x} + \alpha_2 \vec{s} \hspace{1pt})}
                | \hat{u} \rangle
    \end{array}
}\)

% Taylor Results
\hspace{5pt} \(\begin{aligned}[t]
    &\bullet 
        \ \lim_{s \rightarrow 0} 
        \left( 
            \tfrac{ f{\scriptstyle(\vec{x} + \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} }{ s }
            = \vec{\nabla} f{\scriptstyle(\vec{x} + \alpha_1 s \hat{u})} \cdot \cancel{s} \hat{u} 
        \right) 
        \Rightarrow \left( 
            {\scriptstyle \vec{\nabla} f{\scriptstyle(\bar{x})} \cdot \hat{u} \ \geq\ 0}
            \rightarrow 
            \boxed{ {\scriptstyle \vec{\nabla} f{\scriptstyle(\bar{x})} \cdot \vec{s} \ \geq\ 0} } 
        \right)
        \hspace{3pt} , \hspace{5pt} 
        \boxed{ \parbox{3.7cm}{
            \scriptsize Cauchy-Schwarz \(\rightarrow\) \\[3pt]
            \( \max \vec{\nabla} f{\scriptstyle(\vec{x})} \cdot \hat{u}\)
            if \(\vec{u} = \vec{\nabla} f(\vec{x})\)
        } }
        \\[5pt]
    &\bullet
        \begin{aligned}[t]
            &\boxed{ \vec{u} = \mp \vec{\nabla} f{\scriptstyle(\vec{x})} }
                \Rightarrow \ 
                \lim_{s \rightarrow 0} 
                \left( 
                    \tfrac{ f{\scriptstyle(\vec{x} + \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} }{ s }
                    = \mp \cancel{s} \tfrac{
                        \vec{\nabla} f{\scriptstyle(\vec{x} + \alpha_1 s \hat{u})}
                        \cdot 
                        \vec{\nabla} f{\scriptstyle(\vec{x})} 
                    }{
                        \Vert \vec{\nabla} f{\scriptstyle(\vec{x})} \Vert
                    }
                \right)
                = \mp \Vert \vec{\nabla} f{\scriptstyle(\vec{x})} \Vert 
                \ \tfrac{<}{>}\ 0
                \hspace{15pt}
                \boxed{ 
                    \parbox{2.9cm}{
                        \scriptsize if \(\pm \vec{\nabla} f(\vec{x}) \neq 0\),
                        its dir. is an ascent/descent.
                    }
                }
        \end{aligned}
        \\[5pt]
    &\bullet 
        \ \lim_{s \rightarrow 0} 
        \left(
            \tfrac{ 
                f{\scriptstyle(\vec{x} + \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} 
                + f{\scriptstyle(\vec{x} - \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} 
            }{ s^2 }
            =
            \tfrac{\scriptstyle 
                \langle \hat{u} | 
                H_f(\vec{x} + \alpha_2 \vec{s} \hspace{1pt}) + H_f(\vec{x} - \alpha_3 \vec{s} \hspace{1pt}) 
                | \hat{u} \rangle
            }{ 2 }
        \right)
        = {\scriptstyle \langle \hat{u} | H_f(\vec{x}) | \hat{u} \rangle}
        \ \Rightarrow \
        \boxed{ {\scriptstyle \langle \vec{s}\hspace{1pt} | H_f(\bar{x}) | \vec{s}\hspace{1pt} \rangle \ \geq\ 0} } 
\end{aligned}\)

% Unconstrained Optimization Conditions
\vspace{15pt}
\subsubsection{Unconstrained Optimization Conditions}

% Taylor's Theorem
\vspace{5pt}
\(
    \bullet\ \begin{aligned}
        &\boxed{ f(\bar{x}) = \min f }
            \ \Leftrightarrow \ 
            \text{\scriptsize\(
                \left( 
                    \hspace{5pt}
                    \begin{gathered}
                        \vec{\nabla} f (\bar{x}) \cdot \vec{s} \geq 0
                            \ , \ \vec{\nabla} f (\bar{x}) \cdot - \vec{s} \geq 0\\
                        \ \Rightarrow\ \boxed{ \vec{\nabla} f (\bar{x}) = 0 }
                    \end{gathered}
                    \hspace{10pt} , \hspace{10pt}
                    \begin{gathered}
                        \vec{u} = - \vec{\nabla} f(\bar{x})\\
                        \ \Rightarrow\ \boxed{ \vec{\nabla} f (\bar{x}) = 0 }
                    \end{gathered}
                    \hspace{10pt} , \hspace{10pt}
                    \boxed{ 
                        \begin{gathered}
                            \text{\scriptsize(for strict convexity)}\\
                            \langle \vec{s}\hspace{1pt} | H_f(\bar{x}) | \vec{s}\hspace{1pt} \rangle > 0
                        \end{gathered} 
                    }
                    \hspace{5pt}
                \right)
            \)}
    \end{aligned}
\)

% Optimization
\vspace{10pt}
\(
    \begin{aligned}[t]
        &\text{\underline{Optimization}} 
            \hspace{10pt} 
            f: \ \ \mathbb{R}^n \rightarrow \mathbb{R}
            \hspace{20pt}
            \boxed{ \min f(\vec{x}) = y} 
            \\[10pt]
        % Lagrangian and Hessian
        &\hspace{10pt} 
            \boxed{ \mathcal{L}(\vec{x}) = f(\vec{x}) }
            \hspace{10pt} , \hspace{12pt}
            \boxed{ \nabla \mathcal{L}(\bar{x}) = 0 }
            \hspace{10pt} , \hspace{12pt}
            \boxed{ 
                H_\mathcal{L} = \nabla_{xx} \mathcal{L}: \ \
                \langle s | H_\mathcal{L}(\bar{x}) | s \rangle > 0 
            }
            \ \Rightarrow \ \boxed{ y = f(\bar{x}) }
    \end{aligned}
\)

% Constrained Optimization Conditions
\vspace{15pt}
\subsubsection{Constrained Optimization Conditions}

% Taylor's Theorem
\vspace{5pt}
\(
    \bullet\ 
    \boxed{
        \begin{gathered}
            \vec{s} = \text{\scriptsize feasable direction} \\[-3pt]
            {\scriptstyle f(\bar{x}) = \min f \text{ \ given \ } g,\ h}
        \end{gathered}
    }
    \ \Leftrightarrow \ 
    \left(\hspace{5pt}
        \boxed{ \vec{\nabla} f(\bar{x}) \cdot \vec{s} \ \geq\ 0 }
        \hspace{5pt} , \hspace{5pt} 
        \boxed{ \langle \vec{s} \hspace{1pt} | H_f(\bar{x}) | \vec{s} \hspace{1pt} \rangle \ \geq\ 0 }
    \hspace{5pt}\right)
\)

% Optimization
\vspace{10pt}
\(
    \text{\underline{Optimization}}
    \hspace{10pt}
    \text{\scriptsize\(
        \begin{aligned}
            f:& \ \ \mathbb{R}^n \rightarrow \mathbb{R}\\
            g:& \ \ \mathbb{R}^n \rightarrow \mathbb{R}^{m}\\
            h:& \ \ \mathbb{R}^n \rightarrow \mathbb{R}^p\\[5pt]
        \end{aligned}
    \)}
    \hspace{20pt}
    \boxed{ 
        \min f(\vec{x}) = y 
        \ \ \ \text{\scriptsize w/} \ \ \
        \left(\begin{matrix}
            \vec{g} \text{\scriptsize\((\vec{x})\)} = 0\\
            \vec{h} \text{\scriptsize\((\vec{x})\)} \leq 0
        \end{matrix}\right) 
    }
    \hspace{20pt}
    {\scriptstyle
        \begin{aligned}
            \text{\underline{active}} :     &\ \ h_i(\bar{x}) = 0
                \\
            \text{\underline{inactive}} :   &\ \ h_i(\bar{x}) < 0
                \ \rightarrow\ 
                \begin{gathered}[b]
                    \text{\scriptsize(see KKT)}\\[-3pt]
                    \bar{\mu}_i = 0
                \end{gathered}
                \\
        \end{aligned}
    }
\)

\hspace{10pt} \(
    % Lagrangian
    \begin{aligned}
        &\boxed{ 
                \begin{aligned}
                    &\mathcal{L} {\scriptstyle(\vec{x}, \vec{\lambda}, \vec{\mu})}
                        = f(\vec{x}) + \vec{\lambda} \cdot \vec{g}(\vec{x})  + \vec{\mu} \cdot \vec{h}(\vec{x})  \\
                    &\hspace{10pt} = f + \text{\scriptsize\(\sum_i^m\)}\ \lambda_i g_i
                        + \cancel{ \text{\scriptsize\(\sum_i^p\)}\ \mu_i h_i }
                        \hspace{10pt} \begin{gathered}
                            \text{\scriptsize (KKT) if} \\[-9pt]
                            {\scriptstyle \vec{x} \ =\ \bar{x}}
                        \end{gathered}
                \end{aligned}
            }
            \hspace{7pt} , \hspace{7pt}
            % Gradient
            \boxed{
                \begin{aligned}
                    \nabla \mathcal{L} {\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})}
                    &= \text{\scriptsize\(
                            \left( \begin{matrix}
                                \nabla_x \mathcal{L} = 0\\
                                \nabla_\lambda \mathcal{L} = 0\\
                                \nabla_\mu \mathcal{L} \leq 0
                            \end{matrix} \right)
                        \)}
                        = \text{\scriptsize\(
                            \left( \begin{matrix}
                                \nabla f \text{\scriptsize\((\bar{x})\)} 
                                    + J_g^T \text{\scriptsize\((\bar{x})\)} \bar{\lambda} 
                                    + J_h^T \text{\scriptsize\((\bar{x})\)} \bar{\mu}\\
                                \vec{g} \text{\scriptsize\((\bar{x})\)}\\
                                \vec{h} \text{\scriptsize\((\bar{x})\)}
                            \end{matrix} \right)
                        \)}
                \end{aligned}
            } \\[5pt]
        % Hessian
        &H_\mathcal{L} {\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})} = 
            \text{\scriptsize\(\left( \begin{matrix}
                \nabla_{xx} \mathcal{L} & \nabla_{x\lambda} \mathcal{L} & \nabla_{x\mu} \mathcal{L}\\
                \nabla_{\lambda x} \mathcal{L} & \nabla_{\lambda\lambda} \mathcal{L} & \nabla_{\lambda\mu} \mathcal{L}\\
                \nabla_{\mu x} \mathcal{L} & \nabla_{\mu\lambda} \mathcal{L} & \nabla_{\mu\mu} \mathcal{L}  
            \end{matrix}\right)\)}
            =
            \begin{gathered}[t]
                \text{\scriptsize\(
                        \left( \begin{matrix}
                            \nabla_{xx} \mathcal{L} & J_g^T & J_{h}^T\\
                            J_g & 0 & 0\\
                            J_{h} & 0 & 0 
                        \end{matrix}\right)
                    \)}
                    \\
                \text{\scriptsize(can't be pos. def.)}
            \end{gathered}
            \hspace{3pt} , \hspace{10pt}
            \boxed{
                \nabla_{xx} \mathcal{L} {\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})}
                = H_f + \text{\scriptsize\(\sum_i^m\)}\ \bar{\lambda}_i H_{g_i}
                + \text{\scriptsize\(\sum_i^{\text{act} \leq p}\)}\ \bar{\mu}_i H_{h_i}
            }
    \end{aligned}
\)

%---------------------------------------------------------------------------------------------------------------------------------
%
%
%
\newpage
\hspace{10pt}\(
    % Cases for Solution
    \begin{aligned}
        &\bullet\ \underline{ \text{\scriptsize Assume \(m\leq n\) (not overdetermined)} }\\
        &\bullet\ y = f(\bar{x}) : \ \nabla \mathcal{L}{\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})} \ ...
            \ , \ 
            \boxed{ p = 0 :\ \ Z^T (\nabla_{xx} \mathcal{L}) Z > 0 
            \hspace{15pt} {\scriptstyle \text{col. of } Z \ =\ \text{basis of null}(J_g) } }
            \\[5pt]
        &\bullet\ \underline{ \text{\scriptsize Assume \(h_i\) don't contradict each other?} }
            \hspace{15pt}
            \underline{ \text{\scriptsize Assume full rank\((J_{h_\text{act}})\)} }\\
        &\bullet\ y = f(\bar{x}) : \ \nabla \mathcal{L}{\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})} \ ...
            \ , \ 
            \boxed{ p > 0 ,\ \text{\scriptsize Karush-Kuhn-Tucker (KKT)}:\ \ 
            \bar{\mu}_i \geq 0 ,\ \bar{\mu}_i h_i(\bar{x}) = 0 }
            \hspace{10pt} \begin{gathered}
                \text{\scriptsize (2nd deriv. cond. }\\[-9pt]
                \text{\scriptsize not given)}
            \end{gathered}\\
    \end{aligned}    
\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% One Dimension Minimum
\vspace{5pt}
\subsection{Unconstrained One Dimension/Independent Variable}

% Interval Golden Search
\underline{[Interval] Golden-Section Search (if Unimodal)}: 
\ \(\boxed{ \tau^2 = 1 - \tau = .382 }\ ,\ \boxed{r = 1}\ ,\ \boxed{C = \tau}\)

\vspace{5pt}
\hspace{10pt}\(
    [a < x_1 < x_2 < b]: \ 
    \left\{ \ \arraycolsep=0pt \begin{array}{r c c c c c c c c}
        f(x_1) > f(x_2)     &\ \ \rightarrow\ \ & \big[ & x_1  &\ <\ & x_2 \ <\ x_1 + \tau (b-x_1)    &\ <\ & b   & \big] \\[5pt]
        f(x_1) \leq f(x_2)  &\ \ \rightarrow\ \ & \big[ & a    &\ <\ \ & a + (1-\tau)(x_2 - a) \ <\ x_1 &\ \ <\ \ & x_2 & \big]
    \end{array}\right.
\)

% Newton's Method
\vspace{15pt}
\underline{Newton's Method}: \ \(
    f(\bar{x}) = f(x+h) 
    \ \approx\ f(x) + f'(x) h + \tfrac{1}{2} f''(x) h^2 
    = g(h)
\) 

\vspace{5pt}
\hspace{10pt}\(
    g(\tfrac{-b}{2a}) = \min g \ \text{\scriptsize(or max)}
    \ \Rightarrow\ \boxed{ x_{k+1} = x_k + h_k = x_k - \tfrac{b}{2a} = x_k - \tfrac{f'(x)}{f''(x)} } \ ,\ \boxed{r=2}
\)

% Linear Interpolation
\vspace{15pt}
\underline{Sucessive Linear Interpolation [Secant Method]}: \ {\scriptsize Not useful, since lines have no unique minimun}

% Sucessive Parabolic Interpolation
\vspace{5pt}
\underline{Sucessive Parabolic Interpolation}: \ {
    \scriptsize Use 3 pts to approx. a parabola w/ \(\boxed{r = 1.324}\) \ (not guarenteed)
}

%---------------------------------------------------------------
%
%
%
% Unconstrained m-dimensions
\vspace{5pt}
\subsection{Unconstrained \(m\)-Dimensions/Independent Variables}

% Steepest Descent
\vspace{5pt}
\underline{Steepest [Gradient] Descent/Line Search (go down \( -\nabla f{\scriptstyle(\vec{x}_k)} \))}: 

\hspace{10pt} \(\begin{aligned}
    % Descent Algorithm
    &\boxed{ \phi(\alpha) = f \big( \vec{x} - \alpha \vec{\nabla} f{\scriptstyle(\vec{x})} \big) }
        \ , \
        \boxed{ \phi(\alpha_k) = \min \phi }
        \ \Rightarrow \ 
        \boxed{ \vec{x}_{k+1} = \vec{x}_k - \alpha_k \vec{\nabla} f(\vec{x}_k) } 
        \hspace{20pt}
        \boxed{r=1 \ ,\ C \text{\scriptsize varies} }
        \\[10pt]
    % Notes
    &\bullet
        \ \vec{\nabla} f{ \scriptstyle ( \vec{x}_k ) } \cdot \vec{\nabla} f{ \scriptstyle ( \vec{x}_{k+1} ) } = 0
        \ \Rightarrow \ 
        \text{\scriptsize Path will zig-zag to the min. (not too efficient)}
\end{aligned}\)

% Newton's Method
\vspace{5pt}
\underline{Newton's Method}: \ \(
    f(\bar{x}) = f(\vec{x} + \vec{h})
    \ \approx \ f(\vec{x})
    + \vec{\nabla} f(\vec{x}) \cdot \vec{h} 
    + \tfrac{1}{2} \langle \vec{h} | H_f(\vec{x}) | \vec{h} \rangle
\)

\hspace{10pt}\(\begin{aligned}
    &\boxed{ H_f(\vec{x}_k) \hspace{1pt} \vec{h}_k = - \vec{\nabla} f(\vec{x}_k) } 
        \ \Rightarrow \ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
        \hspace{10pt}, \hspace{10pt} \boxed{ r=2 }
\end{aligned}\)

% Secant Updating
\vspace{10pt}
\underline{BFGS [Secant Updating] Method}: \ \(
    \boxed{ B_k \vec{h}_k = - \vec{\nabla} f(\vec{x}_k) } \ ,\ \boxed{ \vec{y}_k = \vec{\nabla} f(x_{k+1}) - \vec{\nabla} f(x_{k}) }     
\)

\hspace{5pt} \(\begin{aligned}
    &\Rightarrow \ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
        \ , \ \boxed{ 
            B_{k+1} = B_k 
            + \tfrac{| y_k \rangle \langle y_k |}{\langle y_k | h_k \rangle} 
            - \tfrac{B_k | h_k \rangle \langle h_k | B_k}{\langle h_k | B_k | H_k \rangle}
        }
        \hspace{15pt} \text{\scriptsize(cost is \(\mathcal{O}(n^3)\))}
        \\
    &\bullet\ \text{\scriptsize Preserves symmetry and pos. def.}\\
    &\bullet\ B_{k} \text{\scriptsize \ factorization is updated to factorization of } 
        B_{k+1} \text{\scriptsize \ at cost } \mathcal{O}(n^2) \text{\scriptsize \ instead of directly from the above eq.}
        \\
    &\bullet\ \text{\scriptsize Lower cost of iter. offsets the larger number of iter. compared to Newton's Method with derivatives}
\end{aligned}\)

%-------------------------------------------------------------------------------------------------------------------------------------
%
%
%
\newpage
% Conjugate Gradient 
\underline{Conjugate Gradient [Line Search]} : \ 

\vspace{5pt}
\hspace{10pt}\(
    \boxed{ 
        \vec{h}_{k+1} = \vec{\nabla} f(\vec{x}_{k+1}) 
        - \tfrac{ 
            \vec{\nabla} f(\vec{x}_{k+1}) \cdot \vec{\nabla} f(\vec{x}_{k+1})
        }{
            \vec{\nabla} f(\vec{x}_{k}) \cdot \vec{\nabla} f(\vec{x}_k)
        } 
        \ \vec{h}_{k} 
    } 
    \hspace{10pt}
    \text{\scriptsize(Fletcher and Reeves)}
    \ \Rightarrow\ \boxed{ \vec{x}_{k+1} = \vec{x}_k - \alpha_k \vec{h}_k }
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \text{\scriptsize Seq. of conj. (where \((a,b) = \langle a | H_f | b \rangle \)) search directions
        implicitly accumulates info. about \(H_f\).}
        \\
    &\bullet\ \text{\scriptsize Better for nonlin. to use } \
        \boxed{ 
            \vec{h}_{k+1} = \vec{\nabla} f(\vec{x}_{k+1}) 
            - \tfrac{ 
                \vec{\nabla} f(\vec{x}_{k+1}) \cdot \vec{\nabla} f(\vec{x}_{k+1}) 
                - \vec{\nabla} f(\vec{x}_{k}) \cdot \vec{\nabla} f(\vec{x}_{k+1})
            }{
                \vec{\nabla} f(\vec{x}_{k}) \cdot \vec{\nabla} f(\vec{x}_k)
            } 
            \ \vec{h}_{k} 
        }
        \hspace{10pt} \text{\scriptsize(Polak and Ribiere)}
        \\
    &\bullet\ \text{\scriptsize Restart algorithm after \(n\) iter. using last point as the new initial; 
        a quadratic func. finishes after at most \(n\) iter.}
\end{aligned}\)

%--------------------------------------------------------------
%--------------------------------------------------------------
% Nonlinear Least Squares
\subsubsection{Nonlinear Least Squares, \(\big\{ \min \Vert \vec{r}(\vec{x}) \Vert^2 : \
    \vec{f}{\scriptstyle(\vec{a}, \vec{x})} + \vec{r}{\scriptstyle(\vec{x})} = \vec{b} \hspace{1pt} \big\}    
\)}

% Diff. between Linear and Nonlinear Least Squares
\vspace{10pt}
\[
    \arraycolsep=2pt \begin{array}{c c c}
        \text{Linear Least Squares} 
            &
            &\text{Nonlinear Least Squares}
            \\[5pt] 
        \left(\begin{matrix}
                \vdots\\
                - \vec{a}_i -\\
                \vdots
            \end{matrix}\right)
            \left(\begin{matrix}
                |\\
                \vec{x}\\
                |
            \end{matrix}\right)
            +
            \left(\begin{matrix}
                |\\
                \vec{r}\\
                |
            \end{matrix}\right) 
            =
            \left(\begin{matrix}
                |\\
                \vec{b}\\
                |
            \end{matrix}\right) 
            &\ \ \ \Rightarrow \ \ \
            &\left(\begin{matrix}
                    |\\
                    \vec{f}{\scriptstyle (\vec{a}, \vec{x}) }_i\\
                    |
                \end{matrix}\right)
                +
                \left(\begin{matrix}
                    |\\
                    \vec{r}\\
                    |
                \end{matrix}\right) 
                =
                \left(\begin{matrix}
                    |\\
                    \vec{b}\\
                    |
                \end{matrix}\right) 
    \end{array}
\]

% Normal Algorithm
\vspace{5pt}
\(
    \begin{gathered}
        \boxed{ \phi(\vec{x}) \ \equiv\ \tfrac{1}{2} \vec{r} \cdot \vec{r} \hspace{1pt} }
            \ , \
            \boxed{ - \vec{\nabla} \phi(\vec{x}) = - J_r^T \vec{r} }
            \\[5pt]
        \boxed{ H_\phi(\vec{x}) = J_r^T J_r + \sum_i H_{r_i} \vec{r}_i }
    \end{gathered}
    \ \ : \ \
    \begin{gathered}
        \text{\scriptsize Newton's Method}\\
        \boxed{ H_\phi(\vec{x}_k) \hspace{1pt} \vec{h}_k = - \vec{\nabla} \phi(\vec{x}_k) }\\[-3pt]
        \text{\scriptsize(usually expensive to compute)}
    \end{gathered}
    \ \Rightarrow \ 
    \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
\)

% Gauss-Newton Method
\vspace{15pt}
\underline{Gauss-Newton Method}: \ \(
    \text{If \(\vec{r}\) is small} 
    \ \Rightarrow\ H_\phi \approx J_r^T J_r
    \ \Rightarrow\ \boxed{
        J_r^T ( J_r \vec{h}_k ) = - J_r^T \vec{r}{\scriptstyle(\vec{x}_k)}
        \hspace{20pt} \begin{gathered}
            \text{\scriptsize System of}\\[-5pt]
            \text{\scriptsize Normal Equations}
        \end{gathered} 
    }
\)

% Levenberg-Marquardt Method
\vspace{15pt}
\underline{Levenberg-Marquardt Method (Gauss-Newton + Line Search)}: 

\vspace{5pt}
\(
    \hspace{10pt}\begin{aligned}
        &\boxed{ 
                (J_r^T J_r + \mu_k I) \vec{h}_k = - J_r^T \vec{r}{\scriptstyle(\vec{x}_k)} 
                \ \Rightarrow \ 
                \vec{x}_{k+1} = \vec{x} + \vec{h_k}
            }
            \\[5pt]
        &\Rightarrow\ \boxed{
                \left(\begin{matrix}
                    J_r^T{\scriptstyle(\vec{x})} & \sqrt{ \mu_k } I
                \end{matrix}\right)
                \left(\begin{matrix}
                    J_r{\scriptstyle(\vec{x})} \\
                    \sqrt{ \mu_k } I
                \end{matrix}\right)
                \vec{h}_k
                = 
                \left(\begin{matrix}
                    J_r^T{\scriptstyle(\vec{x})} & \sqrt{ \mu_k } I
                \end{matrix}\right)
                \left(\begin{matrix}
                    - \vec{r}{\scriptstyle(\vec{x}_k)} \\
                    0
                \end{matrix}\right)
            }
    \end{aligned}
\)
\hfill
\(
    \begin{aligned}
        &\text{\underline{\scriptsize Regularization}}\\
        &\bullet\ \parbox[t]{4.4cm}{\scriptsize Replacing \(H_{r_i} \vec{r}_i\) terms with a \\ scalar mult. of \(I\).} \\
        &\bullet\ \parbox[t]{4.4cm}{\scriptsize Shifting the Gauss-Newton Hessian to make it pos. def (or boosting its rank).}\\
    \end{aligned}
\)

%-----------------------------------------------------------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------------------------------
% Constrained Optimization
\vspace{10pt}
\subsection{Constrained \(m\)-Dimensions/Independent Variables}

\(
    \begin{gathered}
        \text{\scriptsize Newton's Method}\\
        \boxed{ H_\mathcal{L} \hspace{1pt} \vec{h}_k = - \vec{\nabla} \mathcal{L} }
    \end{gathered}
    \ \ \vline \ \
    % KKT Matrix
    \begin{gathered}
        \underline{\text{\scriptsize KKT Matrix (Eq. Constr)}}\\
        \begin{aligned}
                \text{\scriptsize\(
                    \left( \begin{matrix}
                        \nabla_{xx} \mathcal{L} & J_g^T \\
                        J_g & 0
                    \end{matrix}\right)
                    \left( \begin{matrix}
                        \vec{s}_k\\
                        \ \vec{\delta}_k\ 
                    \end{matrix}\right)
                \)}
                &= - \text{\scriptsize\(
                        \left( \begin{matrix}
                            \nabla f \text{\scriptsize\((\bar{x})\)} 
                                + J_g^T \text{\scriptsize\((\bar{x})\)} \bar{\lambda}\\
                            \vec{g} \text{\scriptsize\((\bar{x})\)}
                        \end{matrix} \right)
                    \)}
                    \\[5pt]
                \Aboxed{
                    \text{\scriptsize\(
                        \left( \begin{matrix}
                            B & J^T \\
                            J & 0
                        \end{matrix}\right)
                        \left( \begin{matrix}
                            {s}\\
                            {\delta} 
                        \end{matrix}\right)
                    \)}
                    &= - \text{\scriptsize\(
                            \left( \begin{matrix}
                                {w}\\
                                {g}  
                            \end{matrix} \right)
                        \)}
                }
            \end{aligned}
    \end{gathered}
    \hspace{5pt} \Rightarrow \hspace{5pt}
    \begin{gathered}
        \text{\scriptsize \underline{[Sequential] Quadratic Programming (SQP) Problem}}\\[5pt]
        \min_s \Big( \vec{s}_k \cdot \vec{\nabla}_{x} \mathcal{L} 
            + \tfrac{1}{2} \langle \vec{s}_k | \vec{\nabla}_{xx} \mathcal{L} | \vec{s}_k \rangle \Big)
            \\[5pt]
        \text{s.t.} \ \ \ J_g{\scriptstyle(\vec{x}_k)} \hspace{1pt} \vec{s}_k + \vec{g}{\scriptstyle(\vec{x}_k)} = 0
    \end{gathered}
\)

%---------------------------------------------------------------------------------------------------------------------------------
%
%
%

% Direct Solution
% \vspace{10pt}
\underline{Direct Solution}: \ \parbox{.8\textwidth}{
    \scriptsize KKT Matrix is sym. and sparse 
    \(\rightarrow\) solve for \(\vec{h}_k\) using sym. indef. factorization w/ some pivoting
}

% Column/Range-Space Method
\vspace{10pt}
\underline{\(\begin{gathered}[b]
        \text{\scriptsize(Column-Space)}\\
        \text{Range-Space}
    \end{gathered}\) 
    Method}: \ 
\(
    \boxed{Bs = -w - J^T \delta}
    \hspace{15pt} , \hspace{15pt}
    \begin{aligned}
        Js = -g \ &\rightarrow \ JB^{-1}(- w - J^T \delta) = - g\\
        &\rightarrow\ \boxed{ (JB^{-1}J^T) \delta = g - JB^{-1} w }
    \end{aligned}
\)

% Notes
\vspace{5pt}
\hspace{10pt}\(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Solve for \(\delta\), then for \(s\).}\\
    &\bullet\ \text{\scriptsize \(B\) must be nonsingular and \(J\) full rank.}
\end{aligned}\)
\hspace{20pt}
\(\begin{aligned}[t]
    &\bullet\ \parbox[t]{.5\textwidth}{\scriptsize Forming \((JB^{-1}J^T)_{m \times m}\) leads to issues 
        similar to forming \(A^T A\) (loss of info. and degrades conditioning).}
        \\
    &\bullet\ \text{\scriptsize Useful if \(m\) is small.}
\end{aligned}\)

% Null-Space Method
\vspace{5pt}
\underline{Null-Space Method}: \ \(
    J^T = \text{\scriptsize\(
        \Big( Q_\parallel \ \ Q_\perp \Big) 
        \left(\begin{matrix}
            R\\ 
            0
        \end{matrix}\right)
    \)}
    \hspace{20pt}
    {\scriptstyle(Q_\parallel \hspace{1pt} \in\hspace{1pt} \mathbb{R}^{n\times m})}
    \hspace{10pt} \Rightarrow \hspace{10pt}
    \boxed{ \begin{aligned}
        J Q_\parallel &= R^T\\
        J Q_\perp &= 0
    \end{aligned} }
\)

% Solving for s and delta
\hspace{10pt}\(
    \arraycolsep=2pt \begin{array}{r c c c l}
        \text{Find } u_\parallel &:\ 
            &Js \ \equiv\ \Big( JQ_\parallel u_\parallel + \cancel{JQ_\perp} u_\perp \Big)
            &= \
            &\boxed{ R^T u_\parallel = -g }
            \\[10pt]
        \text{Find } u_\perp &:\
            &Q_\perp^T \big( Bs + J^T \delta= -w \big)
            &\rightarrow\ 
            &(Q_\perp^T B Q_\parallel) u_\parallel + (Q_\perp^T B Q_\perp) u_\perp = - Q_\perp^T w     
                - (\cancel{JQ_\perp})^T \delta
            \\[5pt]
        &
            &
            &
            &\boxed{
                    (Q_\perp^T B Q_\perp) u_\perp 
                    = - Q_\perp^T w - (Q_\perp^T B Q_\parallel) u_\parallel
                }
            \\[10pt]
        \text{Find } \delta &:\
            &Q_\parallel^T \big( J^T \delta = -w - Bs \big) 
            &\rightarrow\
            &\boxed{ R \delta
                = - Q_\parallel^T w - Q_\parallel^T B ( Q_\parallel u_\parallel - Q_\perp u_\perp ) }
    \end{array}
\)

% Notes
\vspace{5pt}
\hspace{10pt}\(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Near a min., \((Q_\perp^T B Q_\perp)\) can be Cholesky factored.}\\
    &\bullet\ \text{\scriptsize \(J\) must be full rank and \(R\) nonsingular.}
\end{aligned}\)
\hspace{20pt}
\(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Avoids issues with loss of info. and degraded conditioning.}\\
    &\bullet\ \text{\scriptsize Useful if \(m\) is large, so \(n-m\) is small.}
\end{aligned}\)

% Line Divider
\vspace{5pt}
\rule{1\textwidth}{.5pt}
\vspace{5pt}

% Decent Initial Guess for Lambda
\underline{Decent Initial \(\vec{\lambda}_0\) Guess Given an \(\vec{x}_0\)}: \ \(
    \boxed{    
        J_g^T{\scriptstyle(\vec{x}_0)} \hs \vec{\lambda}_0 + \vec{r} = - \vec{\nabla} f{\scriptstyle(\vec{x}_0)}
    }
    \hspace{15pt}
    \text{\scriptsize(Linear Least Sq.)}
\)

% Penalty Method
\vspace{5pt}
\( 
    \begin{gathered}
        \underline{\text{Penalty Func. Method}}\\[2pt]
        \boxed{ \lim_{\rho \rightarrow \infty} \vec{x}_\rho = \bar{x} } 
            \hspace{5pt} \text{\scriptsize (not explained)}\\
        (\text{\scriptsize ``Under approp. conds.''})
    \end{gathered}
    \ \vline \ \ \ 
    \boxed{
        \begin{array}{r l}
            % Simple Penalty Func.
            \begin{gathered}
                \text{\scriptsize One Simple Function}\\[-5pt]
                (\text{\scriptsize Ill-conditioned \(\rho \gg 1\)})
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \phi_\rho(\vec{x}) = f(\vec{x}) + \tfrac{1}{2} \rho \Vert g(\vec{x}) \Vert^2
                \\[15pt]
            % Augmented Lagrangian
            \begin{gathered}
                \text{\scriptsize Augmented Lagrangian}\\[-5pt]
                (\text{\scriptsize Less Ill-conditioned})
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \mathcal{L}_\rho(\vec{x}) = f(\vec{x}) + \vec{\lambda}_0 \cdot \vec{g}(\vec{x}) 
                + \tfrac{1}{2} \rho \Vert g(\vec{x}) \Vert^2
        \end{array}
    }
\)

% Barrier Method
\vspace{10pt}
\( 
    \begin{gathered}
        \underline{\text{Barrier Func. Method}}\\[2pt]
        \boxed{ \lim_{\rho \rightarrow 0} \vec{x}_\rho = \bar{x} }\\
        (\text{\scriptsize ``Under approp. conds.''})
    \end{gathered}
    \ \vline \ \ \ 
    \boxed{
        \begin{array}{r l}
            % Inverse
            \begin{gathered}
                \text{\scriptsize Inverse}
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \phi_\rho(\vec{x}) = f(\vec{x}) 
                - \rho \sum_i^p \frac{1}{ h_i{\scriptstyle(\vec{x})} }
                \\[15pt]
            % Logarithmic
            \begin{gathered}
                \text{\scriptsize Logarithmic}
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \phi_\rho(\vec{x}) = f(\vec{x}) 
                - \rho \sum_i^p \log{ ( -h_i{\scriptstyle(\vec{x})} ) }
        \end{array}
    }
    \hspace{10pt}
    \boxed{ \text{\scriptsize(For Ineq. Constr.)} }
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \parbox[t]{.95\textwidth}{\scriptsize Along with line search and trust region (not explained), 
        a merit func. - using perhaps a penalty func. - can be used to make an algorithm more robust.}
        \\
    &\bullet\ \text{\scriptsize An active set strategy (not explained) can be used with an SQP method 
        for ineq.-constr. problems.}
        \\
    &\bullet\ \text{\scriptsize A penalty method penalizes points that violates constraints, 
        but doesn't avoid them. Barrier methods do.}
\end{aligned}\)

%----------------------------------------------------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------------------------------------------------
% Polynomial Interpolation
\section{[Polynomial] Interpolation, \(f(t_i) = \hat{f}(t_i) = \sum_j x_j \hs \phi_j(t_i) \)}

\(
    \begin{aligned}
        \hat{f}(t_i) &= \sum_j x_j \hs \phi_j(t_i)\\
        &= \hsvec{\phi}\hs(t_i) \cdot \hsvec{x}
    \end{aligned}
    \hspace{10pt}
    \vline
    \hspace{10pt}
    \begin{gathered}
        \det(A) \neq 0\\[5pt]
        \text{Given } \hsvec{\phi},\\[-2pt]
        \text{solve for } \hsvec{x}
    \end{gathered}
    \hspace{10pt}
    \vline
    \hspace{10pt}
    \begin{aligned}
        &A\hsvec{x} 
            = \left(\begin{matrix}
                \vdots\\
                - \ \hsvec{\phi}{\scriptstyle(t_i)} \ -\\
                \vdots
            \end{matrix}\right)
            \left(\begin{matrix}
                |\\
                \hsvec{x}\\
                |
            \end{matrix}\right)
            = \hsvec{y} = \left(\begin{matrix}
                \vdots\\
                f{\scriptstyle(t_i)}\\
                \vdots
            \end{matrix}\right)
    \end{aligned}
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \parbox[t]{.95\textwidth}{\scriptsize Runge Phenom.: As \(n\) increases, evenly-spaced \(t_i\) could produce a 
        high-dimensional polynomial \(\hat{f}(t)\) that tends to be extremely wavey near the endpoints (like Gibbs phenom.).
        Choosing \(t_i\) to be Chebyshev nodes between the two endpoints mitigates this.}
        \\
    &\bullet\ \text{\scriptsize Interpolation w/ other func. like rationals are possible.}
        \\
    &\bullet\ \text{\scriptsize Error: } {\scriptstyle 
        \text{\scriptsize\(
            \begin{gathered}
                \max\\[-5pt]
                {t \in [t_1,t_n]}
            \end{gathered}\)} 
            \ \left| \hat{f} - f 
            \ =\ \frac{ f^{(n)}(\xi) }{n!} \prod_i (t-t_i) \right| 
            \ \leq\ \left| \max \frac{ f^{(n)}(t) }{n!} \right| 
            \left| \frac{(n-1)! h^n}{4} \right|
            \ =\ \boxed{ 
                \text{\scriptsize\(
                    \begin{gathered}
                        \max\\[-5pt]
                        {t \in [t_1,t_n]}
                    \end{gathered}
                    \ \left| f^{(n)}(t)\hs \frac{ h^n }{ 4n } \right|
                \)}
            } 
        } 
        \hspace{2pt} \rightarrow \hspace{2pt}
        \parbox{3cm}{\scriptsize error decreases if \(f^{(n)}\)\\ is well behaved}
    \end{aligned}\)

%------------------------------------------------------------------
%------------------------------------------------------------------
% Taylor Series Polynomial Interpolation
\subsection{Taylor Series Polynomial Interpolation}
\vspace{5pt}
\[
    \arraycolsep=3pt
    \begin{array}{l c c c l c l c l c l}
        \hat{f}_n(t) &=& f(t_0) 
            &+& f'(t_0) (t-t_0) 
            &+& \tfrac{ f''(t_0) }{2} (t-t_0)^2 
            &+& \dots &+& \tfrac{ f^{(n-1)}(t_0) }{(n-1)!} (t-t_0)^{n-1}\\[5pt]
        \hat{f}_n(t+h) &=& f(t) 
            &+& f'(t) h 
            &+& \tfrac{ f''(t) }{2}\hs h^2 
            &+& \dots 
            &+& \tfrac{ f^{(n-1)}(t) }{(n-1)!}\hs h^{n-1}
    \end{array}
\]

% Notes
\vspace{-5pt}
\(\begin{aligned}
    &\bullet\ \text{\scriptsize Can interpolate an \(n\)-polynomial from \(n+1\) points/derivatives/info.}
\end{aligned}\)

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Monomial Basis Functions/Vandermonde Matrix
\subsection{Monomial Basis Functions \(\rightarrow\) Vandermonde Matrix}

\vspace{5pt}
\(
    % Basis Functions
    \boxed{ 
        \begin{aligned}
            \hsvec{\phi}(t) &= \big( 1, t, t^2,\ \dots\ , t^{n-1} \big)^T\\[5pt]
            \hat{f}(t) &= x_1 + x_2 t + \dots + x_n t^{n-1}
        \end{aligned}
    }
    \hspace{10pt}
    \vline
    \hspace{10pt}
    % Matrix
    \begin{gathered}[b]
        \text{\scriptsize(Full, Dense}
            \\[-7pt]
        \text{\scriptsize Vandermonde Matrix) }
            \\
        \left(\begin{matrix}
            1 & t_1 & \dots & t_1^{n-1}\\
            \vdots & \vdots & &\vdots \\
            1 & t_n & \dots & t_n^{n-1}\\
        \end{matrix}\right)
    \end{gathered}
    \left(\begin{matrix}
        \vdots\\
        x_i\\
        \vdots
    \end{matrix}\right)
    = \vec{y}
    % Notes
    \hfill
    \begin{aligned}
        &\bullet\ \parbox[t]{4cm}{\scriptsize Solved with \(\mathcal{O}(n^3)\) work using 
            Gauss. Elim. (\(\mathcal{O}(n^2)\) is possible with other tech.).}
            \\
        &\bullet\ \parbox[t]{4cm}{\scriptsize Ill-conditioned since sucessive \\ 
            \(t^j\) look the same at higher \(j\).}
            \\[20pt]
    \end{aligned}
\)

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Lagrange Basis Functions (Fundamental Polynomials)/Identity Matrix
\subsection{Lagrange Basis Functions (Fund. Polynomials) \(\rightarrow\) Identity Matrix}

\vspace{5pt}
% Basis Functions
\(
    \begin{aligned}
        &\boxed{ 
            \begin{aligned}
                l(t) &= (t-t_1)(t-t_2)\dots(t-t_n)\\
                w_j &= (t_j - t_j) / l(t_j) \hspace{10pt} \text{\scriptsize(barycentric weights)}
            \end{aligned} 
        }
            \\[10pt]
        &\begin{aligned}
            \Aboxed{ \phi_j(t) &= \tfrac{ l(t) / (t-t_j) }{ l(t_j) / (t_j-t_j) } 
                = l(t) \frac{w_j}{t-t_j} }
                \\
            \phi_j(t_i) &= \delta_{ij} \ \Rightarrow \ \boxed{ \hsvec{\phi}(t_i) = \vec{e}_i }
        \end{aligned}
            \\[10pt]
        &\begin{aligned}
            \Aboxed{ \hat{f}(t) &= \hsvec{x} \cdot \hsvec{\phi}(t)
                = l(t) \Big[ x_1 \tfrac{w_1}{t-t_1} + \dots + x_n \tfrac{w_n}{t-t_n} \Big] }
                \\
            \hat{f}(t_j) &= x_j = y_i
        \end{aligned}
    \end{aligned}
\)
\hfill
\vline
\hfill
\begin{minipage}{.45\textwidth}
    % Matrix
    \[
        \begin{gathered}[b]
            \text{\scriptsize(Diag. Iden. Matrix)}\\
            \text{\scriptsize\(
                \left(\begin{matrix}
                    1 & 0 & \dots\\
                    0 & 1 & \ddots\\
                    \vdots & \ddots & \ddots
                \end{matrix}\right) 
                \)}
        \end{gathered}
        \hs \vec{x} = \vec{y}
    \]

    % Notes
    \vspace{5pt}
    \(\begin{aligned}
        &\bullet\ \text{\scriptsize Finding \(w_j\) is \(\mathcal{O}(n^2)\) work.}\\
        &\bullet\ \text{\scriptsize Finding \(\hat{f}(t)\) from \(w_j\)'s is \(\mathcal{O}(n)\) work.}\\
        &\bullet\ \boxed{ \parbox[t]{.9\textwidth}{\scriptsize Updating with an extra point \((t_{n+1}, y_{n+1})\) 
            is \(\mathcal{O}(n)\) work \\by changing \(w_j = w_j/(t_j-t_{n+1})\) and finding \(w_{n+1}\).} }\\
        &\bullet\ \text{\scriptsize Basis func. are more varied \(\rightarrow\) better-conditioned.}\\
        &\bullet\ \boxed{ \int_{t_1}^{t_n} \hat{f}(t) dt = \sum_{i=1}^n y_i \int_{t_1}^{t_n} \phi_i(t) dt }
    \end{aligned}\)
\end{minipage}  

%---------------------------------------------------------------------------------------------------------------------------------
%
%
%---------------------------------------------------------------------------------------------------------------------------------
% Newton Basis Functions/Lower Triangular Matrix
\newpage
\subsection{Newton Basis Functions \(\rightarrow\) Low. Triang. Matrix}

\vspace{5pt}
\(
    % Basis Functions
    \begin{aligned}
        &\boxed{ 
            \begin{aligned}
                \phi_j(t) &= (t-t_1)(t-t_2)\dots(t-t_{j-1})\\
                \hsvec{\phi}(t) &= \big[ 1, (t-t_1), (t-t_1)(t-t_2),\ \dots \big]^T
            \end{aligned}
            }
            \\[5pt]
        &\boxed{ \hat{f}(t) = x_1 + x_2(t-t_1) + \dots + x_n \phi_n(t) }
    \end{aligned}
    \hspace{10pt}
    \vline
    \hspace{10pt}
    % Matrix
    \begin{aligned}
        &\begin{gathered}[b]
            \text{\scriptsize(Low. Triang. Matrix)}\\
            \text{\scriptsize\(
                \left(\begin{matrix}
                    1 & 0 & 0 & \dots \\
                    1 & t_1-t_2 & 0 & \dots\\
                    1 & t_3-t_2 & (t_3-t_1)(t_3-t_2) & \ddots\\
                    \vdots & \vdots & \vdots & \ddots
                \end{matrix}\right) 
                \)}
        \end{gathered}
            \text{\scriptsize\( 
                \left(\begin{matrix}
                    \vdots\\
                    x_i\\
                    \vdots
                \end{matrix}\right)
            \)} 
            = \vec{y}
    \end{aligned}
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \text{\scriptsize For. sub. is \(\mathcal{O}(n^2)\).}\\
    &\bullet\ \text{\scriptsize Cond. of \(A\) depends on ordering of points \(\rightarrow\) best to order points from
        their dist. to their mean/other num.}\\
    &\bullet\ \text{\scriptsize Basis func. are more varied \(\rightarrow\) better-conditioned.}
\end{aligned}\)

% Updating
\vspace{15pt}
\begin{minipage}[t]{.5\textwidth}
    \underline{Incremental Updating Newton Interpolation}:
    
    \vspace{5pt}
    \(\hat{f}_{n+1}(t) = \hat{f}_n(t) + x_{n+1}\hs \phi_{n+1}(t)\)

    \vspace{10pt}
    \(
        \begin{aligned}
            &\begin{aligned}
                y_{n+1} &= \hat{f}_{n+1}(t_{n+1})\\
                &= \hat{f}_n(t_{n+1}) + x_{n+1}\hs \phi_{n+1}(t_{n+1})
            \end{aligned}
                \\[5pt]
            &\Rightarrow\ \boxed{ 
                \hat{f}_{j+1}(t) = \hat{f}_j(t) + \tfrac{ y_{j+1} - \hat{f}_j(t_{j+1}) }{ \phi_{j+1}(t_{j+1}) } \phi_{j+1}(t) 
                }
        \end{aligned}
    \)
\end{minipage}
% Divided Differences
\begin{minipage}[t]{.49\textwidth}
    \underline{Divided Differences Newton Interpolation}:
    
    \vspace{5pt}
    \(\displaystyle
        g[t_1, \dots , t_k] \ \equiv\ \frac{ g[t_2, \dots , t_k] - g[t_1, \dots , t_{k-1}] }{t_k - t_1}
    \)

    \vspace{5pt}
    \(
        \boxed{
            \vec{x} 
            = \text{\scriptsize\(
                \left(\begin{matrix}
                    x_1\\
                    x_2\\
                    x_3\\
                    \vdots
                \end{matrix}\right)
            \)}
            = \text{\scriptsize\(
                \left(\begin{matrix}
                    g[t_1]\\
                    g[t_1, t_2]\\
                    g[t_1, t_2, t_3]\\
                    \vdots        
                \end{matrix}\right)
            \)}
        }
        \hspace{10pt}
        % Notes
        \begin{aligned}
            &\bullet\ \text{\scriptsize Also costs \(\mathcal{O}(n^2)\).}\\
            &\bullet\ \parbox[t]{3cm}{\scriptsize Less prone to \\ over/underflow.}
        \end{aligned}
    \)
\end{minipage}

%------------------------------------------------------------------
%------------------------------------------------------------------
% Orthogonal Polynomial Basis
\vspace{5pt}
\subsection{Orthogonal Polynomial Basis (no method given)}

% Inner Product
\vspace{10pt}
\parbox[t]{.55\textwidth}{%
    \underline{Inner Product}:\ \ \(
        \boxed{
            \langle \hsvec{u} | \hsvec{v} \hs \rangle_{ab}^w 
            = \int_a^b \left[ u(t) v(t) \right] w(t) \ dt 
        }
    \)
}
% Orthogonal Polynomials
\parbox[t]{.44\textwidth}{%
    \underline{Orthogonal Polynomials}:\ \ \(
        \boxed{
            \langle u_i | u_j \rangle
            = \delta_{ij}
        }
    \)
}

% Three-Term Recurrence
\vspace{5pt}
\underline{Three-Term Recurrence}: \ \ \(
    \boxed{ \hat{f}_{k+1}(t) = \big[ A(k)t + B(k) \big] \hat{f}_k(t) - C(k) \hat{f}_{k-1}(t) } 
    \hspace{20pt} {\scriptstyle(A(k) \hs \neq\hs 0)}
\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Piecewise Hermite Cubic Interpolation
\subsection{Piecewise [Hermite] Cubic Interpolation}

% Picewise Cubic
\parbox[t]{5cm}{
    \underline{Piecewise Cubic}: \\[5pt]
    {\scriptsize 
        \(n\) knots/pts. \(\Rightarrow\ n-1\) cubics \\
        \(\Rightarrow\) \fbox{ \(4(n-1)\) param./eq.} 
    }
}
% Hermite Interpolation Def
\parbox[t]{5cm}{
    \underline{Hermite Interpolation}: \\[5pt]
    {\scriptsize 
        Using \(k\)-th derivatives as info.\\
        Extra equations can be used \\
        for monotonicity/convexity.
    }
}
% Hermite Cubic Interp.
\parbox[t]{7cm}{
    \underline{Hermite Cubic Interpolation}: \\[5pt] 
    {\scriptsize 
        Continuous 0th and 1st derivatives; \ \(n-1\) cubics \\
        \(\Rightarrow [2(n-1)]_\text{1st deriv. eq} + [n-2]_\text{2nd deriv. eq.}\) \\
        \(=\) \fbox{ \(3n-4\) eq. \(\Rightarrow\ n\) free/extra param./eq}
    }
}

%----------------------------------------------------------------
%----------------------------------------------------------------
% Piecewise Cubic [Spline] Interpolation
\subsection{Piecewise Cubic [Spline] Interpolation}

% Spline
\parbox[t]{4.5cm}{
    \underline{Spline}: \\[4pt] 
    {\scriptsize 
        A piecewise func. of \(n\)-polynomials that is \(n\)-differentiable (of differentiability class \(C^{n-1}\), 
        or \(n-1\) cont. differentiable).
    }
}
\hspace{.75cm}
% Cubic Spline Interp.
\parbox[t]{6.5cm}{
    \underline{Cubic Spline Interpolation}: \\[5pt] 
    {\scriptsize 
        Cont. 0th, 1st, and 2nd derivatives; \ \(n-1\) cubics \\
        \(\Rightarrow [2(n-1)]_\text{1st} + [n-2]_\text{2nd} + [n-2]_\text{3rd}\)\\
        \(=\) \fbox{ \(4n-6\) eq. \(\Rightarrow\ 2\) free/extra param./eq}
    }
}
\hspace{.75cm}
% B-Spline Basis Func.
\parbox[t]{4.5cm}{
    \underline{\(B\)-splines (basis func.)}: \\[4pt] 
    {\scriptsize 
        Orthog. \(\{\phi_j(t)\}\) are \(j\)-poly. splines w/ local compact support and look like bells.
        (not much detail here).
    }
}

%------------------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------------------
% Numerical Integration
\newpage
\section{Numerical Integration/Quadrature, \(I(f) \equiv \int_a^b f(x)\hs dx\)}

% Infinity Norm and Ccondition Number
\subsection{\(\infty\)-Norm and Condition Number}

\vspace{5pt}
% Infinity Norm
\begin{minipage}[t]{.36\textwidth}
    \underline{Function \(\infty\)-Norm}: \\[10pt] 
    \fbox{ \(\Vert f(x) \Vert_\infty = \max_{x \in [a,b]} f(x)\) }    
\end{minipage}
% Abs. Cond Number for Integration if \hat{b}
\begin{minipage}[t]{.63\textwidth}
    \underline{[Abs.] Integration Condition Number if \(\hat{b}\)}: \\[10pt]
    \(
        \left\vert \int_a^{\hat{b}} f(x)\hs dx - \int_a^b f(x)\hs dx \right\vert
        = \left\vert \int_b^{\hat{b}} f(x)\ dx \right\vert
        \ \leq\ \boxed{ (\hat{b}-b) \Vert f(x) \Vert_\infty }
    \)    
\end{minipage}

\vspace{5pt}
% Abs. Cond Number for Integration if \hat{f}
\begin{minipage}[t]{.56\textwidth}
    \underline{[Abs.] Integration Condition Number if \(\hat{f}\)}: \\[10pt]
    \(
        \begin{aligned}
            \left\vert \int_a^b \hat{f}(x) - f(x)\ dx \right\vert
                &\ \leq\ \int_a^b \left\vert \hat{f}(x) - f(x) \right\vert dx
                \\
            &\ \leq\ (b-a) \Vert \hat{f}(x) - f(x) \Vert_\infty
                \\
            \left\vert \frac{\Delta I}{\Delta f} \right\vert 
                &\ \leq\ \boxed{ b-a }
        \end{aligned}
    \)    
\end{minipage}
% Rel. Cond Number for Integration if \hat{f}
\begin{minipage}[t]{.43\textwidth}
    \underline{[Rel.] Integration Condition Number if \(\hat{f}\)}: \\[10pt]
    \(
        \begin{aligned}
            \left\vert \frac{\Delta I / I}{\Delta f / f} \right\vert 
                &\ \leq\ \frac{(b-a) / \left\vert \int_a^b f(x) dx \right\vert}{1 / \Vert f(x) \Vert_\infty}
                \\[5pt]
            &\ =\ \boxed{ \frac{(b-a) \Vert f(x) \Vert_\infty }{\left\vert \int_a^b f(x) dx \right\vert} }
        \end{aligned}
    \)
\end{minipage}

%---------------------------------------------------------------
%---------------------------------------------------------------
% Interpolary Quadrature Rule
\vspace{-5pt}
\subsection{1-D [Interpolary] Quadrature Rule for \(f \approx \hat{f}\)}

% Basis for \hat{f}
\(
    \underline{ \hat{f} \in P_{n-1} } :
    \ \ \hat{f}(x) 
    = \left(\begin{gathered}
        \hsvec{y} \cdot \hsvec{\phi}(x) = \sum_{i=1}^n f(x_i)\hs \phi_i(x) \\
        \text{\scriptsize(Lagrange Basis Vectors)}
    \end{gathered}\right)
    = \left(\begin{gathered}
        \sum_{j=1}^{n} c_j\hs x^{j-1} \\
        \text{\scriptsize(Monomial Basis Vetors)}
    \end{gathered}\right)
    \hspace{20pt} \begin{aligned}
        &{\scriptstyle\bullet\ x_1\ <\ \dots\ <\ x_n}\\ 
        &{\scriptstyle\bullet\ f(x_i) \ =\ \hat{f}(x_i)}
    \end{aligned}
\)

% Quadrature Rule
\(\displaystyle
    \Rightarrow\ \boxed{ Q_n(f) 
        \ \equiv\ I(\hat{f}) 
        =  \int_a^b \hat{f}(x)\hs dx 
        = \sum_{i=1}^n f(x_i) \int_a^b \phi_i(x)\hs dx 
        = \sum_{i=1}^n f(x_i)\hs w_i
        }
        \hspace{20pt} \begin{aligned}
            &{\scriptstyle \bullet\ \boxed{\scriptstyle x_i,\ w_i \ \rightarrow\ 2n \text{\scriptsize\ max param.} } }\\[-4pt]
            &{\scriptstyle \bullet\ a\ \leq\ x_1\ <\ \dots\ <\ x_n\ \leq\ b} \\[-7pt]
            &\text{\scriptsize \(\bullet\ \) closed if equality, open if not}
        \end{aligned}
\)

% Method of Undetermined Coefficients / System of Moment Equations
\vspace{5pt}
\underline{Method of Undetermined Coefficients (MUC) / System of Moment Equations}  

\vspace{5pt}
\(
    % Math to MUC
    \mathscriptsize{ \arraycolsep=2pt \begin{array}{c c c}
        \displaystyle \int_a^b \left( \sum_{j=1}^{n} c_j\hs x^{j-1} \right) dx 
            &=
            &\displaystyle \sum_{i=1}^n \left( \sum_{j=1}^{n} c_j\hs x_i^{j-1} \right) w_i
            \\[20pt]
        \displaystyle \sum_{j=1}^{n} c_j \left( \int_a^b x^{j-1} dx \right) 
            &=
            &\displaystyle \sum_{j=1}^{n} c_j \left( \sum_{i=1}^n x_i^{j-1} \hs w_i \right)
            \\[20pt]
        \multicolumn{3}{c}{ \text{\scriptsize(maybe some dot product to isolate terms)} }
    \end{array} }
    \hfill \Rightarrow \hfill 
    % MUC Conclusion
    \begin{gathered}
        \begin{aligned}
                \Aboxed{ \mathscriptsize{\sum_{i=1}^n} \hs f{\scriptstyle(x_i)} \hs w_i 
                    &= \hat{F}{\scriptstyle(b)} - \hat{F}{\scriptstyle(a)} }
                    \\
                %
                    &\hs \hs \downarrow
                    \\
                \Aboxed{ \mathscriptsize{\sum_{i=1}^n} \hs x_i^{j-1} \hs w_i 
                    &= \tfrac{b^{j}}{j} - \tfrac{a^{j}}{j} }
                    \\
                %
                    &\equiv z_j
            \end{aligned}
            \\[5pt]
        \bullet\ \boxed{ \scriptstyle z_1 \ =\ \sum w_i \ =\ b-a }
    \end{gathered}
    \hfill
    % MUC Vandermonde Matrices
    \boxed{ \begin{aligned}
        \begin{gathered}[b]
                \text{\scriptsize(Vandermode Matrix)}
                    \\
                \mathscriptsize{\left[\begin{matrix}
                        1 & 1 & 1 & \dots\\
                        x_1 & x_2 & x_3 & \dots\\[3pt]
                        x_1^2 & x_2^2 & x_3^2 & \dots\\
                        \vdots & \vdots & \vdots
                    \end{matrix}\right]}
            \end{gathered}
            \hsvec{w}
            &= \hsvec{z}
            \\
        \mathscriptsize{\left[\begin{matrix}
                1 & 0 & 0 & \dots\\
                a & 1 & 1 & \dots\\
                \tfrac{a^2}{2} & x_1 & x_2 & \dots\\
                \vdots & \vdots & \vdots 
            \end{matrix}\right]}
            \mathscriptsize{\left[\begin{matrix}
                1\\
                |\\
                \hsvec{w} \hs\\
                |
            \end{matrix}\right]}
            &=
            \mathscriptsize{\left[\begin{matrix}
                1\\
                b\\
                \tfrac{b^2}{2} \\
                \vdots
            \end{matrix}\right]}
    \end{aligned} }
\)

% Notes
\vspace{10pt}
\(\begin{aligned}
    &\underline{ \text{Error } I}:
        \ \ | \Delta I | 
        \ \leq\ (b-a) \Vert f-\hat{f} \Vert_\infty 
        \ \leq\ \tfrac{b-a}{4n}\hs h^n \Vert f^{(n)} \Vert_\infty 
        \ \leq\ \boxed{ \tfrac{h^{n+1}}{4} \Vert f^{(n)} \Vert_\infty }
        \hspace{5pt} \rightarrow \hspace{5pt}
        \parbox{3cm}{\scriptsize error decreases if \(f^{(n)}\)\\ is well behaved}
        \\[10pt]
    &\underline{ \text{Error } Q_n}:
        \ \begin{aligned}[t]
            &g \approx f 
                \ \rightarrow \ \begin{aligned}[t]
                    &| Q_n(f) - Q_n(g) | \\
                    &= {\scriptstyle \left| \sum w_i \big[ f{\scriptstyle(x_i)} - g{\scriptstyle(x_i)} \big] \right| }
                \end{aligned}
                \ \leq\ \boxed{ \scriptstyle \sum | w_i | \cdot \Vert f - g \Vert_\infty }
                \ \Rightarrow\ \begin{aligned}[t]
                    &\boxed{\forall w_i \geq 0 \ \rightarrow\ \text{\scriptsize cond}(Q_n) = b-a }\\
                    &\text{\scriptsize(otherwise using \(Q_n\) might be unstable.)}
                \end{aligned}
        \end{aligned}
\end{aligned}\)

%
%
%
%------------------------------------------------------------------------------------------------------------------------------
\newpage

% Rule Degree
\underline{[Rule] Degree, \(d\)}: \ \fbox{ 
    \(\forall p(x) \in P_d\), rule \(Q(p) = I(p)\), but not \(\forall p \in P_{d+1}\) 
}

% Newton-Cotes Quadrature
\vspace{10pt}
\underline{Newton-Cotes Quadrature [Rule]}: \ \fbox{\(n\) evenly-spaced \(x_i \ \rightarrow\ n\) param. for \(w_i\)}

% Midpoint, Trap, Simpson Rules
\(\hspace{10pt} \arraycolsep=2pt \begin{array}{r c c c l}
    \text{Midpoint Rule}\ {\scriptstyle(Q_1)}
        &:\ \
        &M(f) = \tfrac{b-a}{1}\hs f(\tfrac{a+b}{2})
        &\hspace{30pt} 
        &\hsvec{w} = (b-a)\hs [1]^T
        \\[5pt]
    \text{Trapezoidal Rule}\ {\scriptstyle(Q_2)}
        &:\ \
        &T(f) = \frac{b-a}{2}\ [f(a) + f(b)]
        &
        &\hsvec{w} = (b-a)\hs [\tfrac{1}{2}, \tfrac{1}{2}]^T
        \\[5pt]
    \text{Simpsons's Rule}\ {\scriptstyle(Q_3)}
        &:\ \
        &S(f) = \frac{b-a}{6}\ [f(a) + 4f(\tfrac{a+b}{2}) + f(b)]
        & 
        &\hsvec{w} = (b-a)\hs [\tfrac{1}{6}, \tfrac{4}{6}, \tfrac{1}{6}]^T
\end{array}\)

% Notes
\vspace{10pt}
\(\hspace{10pt} \begin{aligned}
    % Taylor Expansion and Error
    &\bullet\ \text{Taylor Expansion and Error}\\
    &\begin{aligned}[t]
            % Taylor Expansion
            f(x) &= \boxed{ \sum_{m=0} \tfrac{f^{(m)}(\tfrac{a+b}{2})}{m!} (x-\tfrac{a+b}{2})^m }
                \\[10pt]
            % Trapezoid Expansion
            T(f) &= \tfrac{b-a}{2}
                \sum_{m=0} \tfrac{f^{(m)}(\tfrac{a+b}{2})}{m!} \tfrac{ (b-a)^m }{ 2^m } \left[(-1)^m + 1\right] 
                \\[5pt]
            &= \sum_{m=0}^\text{even} \dboxed{ \tfrac{f^{(m)}(\tfrac{a+b}{2})}{2^m m!} }\hs (b-a)^{m+1} \\
            &= \boxed{ M(f) + \sum_{m=2}^\text{even} \dboxed{ E_m(f) }\hs h^{m+1} }
                \\[10pt]
            % Simpson Expansion
            S(f) &= \boxed{ \tfrac{2}{3} M(f) + \tfrac{1}{3} T(f) }
        \end{aligned}
        \hspace{10pt}
        \begin{aligned}[t]
            % Integral Expansion
            I(f) &= \sum_{m=0} \tfrac{f^{(m)}(\tfrac{a+b}{2})}{(m+1)!} \tfrac{ (2x-a-b)^{m+1} }{ 2^{m+1} } \big|_a^b \\[5pt]
            &= \sum_{m=0}^\text{even} \tfrac{ f^{(m)}(\tfrac{a+b}{2}) }{ 2^{m} (m+1)! }\hs (b-a)^{m+1}\\
            % Midpoint Error
            &= \boxed{ M(f) + \sum_{m=2}^\text{even} \tfrac{ E_m(f) }{ m+1 }\hs h^{m+1} }
                \hspace{10pt} \begin{gathered}
                    \text{\scriptsize \(Q_1\) error is \(f^{(2)}\)}\\[-5pt]
                    \text{\scriptsize derivative, not \(f^{(1)}\)!}
                \end{gathered}
                \\
            % Trapezoid Error
            &= \boxed{ T(f) - \sum_{m=2}^\text{even} m\hs \tfrac{E_m(f)}{m+1}\hs h^{m+1} }
                \hspace{10pt} \begin{gathered}
                    \text{\scriptsize \(Q_2\) error is \(f^{(2)}\) \&}\\[-5pt]
                    \text{\scriptsize twice as large as \(Q_1\)}
                \end{gathered}
                \\
            % Simpson Error
            &= \boxed{ S(f) - \sum_{m=4}^\text{even} \tfrac{m-2}{3} \hs \tfrac{ E_m(f) }{ m+1 } \hs h^{m+1} }
                \hspace{10pt} \begin{gathered}
                    \text{\scriptsize \(Q_3\) error is \(f^{(4)}\)}\\[-5pt]
                    \text{\scriptsize derivative, not \(f^{(3)}\)!}
                \end{gathered}
        \end{aligned}
        \\[10pt]    
    % Degree/Error
    &\bullet\ \arraycolsep=2pt 
        \begin{array}[t]{r c c c c l}
            n \text{ is even} &: \ \
                &\text{\scriptsize \(Q_n\) error is expected \(f^{(n)}\) derivative}
                &\hspace{10pt}
                &Q(p_{n-1}) = I(p_{n-1})
                &\rightarrow\ \boxed{d = n-1}
                \\[5pt]
            n \text{ is odd} &: \ \
                &\text{\scriptsize \(Q_n\) error is \(f^{(n+1)}\) derivative}
                &
                &Q(p_{n}) = I(p_{n})
                &\rightarrow\ \boxed{d = n}
        \end{array}
        \\[5pt]
    % Error Difference
    &\bullet\ \text{\scriptsize \underline{2 Rule Error}: \ Est. diff. between \(T(f)\) and \(M(f)\) can be used 
        to est. \(I(f)\) error in using either.}
        \\[5pt]
    % Progressive
    &\bullet\ \text{\scriptsize Can use subinterval, so can be \underline{progressive}.}\\
    % Runge Phenom
    &\bullet\ \text{\scriptsize Evenly-spaced \(x_i\) exibit the Runge Phenom.} 
        \rightarrow\ \boxed{ Q_\infty(f) \text{ isn't always } I(f) }
        \\[5pt]
    % Ill Conditioned
    &\bullet\ \boxed{ \text{\scriptsize Ill-conditioned and unstable} }:\ 
        (n \geq 11 \Rightarrow \exists w_i < 0) ,\ \left( {\scriptstyle \sum_i^\infty}\hs |w_i| \rightarrow \infty \right) 
\end{aligned}\)

% Clenshaw-Curtis Quadrature
\vspace{15pt}
\underline{Curtis-Clenshaw Quadrature [Rule]}:\ \ \fbox{\(n\) Chebyshev Nodes, \(x_i \ \rightarrow\ n\) param. for \(w_i\)}

% Notes
\vspace{10pt}
\(\hspace{10pt} \begin{aligned}[t]
    &\bullet\ \forall n: \forall w_i > 0 \ \Rightarrow\ \text{\scriptsize cond}(Q) = b-a\\
    &\bullet\ \lim_{n\rightarrow\infty} C_n(f) = I(f)\\
    &\bullet\ \boxed{ d_n = n-1 }
\end{aligned}
\hfill
\begin{aligned}[t]
    &\bullet\ \parbox[t]{6cm}{\scriptsize \(\exists\)\ an algorithm w/ Chebyshev polynomials 
        to find integrand w/o solving for \(w_i\).}\\
    &\bullet\ \text{\scriptsize Using Chebyshev polynomial zeroes is the classical CCQ.}\\
    &\bullet\ \text{\scriptsize Using Chebyshev extrema leads to a progressive rule [practical CCQ].} 
\end{aligned}\)

%
%
%
%------------------------------------------------------------------------------------------------------------------------------
\newpage

% Guassian Quadrature
\underline{Guassian Quadrature [Rule]}:\ \ \fbox{\(2n\) free param. for \(x_i,\ w_i\)} \(\Rightarrow\ \boxed{ d_n = 2n-1 }\) 

% Notes
\vspace{10pt}
\(\hspace{10pt} \begin{aligned}[t]
    % System of Moment Equations
    &\bullet\ x_i, w_i : \ \
        x_{n < i \leq 2n} = w_{n < i \leq 2n} = 0
        \ \rightarrow \
        \boxed{
            \text{\scriptsize\(
                \left(\ \begin{matrix}
                    1 & \dots & 1 & 0 & \dots\\
                    x_1 & \dots & x_n & 0 & \dots\\[3pt]
                    x_1^2 & \dots & x_n^2 & 0 & \dots\\
                    \vdots&  & \vdots & \vdots
                \end{matrix}\ \right)
                \left(\begin{matrix}
                    \vdots\\
                    w_n\\
                    0\\[-4pt]
                    \vdots\\
                \end{matrix}\right) 
            \)}
            \ =\ \hsvec{z}(a,b)
        }
        \hspace{20pt}
        \boxed{ \text{\scriptsize usually \(x_i \notin \mathbb{Q}\)} }
        \\
    % Orthogonal Polynomials
    &\bullet\ \text{Ortho. Poly.}:\ \ 
        \begin{gathered}[t]
            \langle p_n{\scriptstyle(x)} | x^k \rangle_{ab} = 0 \\
            {\scriptstyle(k = 0,\ \dots,\ n-1)}
        \end{gathered}
        \ \Rightarrow\ \boxed{ 
            x_i:\ \begin{array}[t]{c c}
                p_n(x_i) = 0, & x_i \in \mathbb{R}, \\
                x_i \neq x_{j \neq i}, & x_i \in (a,b)
            \end{array}
        } 
        \hspace{20pt}
        \begin{aligned}
            \\[-9pt]
            {\scriptstyle[-1,1]} 
                &- \text{\scriptsize Legendre}
                \\[-7pt]
            {\scriptstyle(-\infty,\infty)} 
                &- \text{\scriptsize Hermite}
                \\[-7pt]
            {\scriptstyle[0,\infty)} 
                &- \text{\scriptsize Laguerre}
                \\[-7pt]
        \end{aligned}
        \\[5pt]
    % Interval Transform
    &\bullet\ \text{\scriptsize Interval Transform}:\ \boxed{ 
        \int_a^b f(t)\hs dt = \tfrac{b-a}{\beta-\alpha} \int_\alpha^\beta f(t)\hs dx
        \hspace{20pt} t = \tfrac{ (b-a)x + a\beta - b\alpha }{ \beta - \alpha } 
    }
        \\[5pt]
    % Convergence
    &\bullet\ \forall n: \forall w_i > 0 \ \Rightarrow\ \text{\scriptsize cond}(Q) = b-a
        \hspace{25pt} \bullet\ \lim_{n\rightarrow\infty} G_n(f) = I(f)
        \\
    % Not progressive
    &\bullet\ n = 2m+1\ \rightarrow\ \tfrac{a+b}{2} \in \{x_i\}_n;\ \ 
        \text{\scriptsize otherwise usually } \{x_i\}_n \cup \{x_i\}_{\neq n} = 0
        \ \rightarrow\ \boxed{ \text{\scriptsize Not progressive} }
        \\[5pt]
    % Progressive Rules
    &\bullet\ \text{\scriptsize\(
        \arraycolsep=2pt \begin{array}[t]{r c c l}
            % Gauss-Kronrod
            \underline{ \text{Progressive Gauss-Kronrod} , K_{2n+1} }: 
                &n \text{ from } G_n
                &\rightarrow\ \begin{aligned}
                        n+1 &\text{ param for } x_{i>n}\\[-3pt]
                        2n+1 &\text{ param for } w_i
                    \end{aligned}
                &\Rightarrow \ \boxed{d_{2n+1} = 3n+1 < 4n+1}
                \\[5pt]
            % GK 2 Rule Error
            \text{GK 2-Rule Error}:
                &\multicolumn{3}{l}{ 
                    \boxed{ \Delta I(f) \approx (200|G_n - K_{2n+1}|)^{1.5} } 
                    }
                \\[10pt]
            % Gauss Patterson
            \text{Progressive Gauss-Patterson}, P_{4n+3}: 
                &2n+1 \text{ from } K_{2n+1}
                &\rightarrow\ \begin{aligned}
                        2n+2 &\text{ param for } x_{i>n}\\[-3pt]
                        4n+3 &\text{ param for } w_i
                    \end{aligned}
                &\Rightarrow \ \boxed{d_{4n+3} = 6n+4 < 8n+5}
        \end{array}
        \)}
        \\
    % Closed Intervals
    &\bullet\ \text{\scriptsize\(
        \arraycolsep=2pt \begin{array}[t]{r r l}
            \text{Closed Gauus-Randau}: 
                &x_i \in [a,b) \text{ or } (a,b] 
                &\rightarrow \ \boxed{d = 2n-2}
                \\[3pt]
            \text{Closed Gauus-Lobatto}: 
                &x_i \in [a,b] 
                &\rightarrow \ \boxed{d = 2n-3}
        \end{array}
        \)}
\end{aligned}\)

% Composite [Subinterval] Quadrature
\vspace{10pt}
\underline{Composite [\(k\)-Subintervals] Quadrature for Rule \(Q_n\)}: \ \(
    Q_n \ \rightarrow \ Q_{kn} \text{ \ or \ } Q_{kn-(k-1)} , 
\)

\(\hspace{10pt}\begin{aligned}
    &\bullet\ \lim_{k\rightarrow\infty} C_{k,n} 
        = \mathscriptsize{ \sum_{j=1}^{k\rightarrow\infty} } \left[ 
            \mathscriptsize{ \sum_{i=1}^n }\hs 
            w_i f{\scriptstyle(x_{ji})} 
        \right]
        = \mathscriptsize{ \sum_{i=1}^n }\hs 
            \tfrac{w_i}{h_k} \left[ 
            \mathscriptsize{ \sum_{j=1}^{k\rightarrow\infty} } 
            h_k f{\scriptstyle(x_{ji})} 
        \right]
        = I(f) \mathscriptsize{ \sum_{i=1}^n }\hs \tfrac{w_i}{h_k} 
        = I(f)     
        \hspace{25pt} \mathscriptsize{ 
            \begin{aligned}
                &\begin{aligned}
                    h_k &= (b-a)/k \\[-2pt]
                    &\geq (x_{jn}- x_{j1})
                \end{aligned}
                    \\[2pt]
                &\boxed{d \geq 0} \ \Rightarrow\ \sum w_i = h_k 
            \end{aligned}
        }
        \\
    &\bullet\ \text{Error}:\ \mathcal{O}(h^{m+1}) 
        \ \rightarrow\ \mathcal{O}(kh_k^{m+1}) 
        = \boxed{ \mathcal{O}(h_k^{m}) }
        \hspace{20pt} {\scriptstyle(k>1)}
\end{aligned}\)

% Adaptive Quadrature
\vspace{10pt}
\underline{Adaptive Quadrature for Rule \(Q_n\)}:\ \ Divide subinterval until a tolerance is met.

%------------------------------------------------------------------
%------------------------------------------------------------------
% n-Dim Integration
\subsection{\(n\)-D Integration}

{\tabcolsep=5pt
    \begin{tabular}{r l}
        {Double Integral}:
            &Use a pair of 1-D routines for the inner/outer integral.
            \\[5pt]
        {\(\scriptstyle (n>2)\)-Dimension Integral}:
            &Monte Carlo is best (error \(\scriptstyle 1/\sqrt{n} \ \rightarrow\ 0\)).
    \end{tabular}
}

%------------------------------------------------------------------
%------------------------------------------------------------------
% Other Integrals
\subsection{Other Integrals}

{\tabcolsep=5pt
    \begin{tabular}{r l}
        Tabular Data:
            &Integrate a piecewise interpolant.
            \\[5pt]
        Improper Integral:
            &\parbox[t]{10cm}{Separate the integral, do a variable change, \\or add/subtract a term to remove singularities.}
            \\[20pt]
        (Fredholm) Integral Equations:
            &skipped 
    \end{tabular}
}

\newpage
%------------------------------------------------------------------
%
%
%------------------------------------------------------------------
% Richardson Extrapolation [Romberg Integration]
\subsection{Richardson Extrapolation [for Integration]}

\vspace{5pt}
\(\hspace{10pt}
    \begin{aligned}
        % Richardson Extrap.
        &\arraycolsep=2pt \begin{array}{l c c c c c c}
                F(h) &= 
                    &I(f) 
                    &+ 
                    &a_1 h^p 
                    &+ 
                    &\mathcal{O}(h^{q > p})
                    \\[5pt]
                F(\tfrac{h}{k}) &=   
                    &I(f) 
                    &+ 
                    &a_1 ( \tfrac{h}{k} )^p 
                    &+ 
                    &\mathcal{O}(h^{r \geq q})
            \end{array}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            \boxed{ I(f) = \frac{ k^{p} \hs F( \tfrac{h}{k} ) - F(h) }{ k^{p} - 1 } + \mathcal{O}(h^{q>p}) }
            \\[10pt]
        % Romberg Integr.
        &\bullet\ \text{Romberg Integration [Quadratic Extrapolation for Comp. Trapezoidal Rule]}:
            \\[5pt]
        % Taylor Exp. for Trap. Rule w/ Error
        &\begin{aligned}
                T(f, \tfrac{h}{2^k} ) &= I(f) 
                    + 2^k \left[ a_1 ( \tfrac{h}{2^k} )^3 
                    + \mathcal{O}(\tfrac{h}{2^k}^{5}) \right]
                    \\
                T_{k,j=0} &= I(f) 
                    + h a_1 [\tfrac{h}{2^k}]^2 
                    + h\hs \mathcal{O}([\tfrac{h}{2^k}]^4)
                    \\[15pt]
                4T_{k+1,0} &= 4I(f) 
                    + h a_1 [\tfrac{h}{2^k}]^2 
                    + \tfrac{h}{4}\hs \mathcal{O}( [\tfrac{h}{2^{k}}]^4)
            \end{aligned}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            % Recursive Alg.
            \begin{aligned}
                T_{k+1, j+1} &\equiv \frac{ 4^{j+1}\ T_{k+1, j} - T_{k, j} }{ 4^{j+1} - 1 }
                    \hspace{20pt} {\scriptstyle (1 \leq j \leq k)}
                    \\[5pt]
                \Aboxed{ I(f) &= T_{k, j} + \mathcal{O}(h^{2j+2}) }
            \end{aligned}
    \end{aligned}
\)

%--------------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------------
\newpage
%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Numerical Differentiation
% \vspace{5pt}
\section{Numerical Differentiation}

% Conditioning
\underline{Conditioning}:\ \ {\scriptsize Inverse of Integration - which smoothes noisy data - so %
    derivatives are inherently sensitive to small changes.%
} 

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Finite-Difference Approx
\vspace{10pt}
\begin{minipage}[t]{.5\textwidth}
    \subsection{Finite-Difference Approx}

    \(\arraycolsep=2pt \begin{array}{r c c c l}
        f'(x) &= 
            &\frac{f(x+h) - f(x)}{h} 
            &- 
            &\mathscriptsize{\displaystyle\sum_{n=2}^\infty}\hs \frac{f^{(n)}(x)}{n!} h^{n-1}
            \\
        &= 
            &\frac{f(x) - f(x-h)}{h} 
            &- 
            &\mathscriptsize{\displaystyle\sum_{n=2}^\infty}\hs \frac{f^{(n)}(x)}{n!} (-h)^{n-1}
            \\
        &= 
            &\frac{f(x+h) - f(x-h)}{2h} 
            &- 
            &\mathscriptsize{\displaystyle\sum_{n=3}^\text{odd}}\hs \frac{f^{(n)}(x)}{n!} h^{n-1}
    \end{array}\)   
    
    % Notes
    \vspace{10pt}
    \(\bullet\ \text{\scriptsize Use more points \(n\) for higher order approx.}\)
\end{minipage}
%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Deriving Polynomial Interpolant
\begin{minipage}[t]{.49\textwidth}
    \subsection{Deriving Interpolant}

    \(\begin{aligned}
        &\begin{aligned}
                f(x) &\ \approx\ \hat{f}_n(x) = p_{n-1}(x) \in P_{n-1}\\
                f^{(m)}(x)&\ \approx\ \hat{f}_n^{(m)}(x)
            \end{aligned}   
            \\[5pt]
        &\bullet\ \text{\scriptsize Equivalent but easier than finite-diff. approach.}\\
        &\bullet\ \text{\scriptsize Using more points \(n\) leads to better accuracy.}\\
        &\bullet\ \text{\scriptsize Polynomials, or other interpolants like trig. func. can be used.}
    \end{aligned}\)
\end{minipage}

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Richardson Extrapolation
\vspace{10pt}
\subsection{Richardson Extrapolation [for Differentiation]}

\vspace{5pt}
\(\hspace{10pt}
    \begin{aligned}
        % Richardson Extrap.
        &\arraycolsep=2pt \begin{array}{l c c c c c c}
                F(h) &= 
                    &D(f) 
                    &+ 
                    &a_1 h^p 
                    &+ 
                    &\mathcal{O}(h^{q > p})
                    \\[5pt]
                F(\tfrac{h}{k}) &=   
                    &D(f) 
                    &+ 
                    &a_1 ( \tfrac{h}{k} )^p 
                    &+ 
                    &\mathcal{O}(h^{r \geq q})
            \end{array}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            \boxed{ D(f) = \frac{ k^{p} \hs F( \tfrac{h}{k} ) - F(h) }{ k^{p} - 1 } + \mathcal{O}(h^{q>p}) }
            \\[10pt]
        % Example
        &\bullet\ \text{E.g.} \hspace{10pt} D(f) = \tfrac{f(x+h) - f(x)}{h} + \mathcal{O}(h)
            \\[10pt]
        &\hspace{10pt}\begin{aligned}
                F(h) &= \frac{f(x+h) - f(x)}{h}\\[5pt]
                F(\tfrac{h}{2}) &= \frac{f(x + \tfrac{h}{2}) - f(x)}{h/2}
            \end{aligned}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            \boxed{ 
                D(f) = \frac{ 
                    2 \cdot \tfrac{f(x + h/2) - f(x)}{h/2} 
                    - \tfrac{f(x+h) - f(x)}{h}
                }{ 2-1 }
                + \mathcal{O}(h^2)
            }
    \end{aligned}
\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Method of Undetermined Coefficients / System of Moment Equations
\subsection{\small Method of Undetermined Coefficients (MUC) / System of Moment Equations}  

% Quadrature Rule
\(\displaystyle
    \boxed{ \big( D_n(f) \big) (a)
        \ \equiv\ \frac{d f}{dx}(a)
        = \frac{d\hat{f}}{dx}(a)
        = \sum_{i=1}^n f(x_i) \hs \phi'_i(a)\hs
        = \sum_{i=1}^n f(x_i)\hs w_i
        }
        \hspace{20pt} \begin{aligned}
            &{\scriptstyle \bullet\ \boxed{\scriptstyle x_i,\ w_i \ \rightarrow\ 2n \text{\scriptsize\ max param.} } }\\[-4pt]
            % &{\scriptstyle \bullet\ a\ \leq\ x_1\ <\ \dots\ <\ x_n\ \leq\ b} \\[-7pt]
            % &\text{\scriptsize \(\bullet\ \) closed if equality, open if not}
        \end{aligned}
\)

\vspace{5pt}
\(
    % Math to MUC
    \mathscriptsize{ \arraycolsep=2pt \begin{array}{c c c}
        \displaystyle \left( \frac{d}{dx} \sum_{j=1}^{n} c_j\hs x^{j-1} \right) {(a)}
            &=
            &\displaystyle \sum_{i=1}^n \left( \sum_{j=1}^{n} c_j\hs x_i^{j-1} \right) w_i
            \\[20pt]
        \displaystyle \sum_{j=1}^{n} c_j \frac{d (x^{j-1}) }{dx} {(a)}
            &=
            &\displaystyle \sum_{j=1}^{n} c_j \left( \sum_{i=1}^n x_i^{j-1} \hs w_i \right)
            \\[20pt]
        \multicolumn{3}{c}{ \text{\scriptsize(maybe some dot product to isolate terms)} }
    \end{array} }
    \hfill \Rightarrow \hfill 
    % MUC Conclusion
    \begin{gathered}
        \begin{aligned}
                \Aboxed{ \mathscriptsize{\sum_{i=1}^n} \hs f{\scriptstyle(x_i)} \hs w_i 
                    &= \tfrac{d \hat{f}}{dx} {\scriptstyle(a)} }
                    \\
                %
                    &\hs \hs \downarrow
                    \\
                \Aboxed{ \mathscriptsize{\sum_{i=1}^n} \hs x_i^{j-1} \hs w_i 
                    &= \tfrac{d (x^{j-1}) }{dx} {\scriptstyle(a)} }
                    \\
                %
                    &\equiv z_j
            \end{aligned}
            \\[5pt]
        \bullet\ \boxed{ \scriptstyle z_1 \ =\ \sum w_i \ =\ 0 }
    \end{gathered}
    \hfill
    % MUC Vandermonde Matrices
    \boxed{ 
        \begin{gathered}[b]
            \text{\scriptsize(Vandermode Matrix)}
                \\
            \mathscriptsize{\left[\begin{matrix}
                    1 & 1 & 1 & \dots\\
                    x_1 & x_2 & x_3 & \dots\\[3pt]
                    x_1^2 & x_2^2 & x_3^2 & \dots\\
                    \vdots & \vdots & \vdots
                \end{matrix}\right]}
        \end{gathered}
        \hsvec{w}
        = \hsvec{z}
        = \mathscriptsize{\left[\begin{matrix}
            0\\
            1\\
            2a\\
            \vdots
        \end{matrix}\right]}
    }
\)

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Initial Value Problems for ODEs
\newpage
\section{Initial Value Problems for ODEs: \(
    \hsvec{y'}{\scriptstyle(t)} = \hsvec{f}{\scriptstyle(t,\hsvec{y})}
    ;\ \hsvec{y}{\scriptstyle(t_0)}
\)}

% Terms
\vspace{5pt}
\(\arraycolsep=2pt 
    \begin{array}{r c l}
        k \text{-th Order}: &\hspace{5pt}
            &f(t, y, \dots, y^{(k)}) = 0
            \\[5pt]
        \text{Autonomous}: &\hspace{5pt}
            &f(y, \dots, y^{(k)}) = 0
    \end{array}
    \hspace{1.5cm}
    \begin{array}{r c l}
        \text{Linear}: &\hspace{5pt}
            &\hsvec{a}(t) \cdot [y(t), \dots, y^{(k)}(t)]^T = b(t) 
            \\[5pt]
        \text{Homogeneous}: &\hspace{5pt}
            &b(t) = 0
    \end{array}
\)

% System of ODEs
\vspace{5pt}
\underline{ \(\begin{gathered}
    \text{\(k\)-th Order}\\[-5pt]
    \text{System of \(n\)}\\[-5pt]
    \text{Coupled ODEs}
\end{gathered}\) }\ : 
\ \ \(
    \hsvec{y'_i}(t) 
    = \mathscriptsize{ 
        \left[ \begin{gathered}
            y_i'(t)\\
            u_2'(t) \\[-4pt]
            \vdots\\[-4pt]
            u_{k-1}'(t) \\
            u_k'(t)
        \end{gathered} \right] 
    }
    = \mathscriptsize{ 
        \left[ \ \begin{gathered}
            u_2(t)\\
            u_3(t)\\[-4pt]
            \vdots\\[-4pt]
            u_k(t)\\
            f(t, u_1, \dots, u_k)
        \end{gathered} \ \right]
    }
    \hspace{5pt} \Rightarrow \hspace{5pt}
    \boxed{ 
        \hsvec{y'}(t) 
        = \mathscriptsize{ 
            \left[ \begin{gathered}
                \hsvec{\ y'_1}(t) \\[-4pt]
                \vdots\\[-4pt]
                \hsvec{\ y'_n}(t)
            \end{gathered} \right] 
        }
        = \hsvec{f}(t, \hsvec{y}) 
        = \mathscriptsize{ 
            \left[ \begin{gathered}
                \hsvec{f_1}(t, \hsvec{y}) \\[-4pt]
                \vdots\\[-4pt]
                \hsvec{f_n}(t, \hsvec{y})
            \end{gathered} \right] 
        }
    }
\)

% Nonautonomous -> Autonomous
\underline{Nonautonomous \(\rightarrow\) Autonomous}:\ \ \(
    \boxed{ 
        \hsvec{y'} = f(t, \hsvec{y})
        \ \rightarrow \ 
        \mathscriptsize{
            \left[ \hs \begin{aligned}
                \hsvec{y'} &= f(y_{n+1}, \hsvec{y}) \\
                y'_{n+1} &= 1
            \end{aligned} \hs \right]
        }
    }
\)

%---------------------------------------------------------------
%
%
%---------------------------------------------------------------
% Stability/Conditioning
\subsection{ODE Stability (Conditioning)}

% Unique/Exist if Lipschitz Cont.
\underline{Unique/Exists if Lipschitz Continuous}: \\[5pt] 
\(
    \Vert{ \hsvec{f}(t, \hat{y}) - \hsvec{f}(t, \hsvec{y}) }\Vert_\infty 
    \leq L \Vert{ \hat{y} - \hsvec{y} }\Vert^{\alpha = 1}_\infty
    = \big( \max \Vert{ J_f(t, \hsvec{y}) }\Vert \big) \cdot \Vert{ \hat{y} - \hsvec{y} }\Vert
    \hspace{30pt} \text{\scriptsize Assured if \(\hsvec{f}\) is differentiable }
\)

% Bound
\vspace{5pt}
\(
    \underline{ \hat{y'} = \hat{f}(t, \hat{y}) }: 
    \ \ \Vert{ \hat{y} - \hsvec{y} }\Vert_\infty 
    \leq e^{L(t-t_0)} \Vert{ \hat{y}(t_0) - \hsvec{y}(t_0) }\Vert_\infty
    + \frac{ e^{L(t-t_0)} - 1 }{L} \left( \max \Vert{ \hat{f} - \hsvec{f} }\Vert \right)
    \hspace{20pt} \Big( \begin{gathered}
        \text{\scriptsize cont./well-posed but}\\[-7pt]
        \text{\scriptsize possibly sensitive}
    \end{gathered} \Big)
\)

% Stability epsilon-delta
\vspace{5pt}
\underline{Stable (\(\epsilon\)-\(\delta\))}:\ \ \(
    \forall \epsilon>0, \exists \delta > 0 :\ 
    \Vert{ \hat{y}(t_0) - \hsvec{y}(t_0) }\Vert < \delta 
    \Rightarrow \Vert{ \hat{y}(t) - \hsvec{y}(t) }\Vert < \epsilon
    \hspace{20pt} \text{\scriptsize(exp. above ruled out)}
\)

% Asymtotically stable
\vspace{5pt}
\underline{Asymptotically Stable}:\ \ \(\displaystyle \lim_{t \rightarrow \infty}\Vert{ \hat{y}(t) - \hsvec{y}(t) }\Vert = 0\)

% One dimension ODE
\[
    y' = \lambda y \ \Rightarrow \ \lim_{t \rightarrow \infty} y = y_0 e^{\lambda t} 
    \hspace{10pt} \left\{ \mathscriptsize{ 
        \begin{array}{r c l}
            \text{Re}(\lambda) < 0 &\rightarrow& \text{Asymp. Stable [Cond.]}\\
            \text{Re}(\lambda) = 0 &\rightarrow& \text{Oscillating (Stable Cond.)}\\
            \text{Re}(\lambda) > 0 &\rightarrow& \text{Unstable [Cond.]}\\
        \end{array} 
    } \right.
\]

\(
    \hspace{.5cm} \begin{aligned}
        % Linear System of ODEs
        &\underline{\text{Linear System of ODEs}}\\
        &\boxed{ \hsvec{y'} = A\hsvec{y} }\\
        &\hsvec{y}(0) = \hsvec{y}_0
            \\[10pt]
        % Nonlinear System of ODEs
        &\underline{\text{Noninear System of ODEs}}\\
        &\boxed{ \hsvec{y'} = \hsvec{f}(t, \hsvec{y}) }\\
        &\rightarrow\ \hsvec{y'} \approx J_f{\scriptstyle(t, \hsvec{y})} \hs \hsvec{y}
    \end{aligned}
    \hspace{1cm} \rightarrow \hspace{1cm}
    \begin{aligned}
        % A is diagonalizable / J_f is diagonalizable & f is autonomous
        &\bullet\ A \text{ is diagonalizable %
            / \(J_f\) is diagonalizable}
            \\
        &\Rightarrow\ \hsvec{y}_0 = \sum a_i \hsvec{v}_i
            , \ \ \hsvec{y}(t) = \sum a_i \hsvec{v}_i \hs e^{\lambda_i t}
            \\[5pt]
        &\rightarrow \ \mathscriptsize{ 
            \arraycolsep=3pt \left\{ \begin{array}{r c l}
                \forall \lambda_i \ \ \text{Re}(\lambda_i) < 0 &\rightarrow& \text{Asymp. Stable [Cond.]}\\
                \forall \lambda_i \ \ \text{Re}(\lambda_i) = 0 &\rightarrow& \text{Stable [Cond.]}\\
                \exists \lambda_i \ \ \text{Re}(\lambda_i) > 0 &\rightarrow& \text{Unstable [Cond.]}\\
            \end{array} \right.
            }\\[5pt] 
        % A Isn't diagonalizable / J_f isnt diagonalizable & f is autonomous 
        &\bullet\ A \text{ isn't diagonalizable %
            / \(J_f\) isn't diagonalizable}
            \\[5pt]
        &\rightarrow \text{\scriptsize 
            Stable [Cond.] if \ \(
                \forall \lambda_i \ \ 
                \arraycolsep=3pt \begin{array}{r c l}
                    \bullet\ \text{Re}(\lambda_i) &\leq& 0 \\
                    \bullet\ \text{Re}(\lambda_i) &<& 0 \text{ if \(\lambda_i\) isn't simple.} 
                \end{array}
            \)
            }\\[5pt]
        % A = A(t) / f isn't autonomous
        &\bullet\ A = A(t) \text{ / } \vec{f} \text{ isn't autonomous} \ \rightarrow\ J_f = J_f(t, \hsvec{y})\\
        &\rightarrow\ \text{\scriptsize Might not be long-term stable}
    \end{aligned}
\)

%----------------------------------------------------------
%
%
%----------------------------------------------------------
\newpage

% Algorithm Stability and Error
\subsection{Algorithm Stability and Error}

% Local Truncation Error
\underline{Local [Trunc.] Error (Accuracy)}: \ \ \(
    \hsvec{l}_k = \hsvec{y}_k{\scriptstyle(t_k)} - \hsvec{y}_{k-1}{\scriptstyle(t_k)}
    = \mathcal{O}(h_k^{p+1}) 
    \ \rightarrow \
    \tfrac{ \hsvec{l}_k }{ h_k } = \mathcal{O}(h_k^{p})
    \hspace{20pt} \boxed{ \text{\scriptsize(Order \(p\))} }
\)

% Global Truncation Error
\underline{Global [Trunc.] Error (Stability)}: \ \ \(
    \hsvec{e}_k 
    = \hsvec{y}_k - \hsvec{y}(t_k) 
    \ =\ \mathcal{O}(\widehat{h_k}^p) \hspace{20pt} \text{\scriptsize(under ``reasonable'' conditions)}
\)

% Growth/Amplification Factor
\underline{\(\begin{gathered}
    \text{Growth/}\\[-5pt]
    \text{Amplification}\\[-5pt]
    \text{Factor}, g
\end{gathered}\)}\ : \ \ \(
    \begin{aligned}
        &\bullet\ y' = \lambda y \ \Rightarrow\ y_k = g^k \hs y_0
            \ \left\{ \ \mathscriptsize{ \begin{aligned}
                |g| \leq 1 &\ \rightarrow\ \text{Stable}\\[-4pt]
                |g| > 1 &\ \rightarrow\ \text{Unstable}
            \end{aligned} } \right.
            \\
        &\bullet\ \hsvec{e}_{k+1}  = g \hsvec{e}_k + \hsvec{l}_{k+1} 
            \ \left\{ \ \mathscriptsize{ \begin{aligned}
                \rho(g) \leq 1 &\ \rightarrow\ \text{Stable}\\[-4pt]
                \rho(g) > 1 &\ \rightarrow\ \text{Unstable}
            \end{aligned} } \right.
            % Spectral Radius
            \hspace{20pt} \big( \ \begin{gathered}
                \text{\scriptsize Spectral Radius, \(\rho(\mathbb{R}^{n \times n})\)}, \\[-9pt]
                \text{\scriptsize may vary with \(t\)}
            \end{gathered} \ \big)
    \end{aligned}
\)

% Unconditionally Stable
\underline{Unconditionally Stable}: \ \ If stable alg. when \(
    (\forall h,\ h > 0),
    \ (\forall\lambda_i,\ \text{Re}{\scriptstyle(\lambda_i)} < 0 \Rightarrow \text{\scriptsize Stable [Cond.]}) 
\)

% Implicit vs Explicit Method
\underline{Implicit Method}: \ \ \(y_{k+1} = y_{k+1}{\scriptstyle(t_k,\ t_{k+1},\ \dots)}\)
    \hspace{20pt} \text{\scriptsize(usually more stable than expicit methods, \(y_{k+1} = y_{k+1}{\scriptstyle(t_k)}\))}

%----------------------------------------------------------
%----------------------------------------------------------
% ODE Stiffness
\subsection{ODE Stiffness}
% Asymptotic Stiffness
\underline{[Asymptotic] Stiffness}: \ \ \(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Rapid asymp. decay to convergence; 
        Re\((\lambda_i(J_f)) \ll\ \sim 0\) and differ greatly in magnitude.}
        \\[-5pt]
    &\bullet\ \parbox[t]{.7\textwidth}{\scriptsize Normally small \(h_k\) required; %
        \underline{even w/ an alg. with no local error, a perturbation of an initial}\\[2pt] %
        \underline{value may cause a step to overshoot to neighboring solutions/level sets.}
        }\\[-1pt]
    &\bullet\ \text{\scriptsize Implicit methods with greater range of %
        stability allow larger \(h_k\) for stiff ODEs than explicit ones.}
\end{aligned}\)

% Oscillatory Stiffness
\underline{[Oscillatory] Stiffness}: \ \ {\scriptsize Rapid oscillation stiffness; %
    \(|{ \text{Im}(\lambda_i(J_f)) }| \gg\ \sim 0\) and differ greatly in magnitude. Treatment not given.%
}

%------------------------------------------------------------
%------------------------------------------------------------
% Taylor Series Methods
\subsection{Taylor Series Algorithms, \(\displaystyle \boxed{ 
        \hsvec{y}{\scriptstyle(t+h)} = \hsvec{y}{\scriptstyle(t)}
        + \mathscriptsize{\sum_i^p}\hs \tfrac{h^i}{i!} \hsvec{y}^{(i)}{\scriptstyle(t)}
        + \mathcal{O}{\scriptstyle (h^{p+1})}
    }
\)}

% Local Error Tolerance
\underline{\(
    \begin{gathered}
        \text{Local Error}\\[-5pt]
        \text{Tolerance}
    \end{gathered}
\)}\ : 
\ \ \(
    \tfrac{h^{p+1}}{(p+1)!} \Vert{ \hsvec{y}^{(p+1)}{\scriptstyle(t)} }\Vert \leq tol
    \ \Rightarrow \
    \boxed{ h_k \ \lesssim\ \sqrt[p+1]{ (p+1)! \cdot tol / \Vert{ \hsvec{y}^{(p+1)}_k }\Vert } }
    \hspace{20pt} \Big( \hs \begin{gathered}
        \text{\scriptsize Could use finite}\\[-7pt]
        \text{\scriptsize diff for \(\Vert{ \hsvec{y}''_k }\Vert\)}
    \end{gathered} \hs \Big)
\)

% Euler's Method
\underline{[Explicit] Forward Euler's Method (1st Order)}: \ \ \(\hsvec{y}_{k+1} 
    \ =\ \hsvec{y}_k + h_k \hsvec{y'}_k
    \ =\ \boxed{ \hsvec{y}_k + h_k \hsvec{f}{\scriptstyle(t_k, \hsvec{y}_k)} }
\)

% Euler's Method Algorithm Stability
\(\hspace{15pt}
    \begin{aligned}
        % One Dimensional
        &\bullet\ g:\ y_k = (1 + h_k \lambda)^k \hs y_0
            \ \Rightarrow \ 
            % Accuracy
            \mathscriptsize{ e^{\lambda h} = g + \mathcal{O}(h^2) }
            \hspace{10pt} \boxed{\scriptstyle p=1}
            \hspace{10pt} , \hspace{10pt} 
            % Stable Radius 
            \mathscriptsize{
                |1 + h\lambda| \leq 1 \ \rightarrow\ \boxed{ \text{Stable if } \lambda: |1/h + \lambda| \leq 1/h }
            } 
            \\
        % Multidimension
        &\bullet\ \begin{aligned}[t]
                g:\ \hsvec{e}_{k+1} &= \hsvec{y}_{k+1} - \hsvec{y}{\scriptstyle(t_k + h_k)} 
                    = \big[ \hsvec{y}_k + h_k \hsvec{y'}_k \big]
                    - \big[ \hsvec{y}{\scriptstyle(t_k)} + h_k \hsvec{y'}{\scriptstyle(t_k)} + \mathcal{O}(h_k^2) \big]      
                    \\
                &= \big[ \hsvec{y}_k - \hsvec{y}{\scriptstyle(t_k)} \big]
                    + h_k \big[ \hsvec{f}{\scriptstyle(t_k, \hsvec{y}_k)} - \hsvec{f}{\scriptstyle(t_k, \hsvec{y}(t_k))} \big]
                    - \mathcal{O}(h_k^2)
                    \hspace{10pt} \boxed{\scriptstyle p=1}
                    \\
                &= \hsvec{e}_k 
                    + h_k \bar{J}_f \hsvec{e}_k - \mathcal{O}(h_k^2)
                    % Mean Value Theorem
                    \hspace{10pt} , \hspace{10pt}
                    \Big(\hs
                        \begin{gathered}
                            \text{\scriptsize From Mean}\\[-9pt]
                            \text{\scriptsize Value Theorem}
                        \end{gathered} 
                        \hspace{5pt} : \hspace{5pt}
                        \bar{J}_f \hsvec{e}_k = \mathscriptsize{\int_0^1} J_f( 
                            {\scriptstyle t_k,\ \alpha \hsvec{y}_k + (1-\alpha) \hsvec{y}(t_k)} 
                        ) \hs d\alpha \cdot \hsvec{e}_k
                    \hs\Big)
                    \\
                &= \big[ I + h_k \bar{J}_f \big] \hsvec{e}_k 
                    + \hsvec{l}_{k+1}
                    \ \Rightarrow \ 
                    % Stable Radius 
                    \mathscriptsize{ \begin{aligned}
                        \Aboxed{ \rho{\scriptstyle(I + h_k \bar{J}_f)} \leq 1 &\ \rightarrow\ \text{Stable} }
                    \end{aligned} }
            \end{aligned}
            \\
        % Stiffness tolerance
        \vspace{5pt}
        &\bullet\ \underline{\text{Stiffness Tolerance}}: \ \boxed{ 
            h_k \cdot \min( \text{Re}( \lambda_i{\scriptstyle(J_f)} ) ) \ll -1 
            }
            \hspace{10pt}
            \text{\scriptsize(small tolerance, not uncond. stable)}
    \end{aligned}
\)

% Backwards Euler's Method
\underline{[Imp.] Backwards Euler's Method}: \ \ \(\hsvec{y}_{k+1} 
    \ =\ \boxed{ \hsvec{y}_k + h_k \hsvec{f}{\scriptstyle(t_{k+1}, \hsvec{y}_{k+1})} }
    \hspace{20pt}
    \Big(\hs \begin{gathered}
        \text{\scriptsize Solve Nonlin. Eq.; use init.}\\[-9pt]
        \text{\scriptsize guess from an explicit method}
    \end{gathered}\hs \Big)
\)

% Backwards Euler's Method Algorithm Stability
\(\hspace{15pt}
    \begin{aligned}
        % One Dimensional
        &\bullet\ g:\ y_k = (\tfrac{1}{1 - h_k \lambda})^{k} \hs y_0
            \ \Rightarrow \
            % Accuracy
            \mathscriptsize{ e^{\lambda h} = 1/(1 - h \lambda) + \mathcal{O}(h^2) } 
            \hspace{10pt} \boxed{\scriptstyle p=1}
            \hspace{10pt} , \hspace{10pt}
            % Stable Radius 
            \mathscriptsize{ \begin{aligned}
                \Aboxed{ |1 - h\lambda| \geq 1 &\ \rightarrow\ \text{Unconditionally Stable} }\\
            \end{aligned} }
            \\[5pt]
        % Multidimensional
        &\bullet\ g: \ \rho({\scriptstyle(I - h J_f)^{-1}}) \leq 1 
            \ \ \text{\scriptsize(\emph{sic} \(h J_f\))}
            \ \rightarrow\ \boxed{ \text{\scriptsize Unconditionally Stable \(\rightarrow\) Greater Stiffness Tol.} } 
    \end{aligned}
\)

% 2nd Order
% \vspace{5pt}
\underline{\(\begin{gathered}
    \text{[Exp.]}\ p=2
\end{gathered}\)}\ : 
\ \ \(
    \hsvec{y}_{k+1} = \hsvec{y}_k 
        + h_k \hsvec{y'}_k 
        + \tfrac{h_k^2}{2} \hsvec{y''}_k
    = \boxed{ \hsvec{y}_k 
        + h_k \hsvec{f}{\scriptstyle(t_k, \hsvec{y}_k)} 
        + \tfrac{h_k^2}{2} \left( \hsvec{f}_t{\scriptstyle(t_k, \hsvec{y}_k)} 
            + \hsvec{f}_y{\scriptstyle(t_k, \hsvec{y}_k)} \hsvec{f}{\scriptstyle(t_k, \hsvec{y}_k)} 
        \right) }
\)

%
%
%------------------------------------------------------------
%------------------------------------------------------------
\newpage
% Explicit Runge-Kutta Methods
\subsection{[Exp.] Runge-Kutta Algorithms}

\[\boxed{
    \hsvec{y}_{k+1} \ =\ \hsvec{y}_k + h_k \sum_{i=1}^{s} b_i k_i  
    \hspace{15pt} , \hspace{15pt}
    k_i = \hsvec{f}( t_k + c_i h_k,\ \hsvec{y}_k + h_k \hs \mathscriptsize{\sum_j^{i-1}}\hs a_{ij}k_j )
    \hspace{15pt} , \hspace{15pt}
    \mathscriptsize{\sum_i^s}\hs b_i = 1
}\]

% Heun's Method/Trapezoid Rule
\vspace{-5pt}
\underline{\(\begin{gathered}
    \text{Heun's Method/}\\[-3pt]
    \text{Trapezoid Rule}\\[-3pt]
    (p=2, s=2)
\end{gathered}\)}\ : 
\ \ \(
    \begin{aligned}
        \hsvec{y}_{k+1} &\ \approx\ \hsvec{y}_k + \tfrac{h_k}{2} \big[
                \hsvec{f}{\scriptstyle(t_k, \hsvec{y}_k)} 
                + \hsvec{f}{\scriptstyle(t_{k+1}, \hsvec{y}_{k+1})}
            \big] 
            \hspace{10pt}
            \text{\scriptsize(See Multistep Trap. Rule Below)}
            \\
        &\ =\ \boxed{ 
                \hsvec{y}_k 
                + \tfrac{h_k}{2} \big[
                    \hsvec{f}{\scriptstyle(t_k, \hsvec{y}_k)} 
                    + \hsvec{f}( {\scriptstyle t_k + h_k,\ \hsvec{y}_{k} + h_k \hsvec{f}(t_k, \hsvec{y}_k) } )
                \big]
            }\\
        &\ =\ \hsvec{y}_k + \tfrac{h_k}{2} \left[ k_1 + k_2 \right]
    \end{aligned}
    \hspace{-40pt}
    \mathscriptsize{ \begin{aligned}
            \\[10pt]
        \hsvec{c} = \left[ \begin{matrix}
                0\\
                1
            \end{matrix} \right]
            &, 
            \left[ \begin{matrix}
                \\
                1 & 
            \end{matrix} \right]
            = a
            \\
        &, \left[ \tfrac{1}{2}, \tfrac{1}{2} \right]^T = \hsvec{b} 
    \end{aligned} }
\)

% RK4/Simpsons Rule
\vspace{5pt}
\underline{\(\begin{gathered}
    \text{RK4/}\\[-3pt]
    \text{Simpson's Rule}\\[-3pt]
    (p=4, s=4)
\end{gathered}\)}\ : 
\ \ \(
    \begin{aligned}
        \hsvec{y}_{k+1} &\ =\ \hsvec{y}_k + \tfrac{h_k}{6} \big[ k_1 + 2 k_2 + 2 k_3+ k_4 \big]
    \end{aligned}
    \hspace{10pt}
    \mathscriptsize{ \begin{aligned}
        \hsvec{c} = \left[ \begin{matrix}
                0\\
                1/2\\
                1/2\\
                1
            \end{matrix} \right]
            &, 
            \left[ \begin{matrix}
                \\
                \tfrac{1}{2} \\
                0 & \tfrac{1}{2}\\
                0 & 0 & 1 & \ \
            \end{matrix} \right]
            = a
            \\
        &, \left[\ \tfrac{1}{6}\ \ \ \tfrac{2}{6}\ \ \ \tfrac{2}{6}\ \ \tfrac{1}{6}\ \right]^T = \hsvec{b} 
    \end{aligned} }
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \text{\scriptsize Explicit Runge-Kutta have no error estimates to base step-size on.}\\[-5pt]
    &\bullet\ \text{\scriptsize Embedded [paired] RK methods have pair-diff. error estimates.}\\[-10pt]
    &\bullet\ \text{\scriptsize Implicit RK methods exist for stiff ODEs: \(
            k_i = \hsvec{f}( t_k + c_i h_k,\ \hsvec{y}_k + h_k \hs \mathscriptsize{\sum_j^{s}}\hs a_{ij}k_j )
        \)}
\end{aligned}\)

%--------------------------------------------------------------
%--------------------------------------------------------------
% [Linear] Multistep Algorithms
\subsection{[Linear] Multistep Algorithms ({\scriptsize May Use Previous Points/Not Self-Starting})}

\[
    \begin{gathered}
        \text{Interpolate}\\[-5pt]
        \text{\scriptsize(use MUC for coefficients)}\\[-7pt]
        {\scriptstyle(\forall t_k,\ \text{\scriptsize Let } h=1)}
    \end{gathered}
    \hspace{15pt}
    \boxed{
        \hsvec{y}_{k+1} \ =\ \sum_{i=1}^m \alpha_i \hs y_{k-(i-1)} 
        \ +\ h \sum_{i=0}^m \beta_i \hsvec{y'}_{k+1-i}
    }    
    \hspace{15pt} \text{\scriptsize or} \hspace{15pt}
    \boxed{
        \begin{gathered}
            \mathscriptsize{ \sum_{i=0} a_i y_{k+1-i} = h \sum_{i=0} b_i y'_{k+1-i} }
                \\
            \text{\scriptsize(Explicit if \(\beta_0 = 0\))}
        \end{gathered}
    }
\]

% Adams Method
\underline{\(\begin{gathered}
    \text{Adams Methods}\\[-1pt]
    \mathscriptsize{ \begin{aligned}
        &\text{Explicit = Adams-Bashforth (AB)}\\[-4pt]
        &\text{Implicit = Adams-Moulton (AM)}
    \end{aligned} }
\end{gathered}\)}\ : 
\ \ \(\displaystyle 
    \boxed{
        \hsvec{y}_{k+1}
        \ =\ \hsvec{y}_k
        + h \hs \mathscriptsize{\sum_{i=0}^m} \hs \beta_i \hsvec{y'}_{k+1-i}
    }
    \ \Leftrightarrow\ \mathscriptsize{
        \begin{aligned}
            \hsvec{y}_{k+1} - \hsvec{y}_k
                &\ =\ \mathscriptsize{\sum} \hs \hsvec{y'}_{k+1-i} \hs 
                \mathscriptsize{\int_{t_k}^{t_k+h}} \phi_{k+1-i}{\scriptstyle(t)} \hs dt
                \\[-3pt]
            \tfrac{t_{k+1}^j}{j} - \tfrac{t_{k}^j}{j} &\ =\ \mathscriptsize{\sum} \hs t_{k+1-i}^{j-1} \hs w_{k+1-i}
        \end{aligned}
    }
\)

% Explicit 2-Step
\underline{\(\begin{gathered}
    \text{2-Step AB}
\end{gathered}\)}\ : 
\ \ \(
    \begin{gathered}
        {\scriptstyle t_{k-1}=0}\\[-7pt]
        {\scriptstyle h=1}
    \end{gathered}
    \ ,\ 
    \begin{aligned}[t]
        &\mathscriptsize{ 
            % 3x3 Matrix
            \begin{aligned}[b]
                &\hspace{6pt} \arraycolsep=5pt \begin{array}{l l l}
                        y_k & y'_k & \ y'_{k-1}
                    \end{array}
                    \\
                &\left[\arraycolsep=5pt \begin{array}{c c c}
                        1     & 0      & 0\\
                        t_k   & 1      & 1\\
                        t_k^2 & 2t_k   & 2t_{k-1}
                    \end{array}\right]
            \end{aligned}
            \left[\begin{matrix}
                \alpha_1\\ 
                h\beta_1\\
                h\beta_2
            \end{matrix}\right]
            = 
            \begin{gathered}[b]
                y_{k+1}\\
                \left[\arraycolsep=2pt\begin{array}{c}
                        1\\ 
                        t_{k+1}\\
                        t_{k+1}^2
                    \end{array}\right]
            \end{gathered}
            \hspace{10pt} \text{or} \hspace{10pt}
            % 2x2 Matrix
            \begin{aligned}[b]
                &\hspace{6pt} \arraycolsep=5pt \begin{array}{l l}
                        t_k^{j-1} & t_{k-1}^{j-1}
                    \end{array}
                    \\
                &\left[\arraycolsep=5pt \begin{array}{c c}
                        1    & 1\\
                        t_k  & t_{k-1}
                    \end{array}\right]
            \end{aligned}
            \left[\begin{matrix}
                h \beta_1\\
                h \beta_2
            \end{matrix}\right]
            = 
            \begin{gathered}[b]
                (t_{k+1}^{j} - t_k^{j})/j\\
                \left[\arraycolsep=2pt\begin{array}{c}
                        t_{k+1} - t_k\\
                        ( t_{k+1}^2 - t_{k}^2 )/2
                    \end{array}\right]
            \end{gathered}
            }\\
        % Solution
        &\Rightarrow\
            \boxed{
                \hsvec{y}_{k+1} = \hsvec{y_k} + h \left( \tfrac{3}{2} \hsvec{y'}_k- \tfrac{1}{2} \hsvec{y'}_{k-1} \right)
            }
    \end{aligned}
\)

% Implicit 2-Step/Trapezoid Rule
\vspace{5pt}
\underline{\(\begin{gathered}
    \text{2-Step AM/}\\[-3pt]
    \text{Trapezoid Rule}\\[-3pt]
\end{gathered}\)}\ : 
\ \ \(
    \begin{aligned}
        % 2x2 Matrix
        &\begin{gathered}
                {\scriptstyle t_k=0}\\[-7pt]
                {\scriptstyle h=1}
            \end{gathered}
            \hspace{10pt} 
            \mathscriptsize{
                \begin{aligned}[b]
                    &\hspace{6pt} \arraycolsep=4pt \begin{array}{l l}
                            t_{k+1} & \ t_{k}
                        \end{array}
                        \\
                    &\left[\arraycolsep=5pt \begin{array}{c c}
                            1    & 1\\
                            t_{k+1}  & t_{k}
                        \end{array}\right]
                \end{aligned}
                \left[\begin{matrix}
                    h \beta_0\\
                    h \beta_1
                \end{matrix}\right]
                = 
                \begin{gathered}[b]
                    (t_{k+1}^{j} - t_k^{j})/j\\
                    \left[\arraycolsep=2pt\begin{array}{c}
                            t_{k+1} - t_k\\
                            ( t_{k+1}^2 - t_{k}^2 )/2
                        \end{array}\right]
                \end{gathered}
            }
            \hspace{10pt} \text{\scriptsize or} \hspace{10pt}
            % Taylor Series
            \hsvec{y}_k + h \hsvec{y'}_k + \tfrac{h^2}{2} \left[ 
                \tfrac{ \hsvec{y'}_{k+1} - \hsvec{y'}_{k} }{ h }
            \right]
            \\
        &\hspace{10pt} \text{\scriptsize or} \hspace{10pt} 
            % Trapezoid
            \hsvec{y}_k + \tfrac{(t_k + h) - t_k}{2} \big[ \hsvec{y'}_k + \hsvec{y'}_{k+1} \big]
            % Solution
            \ \Rightarrow\ 
            \boxed{ 
                \hsvec{y}_{k+1} 
                \ =\ \hsvec{y}_k 
                + \tfrac{h}{2} \big[
                    \hsvec{y'}_k
                    + \hsvec{y'}_{k+1}
                \big]
            }
    \end{aligned}
\)

% Trapezoid Rule Algorithm Stability
\vspace{5pt}
\(\hspace{15pt}
    \begin{aligned}
        % One Dimensional
        &\bullet\ g:\ y_k = (\tfrac{1 + h \lambda / 2}{1 - h \lambda / 2})^{k} \hs y_0
            \ \Rightarrow \ 
            % Accuracy
            \mathscriptsize{ e^{\lambda h} = g + \mathcal{O}(h^3) } 
            \hspace{10pt} \boxed{\scriptstyle p=2}
            \hspace{10pt} , \hspace{10pt}
            % Stable Radius
            \mathscriptsize{ \begin{gathered}
                \boxed{ |g| < 1 \ \rightarrow\ \text{Unconditionally Stable} }\\
            \end{gathered} }
            \\[5pt]
        % Multidimension
        &\bullet\ g: \ \rho({\scriptstyle(I + h J_f)(I - h J_f)^{-1}}) < 1 
            \ \ \text{\scriptsize(\emph{sic} \(h J_f\))}
            \ \rightarrow\ \boxed{ \text{\scriptsize Unconditionally Stable \(\rightarrow\) Greater Stiffness Tol.} } 
            \\[5pt]
        % Local Trunc. Error
        &\bullet\ \text{Local Error Tolerance}:\ \boxed{
                \tfrac{2}{2+1} \tfrac{ 1 }{ 2^2 2! } 
                \Vert{ \hsvec{f}''( {\scriptstyle t_k + h/2,\ \hsvec{y}(t_k + h/2) } ) }\Vert
                \hs h^3 
                \ \lesssim\ tol
            }
    \end{aligned}
\)

%--------------------------------------------------------------
%--------------------------------------------------------------
\newpage
%
%
% Backwards Differentiation Formula Methods
\underline{\(\begin{gathered}
    \text{Backwards Differentiation}\\[-3pt]
    \text{Formula (BDF) Methods}    
\end{gathered}\)}\ : 
\ \ \(\displaystyle 
    \boxed{
        \hsvec{y}_{k+1}
        \ =\ \mathscriptsize{\sum_{i=1}^m} \hs \alpha_i \hs \hsvec{y}_{k-(i-1)} 
        + h \beta_0 \hsvec{y'}_{k+1}
    }
    \ , \ \mathscriptsize{
        \begin{aligned}
            \hsvec{y'}_{k+1}
                &= \sum \hs \hsvec{y}_{k+1-i} \phi'_{k+1-i}{\scriptstyle(t_{k+1})}
                \\
            \tfrac{d(t^{j-1})}{dt}(t_{k+1})
                &= \sum \hs t_{k+1-i}^{j-1} w_{k+1-i}
        \end{aligned}
    }
\)

% Explicit 2-Step BDF
\vspace{5pt}
\underline{\(\begin{gathered}
    \text{2-Step BDF}
\end{gathered}\)}\ : 
\ \ \(
    \begin{gathered}
        {\scriptstyle t_{k-1}=0}\\[-7pt]
        {\scriptstyle h=1}
    \end{gathered}
    \ ,\ 
    \begin{aligned}[t]
        &\mathscriptsize{ 
            % 3x3 Matrix
                \begin{aligned}[b]
                    &\hspace{6pt} \arraycolsep=5pt \begin{array}{l l l}
                            y_k & y_{k-1} & \ y'_{k+1}
                        \end{array}
                        \\
                    &\left[\arraycolsep=5pt \begin{array}{c c c}
                            1     & 1           & 0\\
                            t_k   & t_{k-1}     & 1\\
                            t_k^2 & t^2_{k-1}   & 2t_{k+1}
                        \end{array}\right]
                \end{aligned}
                \left[\begin{matrix}
                    \alpha_1\\ 
                    \alpha_2\\ 
                    h\beta_0
                \end{matrix}\right]
                = 
                \begin{gathered}[b]
                    y_{k+1}\\
                    \left[\arraycolsep=2pt\begin{array}{c}
                            1\\ 
                            t_{k+1}\\
                            t_{k+1}^2
                        \end{array}\right]
                \end{gathered}
            }
            \hspace{10pt} \text{\scriptsize or} \hspace{10pt}
            % Differentiated Interpolation
            \mathscriptsize{ 
                \begin{aligned}[b]
                    &\hspace{6pt} \arraycolsep=5pt \begin{array}{l l l}
                        t_{k+1}^{j-1} & t_k^{j-1} & t_{k-1}^{j-1}
                        \end{array}
                        \\
                    &\left[\arraycolsep=5pt \begin{array}{c c c}
                            1           & 1     & 1\\
                            t_{k+1}     & t_k   & t_{k-1} \\
                            t^2_{k+1}   & t^2_k & t^2_{k-1}
                        \end{array}\right]
                \end{aligned}
                \left[\begin{matrix}
                    1/h\beta_0\\ 
                    -\alpha_1/h\beta_0\\ 
                    -\alpha_2/h\beta_0
                \end{matrix}\right]
                = 
                \begin{gathered}[b]
                    ({\scriptstyle j-1})(t_{k+1}^{j-2})\\
                    \left[\arraycolsep=2pt\begin{array}{c}
                            0\\ 
                            1\\
                            2t_{k+1}
                        \end{array}\right]
                \end{gathered}
            }
            \\[5pt]
        % Solution
        &\Rightarrow\
        \boxed{
            \hsvec{y}_{k+1} = \tfrac{4}{3} \hsvec{y}_k 
            - \tfrac{1}{3} \hsvec{y}_{k-1} 
            + h \left( \tfrac{2}{3} \hsvec{y'}_{k+1} \right)
        }
    \end{aligned}
\)

% Kinda-Generalized Adams
\vspace{5pt}
\underline{Kinda-Generalized Adams (Interpolating \(y'(t)\) then Integrating)}

\(\arraycolsep=0pt 
    \begin{array}{r l c l c r l}
        \displaystyle \mathscriptsize{\sum_{i=0}^m} \hs \left[ \widehat{\mathcal{I}} \phi_{k+1-i}(t) \right]
            &y'_{k+1-i} 
            &\hspace{5pt} = \hspace{5pt} 
            &\widehat{\mathcal{I}} y'(t)
            &\hspace{5pt} = \hspace{5pt} 
            &\displaystyle \mathscriptsize{\sum_{i=0}^m} \hs a_{k+1-i}
            &\hs y_{k+1-i} 
            \\[15pt]
        \displaystyle \mathscriptsize{\sum_{i=0}^m} \hs \left[ b_{k+1-i} \right]
            &\tfrac{d (t^{j-1})}{dt}_{k+1-i}
            &= 
            &\widehat{\mathcal{I}} \tfrac{d (t^{j-1})}{dt}
            &= 
            &\displaystyle \mathscriptsize{\sum_{i=0}^m} \hs a_{k+1-i}
            &\hs t^{j-1}_{k+1-i} 
    \end{array}
    \hspace{20pt}
    % Notes
    \boxed{
    \mathscriptsize{ \begin{aligned}
        &\bullet\ \widehat{\mathcal{I}} f(t) = \int_{t_k}^{t_{k+1}} f(t) \hs dt \hspace{10pt} \text{\scriptsize(Adams)} \\
        &\bullet\ \widehat{\mathcal{I}} f(t) 
            = \big( \hsvec{a} \cdot \left[ 1, e^{-h\nabla}, \dots , e^{-mh\nabla} \right] F \big)(t_{k+1})
    \end{aligned} }
    }
\)

% Kinda-Generalized BDF
\vspace{5pt}
\underline{Kinda-Generalized BDF (Interpolating \(y(t)\) then Deriving)}

\(\arraycolsep=0pt 
    \begin{array}{r l c l c r l}
        \displaystyle \mathscriptsize{\sum_{i=0}^m} \hs \left[ \widehat{\mathcal{D}} \phi_{k+1-i}(t) \right]
            &y_{k+1-i}
            &\hspace{5pt} = \hspace{5pt} 
            &\widehat{\mathcal{D}} y(t)
            &\hspace{5pt} = \hspace{5pt} 
            &\displaystyle \mathscriptsize{\sum_{i=0}^m} \hs b_{k+1-i}
            &\hs y'_{k+1-i} 
            \\[15pt]
        \displaystyle \mathscriptsize{\sum_{i=0}^m} \hs \left[ a_{k+1-i} \right] \hs
            &t^{j-1}_{k+1-i}
            &= 
            &\widehat{\mathcal{D}} t^{j-1}
            &= 
            &\displaystyle \mathscriptsize{\sum_{i=0}^m} \hs b_{k+1-i}
            &\hs \tfrac{d (t^{j-1})}{dt}_{k+1-i} 
    \end{array}
    \hspace{20pt}
    % Notes
    \boxed{
    \mathscriptsize{ \begin{aligned}
        &\bullet\ \widehat{\mathcal{D}} f(t) = \tfrac{df}{dt}(t_{k+1}) \hspace{10pt} \text{\scriptsize(BDF)} \\
        &\bullet\ \widehat{\mathcal{D}} f(t) 
            = \big( \hsvec{b} \cdot \left[ 1, e^{-h\nabla}, \dots , e^{-mh\nabla} \right] f' \big)(t_{k+1})
    \end{aligned} }
    }
\)

% Predictor-Corrector pairs/PECE
\vspace{10pt}
\underline{\(\begin{gathered}
    \text{PECE - Predict[or], Evaluate}\\[-3pt]
    \text{Correct[or], Evaluate}
\end{gathered}\)}\ :
\ \ \parbox{.65\textwidth}{\scriptsize%
    A set of previous point values is used in an explicit multistep algorithm as a \emph{predictor} to find %
    the next value, \(y_{k+1}\). The derivative is then \emph{evaluated} at this next time as %
    \(y'_{k+1} = f(t_{k+1}, y_{k+1})\). With this derivative, an improved value for \(y_{k+1}\) %
    is found with an implicit multistep algorithm as a \emph{corrector}. The derivative \(y'_{k+1}\) can then be improved
    by \emph{evaluating} it again with the improved \(y_{k+1}\) from the corrector. The implicit corrector
    can be repeated to re-evaluate \(y_{k+1}\) and \(y'_{k+1}\) until convergence. PECE is explicit.
}

% Notes
\vspace{5pt}
\(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Mult. methods must be used to get previous values.}\\[-5pt]
    &\bullet\ \parbox[t]{.4\textwidth}{\scriptsize \underline{Changing step-size \(h\) is hard} since %
        interpolation is most convenient for equal-spaced points.}
\end{aligned}
\hspace{1.5cm}
\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Relatively hard to code.}\\[-5pt]
    &\bullet\ \text{\scriptsize Not all imp. methods are unconditionally stable.}\\[-5pt]
    &\bullet\ \text{\scriptsize Method pairs can be used for error estimates.}
\end{aligned}\)

% Multivalue Methods
% \vspace{10pt}
\subsection{Multivalue Methods}

\(
    y' = f(t,y)
    \hspace{10pt} , \hspace{10pt}
    \hsvec{y}_k = \left[ y_k,\ h y'_k,\ \tfrac{h^2}{2} y''_k,\ \tfrac{h^3}{3!} y'''_k \right]^T
    \hspace{10pt} , \hspace{10pt}
    \hsvec{y}_{k+1} = \left[ y_{k+1},\ h y'_{k+1},\ \tfrac{h^2}{2} y''_{k+1},\ \tfrac{h^3}{3!} y'''_{k+1} \right]^T
\)

\(
    % By = \hat{Y}
    \begin{aligned}
        \hsvec{\hat{y}}_{k+1} 
            &\hs \equiv \hs B\hsvec{y}_k 
            = \begin{gathered}[b]
                \big(\begin{gathered}
                    \text{\scriptsize Pascal's}\\[-9pt]
                    \text{\scriptsize Triangle}
                \end{gathered}\big)
                \\[-5pt]
                \left[\mathscriptsize{\begin{matrix}
                    1 & 1 & 1 & 1\\
                    0 & 1 & 2 & 3\\
                    0 & 0 & 1 & 3\\
                    0 & 0 & 0 & 1
                \end{matrix}}\right]
            \end{gathered}
            \left[\mathscriptsize{\begin{aligned}
                &y_k\\[2pt] 
                h &y'_k\\[2pt] 
                (h^2/2!) &y''_k\\[2pt] 
                (h^3/3!) &y'''_k
            \end{aligned}}\right]
            \\
        \left[\mathscriptsize{\begin{aligned}
            &\hat{y}_{k+1}\\[2pt] 
            h &\hat{y}'_{k+1}\\[2pt] 
            (h^2/2!) &\hat{y}''_{k+1}\\[2pt] 
            (h^3/3!) &\hat{y}'''_{k+1}
        \end{aligned}}\right]
            &= \left[\mathscriptsize{
                \begin{aligned}
                    &[y_k + h y'_k + (h^2/2!) y''_k + (h^3/3!) y'''_k]\\[2pt] 
                    h&[y'_k + h y''_k + (h^2/2!) y'''_k]\\[2pt] 
                    (h^2/2!)&[y''_k + h y'''_k]\\[2pt] 
                    (h^3/3!)&[y'''_k]
                \end{aligned}
            }\right]
    \end{aligned}
    \hspace{3pt} \Rightarrow \hspace{3pt}
    \begin{aligned}
        % Conclusion
        &\begin{aligned}
                \hsvec{y}_{k+1}   
                    &= \hsvec{\hat{y}}_{k+1} + \alpha \hsvec{r}
                    \\
                \left[\mathscriptsize{\begin{aligned}
                        &y_{k+1}\\[2pt] 
                        h &y'_{k+1}\\[2pt] 
                        \tfrac{h^2}{2!} &y''_{k+1}\\[2pt] 
                        \tfrac{h^3}{3!} &y'''_{k+1}
                    \end{aligned}}\right]
                    &= \left[\mathscriptsize{\begin{aligned}
                        &\hat{y}_{k+1}\\[2pt] 
                        h &\hat{y}'_{k+1}\\[2pt] 
                        \tfrac{h^2}{2!} &\hat{y}''_{k+1}\\[2pt] 
                        \tfrac{h^3}{3!} &\hat{y}'''_{k+1}
                    \end{aligned}}\right]
                    + h({\scriptstyle y'_{k+1} - \hat{y}'_{k+1}}) 
                    \left[\mathscriptsize{\begin{matrix}
                        r_1 = \tfrac{3}{8}\\[2pt]
                        1\\[2pt]
                        r_3 = \tfrac{3}{4}\\[2pt]
                        r_4 = \tfrac{1}{6}
                    \end{matrix}}\right]
            \end{aligned}
            \\[5pt]
        % Notes
        &\bullet\ \text{\scriptsize Equivalent to Multistep Methods.}\\[-5pt]
        &\bullet\ \text{\scriptsize Easy to change step size \(h\) at \(\hsvec{y}_i\).}\\[-5pt]
        &\bullet\ \text{\scriptsize Easy to change order \(p\) from \(\hsvec{r}\).}\\[-5pt]
    \end{aligned}
\)




\end{document}

