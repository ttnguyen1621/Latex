\documentclass[12pt]{article}
\usepackage[left=.75in, right=.75in, top=1in, bottom = 1in]{geometry}
\usepackage{enumitem, amssymb, amsmath, amsfonts, mathtools, parskip}
\usepackage{relsize}
\usepackage{cancel}
\usepackage{dashbox}

\newcommand{\hs}{\hspace{1pt}}
\newcommand{\rdiag}{\raisebox{0.5ex}{\scalebox{0.7}{$\diagdown$}}}
\newcommand{\hsvec}[1]{\vec{\hs #1}}
\newcommand{\dboxed}[1]{\dbox{\ensuremath{#1}}}
\newcommand{\mathscriptsize}[1]{\text{\scriptsize\(#1\)}}

\begin{document}

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
% Solving Nonlinear Equation (1-D)
\section{Solving Nonlinear Equations [by Root Finding \(y=0\)]}

% Root multiplicity
\underline{Root Multiplicity, \(m\)}: \ \(0 = f(\bar{x}) = f'(\bar{x}) = ... = f^{(m-1)}(\bar{x}) 
    \hspace{15pt} \text{\scriptsize(Simple Root: \(m=1\))}\) 

% K-th iteration error
\vspace{10pt}
\underline{\(k\)-th Iteration Error}: \ \(\boxed{ e_k = x_k - \bar{x} }\)
% Convergence rate
\hfill
\underline{Convergence Rate, \(r\)}: \ \(\displaystyle 
    \boxed{ \lim_{k \rightarrow \infty} \frac{ \Vert e_{k+1} \Vert }{ \Vert e_k \Vert^r } = C }
    \hspace{15pt} \text{\scriptsize(\(0 < C < 1\) if \(r = 1\))} \)

%-------------------------------------------------------------
%-------------------------------------------------------------
% 1-Dimension
\vspace{5pt}
\subsection{One Dimension/Equation \hspace{15pt} {\scriptsize skipped a lot}}

% Interval Bisection
\vspace{10pt}
\underline{Iterval Bisection (Finding \(y=0\))}: \ \(
    \boxed{ [f(a) < 0] \ ,\  [f(b) > 0] \ , \ [f \text{ is cont.}] 
    \ \Rightarrow\ \exists m \text{ s.t. } f(m) = 0 }
\)

% Fixed-Point Iteration
\vspace{10pt}
\underline{Fixed-Point Iteration (Finding \(y=x\))}: \ \(\boxed{ \text{cont. } f(x) = 0 
    \ \Rightarrow\ \text{Find } g(x) = x } \rightarrow \boxed{ x_{k+1} = g(x_k) }\)

% ~ Banach FP Theorem conditions
\vspace{2pt}
\hspace{5pt} \(\begin{aligned}
    &\sim \text{Banach-Fixed Point Theorem (there are many FP theorems)}\\[5pt]
    &\bullet\ \text{\(g\) is Contractive (over a domain): \ \ dist}(g(x), g(y)) \leq q \cdot \text{dist}(x,y)
        \hspace{15pt} \text{\scriptsize\(q \in [0,1)\)}\\[5pt]
    &\bullet\ e_{k+1} = [x_{k+1} - \bar{x}] = [g(x_k) - g(\bar{x})] = g'(\xi_k) (x_k - \bar{x}) = g'(\xi_k) e_k \\[5pt]
    &\bullet\ \forall \vert g'(\xi_k) \vert < G < 1 
        \ \Rightarrow\ \Big( \vert e_{k+1} \vert \leq G \vert e_k \vert \leq ... \leq G^k \vert e_0 \vert \Big)
        \ \Rightarrow\ \lim_{k\rightarrow\infty} e_{k} = 0
        \hspace{15pt} \text{\scriptsize(\(G = \max g'\) over domain)} \\[5pt]
    &\bullet\ \lim_{k\rightarrow\infty} \vert g'(\xi_k) \vert = \boxed{ 
        \begin{gathered}[t]
            \Big( 0 < \vert g'(\bar{x}) \vert < 1 \Big) \\[-3pt]
            \text{\scriptsize(one contractive condition)}
        \end{gathered}
         = C } \hspace{15pt} \text{\scriptsize(\(r=1\))}\\[5pt]
    &\bullet\ \boxed{g'(\bar{x}) = 0} \ \Rightarrow\ \big[ g(x_k) - g(\bar{x}) \big] = \tfrac{g''(\xi_k)}{2}(x_k - \bar{x})^2 
        \ \Rightarrow\ \boxed{ \left\vert \tfrac{g''(\bar{x})}{2} \right\vert = C } 
        \hspace{15pt} \text{\scriptsize(\(r=2\) if \(\bar{x}\) is an \(m=2\) root of g)}
\end{aligned}\)

% Newton's Method
\vspace{10pt}
\underline{Newton's Method (Finding \(y=0\))}:

\hspace{5pt} \(\begin{aligned}
    &f(\bar{x}) = 0 = f(x_k + h_k) \ \approx\ f(x_k) + f'(x_k) h_k 
        \ \Rightarrow\ \boxed{ x_{k+1} = x_k + h_k = x_k - \tfrac{f(x_k)}{f'(x_k)} }\\[5pt]
    &\bullet\ \boxed{ g(x) \equiv x - \tfrac{f(x)}{f'(x)}}\ \Rightarrow \ 
        g(\bar{x}) = \bar{x} \ , \ \boxed{ g'(\bar{x}) = \tfrac{f(\bar{x}) f''(\bar{x})}{f'(\bar{x})^2} = 0 } \ , \ \boxed{ r = 2 } 
        \hspace{15pt} \text{\scriptsize(if \(\bar{x}\) is a simple root of \(f\))}\\[5pt]
    &\bullet\ \bar{x} \text{ is an } m > 1 \text{ root of } f \ \Rightarrow \ \boxed{ r = 1 \ , \ C = 1 - 1/m }
        \hspace{15pt} \text{\scriptsize(proof not given)}
\end{aligned}\)

% Secant Method
\vspace{10pt}
\underline{Secant Method/Linear Interpolation (Finding \(y=0\))}:

\hspace{5pt} \(\begin{aligned}
    &f'(x_k) \ \approx\ \tfrac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}} 
        \hspace{15pt} \parbox{3cm}{\scriptsize Approx. \(f'(x_k)\) with a secant line's slope} 
        \hspace{5pt} \ \Rightarrow \ 
        \boxed{ x_{k+1} = x_k + h_k = x_k - \tfrac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} f(x_k) }
        \\[5pt]
    &\bullet\ \boxed{ r = r_+ \approx 1.618 }: \ r_+^2 - r_+ - 1 = 0 \hspace{15pt} \text{\scriptsize(proof hard)}\\[5pt]
    &\bullet\ \text{\scriptsize Lower cost of iter. offsets the larger number of iter. compared to Newton's Method with derivatives}
\end{aligned}\)

%-------------------------------------------------------------------------------------------------------------------------------------
%
%
%
\newpage
% Inverse Parabolic Interpolation
\underline{Inverse Parabolic Interpolation}: \ Use 3 pts to approx. an inverse [sideways] parabola

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% m-dimension/equations
\vspace{15pt}
\subsection{\(m\) Dimensions/System of Equations \hspace{15pt} {\scriptsize stuff skipped}}

% Newton's Method
\vspace{10pt}
\underline{Newton's Method (Solving \(\vec{y}=0\))}:

\hspace{5pt} \(\begin{aligned}
    &\boxed{ \left\{ J_f(\vec{x}) \right\}_{ij} = \tfrac{\partial f_i(\vec{x})}{\partial x_j} }: \ \
        \boxed{ J_f(\vec{x}_k) \vec{h}_k = - \vec{f}(x_k) }
        \ \Rightarrow\ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k = \vec{x}_k - J_f(\vec{x}_k)^{-1} \vec{f}(\vec{x}_k) }
        \\[5pt]
    &\bullet\ \boxed{ \vec{g}(\vec{x}) \equiv \vec{x} - J_f(\vec{x})^{-1} \vec{f}(\vec{x}) }\ \Rightarrow \ 
        \begin{aligned}
            J_g(\bar{x}) &= \begin{gathered}[t]
                    \cancel{ I - J_f(\bar{x})^{-1} J_f(\bar{x}) }\\[-3pt]
                    \text{\scriptsize(if \(J_f(\bar{x})\) is nonsingular)}
                \end{gathered} 
                + \sum_{i=1}^n H_i(\bar{x}) f_i(\bar{x})
                \hspace{20pt} \parbox[t]{2.2cm}{\scriptsize \vspace{-10pt} 
                \(H_i =\) component matrix of the \\ tensor, \(D_x J_f(\bar{x})\)}
                \\[5pt]
            &= \mathcal{O} \ \Rightarrow \ \boxed{ r = 2 }
                \hspace{15pt} \text{\scriptsize(uh... idk)}
        \end{aligned}
        \\
    &\bullet\ \text{\scriptsize \(LU\) fact. of the Jacobian costs \(\mathcal{O}(n^3)\)}
\end{aligned}\)

% Secant Method
\vspace{10pt}
\underline{Broyden's [Secant Updating] Method (Solving \(\vec{y}=0\))}:

\hspace{5pt} \(\begin{aligned}
    &\boxed{ B_k \vec{h}_k = - \vec{f}(x_k) }
        \ \Rightarrow \ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
        \ , \ \boxed{ B_{k+1} = B_k + \tfrac{f(x_{k+1}) h_k^T}{h_k^T h_k} }
        \hspace{15pt} \text{\scriptsize(cost is \(\mathcal{O}(n^3)\))}
        \\[5pt]
    &\bullet\ B_{k+1} (\vec{x}_{k+1} - \vec{x}_k) \ =\ B_{k+1} \vec{h}_k \ =\ f(\vec{x}_{k+1}) - f(\vec{x}_k)\\[5pt]
    &\bullet\ B_{k} \text{\scriptsize \ factorization is updated to factorization of } 
        B_{k+1} \text{\scriptsize \ at cost } \mathcal{O}(n^2) \text{\scriptsize \ instead of directly from the above eq.}\\[5pt]
    &\bullet\ \text{\scriptsize Lower cost of iter. offsets the larger number of iter. compared to Newton's Method with derivatives}
\end{aligned}\)

%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
%------------------------------------------------------------------
% Optimization/Minimum Finding
\vspace{10pt}
\section{Optimizing [By Finding \(\min f(\vec{x}) = f(\bar{x})\)]}

\subsection{Function Shape and Convexity}

\vspace{5pt}
% Coercivity
\parbox{.37\textwidth}{
    \underline{Coercive}: \ \(\boxed{ \lim_{x \rightarrow \pm \infty} f(x) = \infty }\)
}
% Unimodal
\parbox{.62\textwidth}{
    \underline{Unimodal}: \ \(
        \begin{gathered}
            a \leq \bar{x} \leq b \\[-5pt]
            x_1 < x_2    
        \end{gathered}
        \ : \ 
        \arraycolsep=2pt \boxed{ \begin{array}{c c l}
            x_2 < \bar{x} &\ \rightarrow\ & f(x_1) > f(x_2)\\ 
            \bar{x} < x_1 &\ \rightarrow\ & f(x_1) < f(x_2)
        \end{array} }
    \)
}

% Global Minimum
\vspace{5pt}
\(\begin{aligned}
    &\underline{ \exists \text{ global } \min f \text{ if} }\\[3pt]
    &\hspace{10pt} \begin{aligned}
            &\bullet\ \text{\scriptsize cont. \(f\) on a closed and bounded set}\\
            &\bullet\ \text{\scriptsize cont. \(f\) is coercive on a closed, unbounded set}\\
            &\bullet\ \text{\scriptsize cont. \(f\) on a set and has a nonempty, closed, and bounded sublevel set}\\
            &\bullet\ \text{\scriptsize domain set is unbounded: cont. \(f\) is 
                coercive \(\Leftrightarrow\) all sublevel sets are bounded}
        \end{aligned}
\end{aligned}\)

% Convexity
\vspace{8pt}
\(\begin{aligned}
    &\underline{ \text{\(f\) is convex [on a convex set]} }: \\[3pt]
    &\hspace{10pt} \begin{aligned}
            &\bullet\ \text{\scriptsize any sublevel set is convex}\\
            &\bullet\ \text{\scriptsize any local min. is a global min}
        \end{aligned}
\end{aligned}\)
\hfill
\(\begin{aligned}
    &\underline{ \text{\(f\) is strictly convex [on a convex set]} }:\\[3pt]
    &\hspace{10pt} \begin{aligned}
            &\bullet\ \text{\scriptsize any local min. is a unique global min.}\\
            &\bullet\ \text{\scriptsize if set is unbounded: \(f\) is 
                coercive \(\Leftrightarrow\) \(f\) has a unique global min.} 
        \end{aligned}
\end{aligned}\)

%-----------------------------------------------------------------------------------------------------------------------------------
%
%
%-----------------------------------------------------------------------------------------------------------------------------------
\newpage

% Derivative Tests of Un/constrained Optimization
\subsection{Derivative Tests (Gradient, Jacobian, Hessian) and Lagrangians}

% Function requirements
\vspace{5pt}
\underline{Req.} : \ \( \boxed{ 
    \text{\scriptsize cont. } f(\bar{x}) = \min f 
    \ ,\ \text{\scriptsize cont. } \vec{\nabla} f(\bar{x})
    \ ,\ \text{\scriptsize cont. }  H_f(\bar{x})
} \)

% Taylor's Theorem
\vspace{5pt}
\underline{Taylor's Theorem}: \( \boxed{
    \arraycolsep=2pt \begin{array}{c c c c l c}
        f{\scriptstyle(\bar{x} + \vec{s}\hspace{1pt})} - f{\scriptstyle(\bar{x})} &=
            &\vec{\nabla} \begin{gathered}[b]
                    {\scriptstyle \alpha_1 \hspace{1pt} \in\hspace{1pt} (0,1)}
                        \\[-5pt]
                    f{\scriptstyle(\bar{x} + \alpha_1 \vec{s}\hspace{1pt})}
                \end{gathered} \cdot \vec{s}
            &=
            &\vec{\nabla} f{\scriptstyle(\bar{x})} \cdot \vec{s} 
                + \tfrac{1}{2} 
                \langle \vec{s}\hspace{1pt} | 
                \begin{gathered}[b]
                    {\scriptstyle \alpha_2 \hspace{1pt}\in\hspace{1pt} (0,1)}
                        \\[-5pt]
                    H_f{\scriptstyle(\bar{x} + \alpha_2 \vec{s} \hspace{1pt})}
                \end{gathered} 
                | \vec{s} \hspace{1pt} \rangle
            &\geq\ 0
            \\[5pt]
        f{\scriptstyle(\vec{x} + s \hat{u})} - f{\scriptstyle(\vec{x})} &=
            &\vec{\nabla} f{\scriptstyle(\vec{x} + \alpha_1 s \hat{u})} \cdot s \hat{u}
            &=
            &\vec{\nabla} f{\scriptstyle(\vec{x})} \cdot \vec{s} 
                + \tfrac{s^2}{2} 
                \langle \hat{u} | 
                H_f{\scriptstyle(\vec{x} + \alpha_2 \vec{s} \hspace{1pt})}
                | \hat{u} \rangle
    \end{array}
}\)

% Taylor Results
\hspace{5pt} \(\begin{aligned}[t]
    &\bullet 
        \ \lim_{s \rightarrow 0} 
        \left( 
            \tfrac{ f{\scriptstyle(\vec{x} + \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} }{ s }
            = \vec{\nabla} f{\scriptstyle(\vec{x} + \alpha_1 s \hat{u})} \cdot \cancel{s} \hat{u} 
        \right) 
        \Rightarrow \left( 
            {\scriptstyle \vec{\nabla} f{\scriptstyle(\bar{x})} \cdot \hat{u} \ \geq\ 0}
            \rightarrow 
            \boxed{ {\scriptstyle \vec{\nabla} f{\scriptstyle(\bar{x})} \cdot \vec{s} \ \geq\ 0} } 
        \right)
        \hspace{3pt} , \hspace{5pt} 
        \boxed{ \parbox{3.7cm}{
            \scriptsize Cauchy-Schwarz \(\rightarrow\) \\[3pt]
            \( \max \vec{\nabla} f{\scriptstyle(\vec{x})} \cdot \hat{u}\)
            if \(\vec{u} = \vec{\nabla} f(\vec{x})\)
        } }
        \\[5pt]
    &\bullet
        \begin{aligned}[t]
            &\boxed{ \vec{u} = \mp \vec{\nabla} f{\scriptstyle(\vec{x})} }
                \Rightarrow \ 
                \lim_{s \rightarrow 0} 
                \left( 
                    \tfrac{ f{\scriptstyle(\vec{x} + \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} }{ s }
                    = \mp \cancel{s} \tfrac{
                        \vec{\nabla} f{\scriptstyle(\vec{x} + \alpha_1 s \hat{u})}
                        \cdot 
                        \vec{\nabla} f{\scriptstyle(\vec{x})} 
                    }{
                        \Vert \vec{\nabla} f{\scriptstyle(\vec{x})} \Vert
                    }
                \right)
                = \mp \Vert \vec{\nabla} f{\scriptstyle(\vec{x})} \Vert 
                \ \tfrac{<}{>}\ 0
                \hspace{15pt}
                \boxed{ 
                    \parbox{2.9cm}{
                        \scriptsize if \(\pm \vec{\nabla} f(\vec{x}) \neq 0\),
                        its dir. is an ascent/descent.
                    }
                }
        \end{aligned}
        \\[5pt]
    &\bullet 
        \ \lim_{s \rightarrow 0} 
        \left(
            \tfrac{ 
                f{\scriptstyle(\vec{x} + \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} 
                + f{\scriptstyle(\vec{x} - \vec{s}\hspace{1pt})} -  f{\scriptstyle(\vec{x})} 
            }{ s^2 }
            =
            \tfrac{\scriptstyle 
                \langle \hat{u} | 
                H_f(\vec{x} + \alpha_2 \vec{s} \hspace{1pt}) + H_f(\vec{x} - \alpha_3 \vec{s} \hspace{1pt}) 
                | \hat{u} \rangle
            }{ 2 }
        \right)
        = {\scriptstyle \langle \hat{u} | H_f(\vec{x}) | \hat{u} \rangle}
        \ \Rightarrow \
        \boxed{ {\scriptstyle \langle \vec{s}\hspace{1pt} | H_f(\bar{x}) | \vec{s}\hspace{1pt} \rangle \ \geq\ 0} } 
\end{aligned}\)

% Unconstrained Optimization Conditions
\vspace{15pt}
\subsubsection{Unconstrained Optimization Conditions}

% Taylor's Theorem
\vspace{5pt}
\(
    \bullet\ \begin{aligned}
        &\boxed{ f(\bar{x}) = \min f }
            \ \Leftrightarrow \ 
            \text{\scriptsize\(
                \left( 
                    \hspace{5pt}
                    \begin{gathered}
                        \vec{\nabla} f (\bar{x}) \cdot \vec{s} \geq 0
                            \ , \ \vec{\nabla} f (\bar{x}) \cdot - \vec{s} \geq 0\\
                        \ \Rightarrow\ \boxed{ \vec{\nabla} f (\bar{x}) = 0 }
                    \end{gathered}
                    \hspace{10pt} , \hspace{10pt}
                    \begin{gathered}
                        \vec{u} = - \vec{\nabla} f(\bar{x})\\
                        \ \Rightarrow\ \boxed{ \vec{\nabla} f (\bar{x}) = 0 }
                    \end{gathered}
                    \hspace{10pt} , \hspace{10pt}
                    \boxed{ 
                        \begin{gathered}
                            \text{\scriptsize(for strict convexity)}\\
                            \langle \vec{s}\hspace{1pt} | H_f(\bar{x}) | \vec{s}\hspace{1pt} \rangle > 0
                        \end{gathered} 
                    }
                    \hspace{5pt}
                \right)
            \)}
    \end{aligned}
\)

% Optimization
\vspace{10pt}
\(
    \begin{aligned}[t]
        &\text{\underline{Optimization}} 
            \hspace{10pt} 
            f: \ \ \mathbb{R}^n \rightarrow \mathbb{R}
            \hspace{20pt}
            \boxed{ \min f(\vec{x}) = y} 
            \\[10pt]
        % Lagrangian and Hessian
        &\hspace{10pt} 
            \boxed{ \mathcal{L}(\vec{x}) = f(\vec{x}) }
            \hspace{10pt} , \hspace{12pt}
            \boxed{ \nabla \mathcal{L}(\bar{x}) = 0 }
            \hspace{10pt} , \hspace{12pt}
            \boxed{ 
                H_\mathcal{L} = \nabla_{xx} \mathcal{L}: \ \
                \langle s | H_\mathcal{L}(\bar{x}) | s \rangle > 0 
            }
            \ \Rightarrow \ \boxed{ y = f(\bar{x}) }
    \end{aligned}
\)

% Constrained Optimization Conditions
\vspace{15pt}
\subsubsection{Constrained Optimization Conditions}

% Taylor's Theorem
\vspace{5pt}
\(
    \bullet\ 
    \boxed{
        \begin{gathered}
            \vec{s} = \text{\scriptsize feasable direction} \\[-3pt]
            {\scriptstyle f(\bar{x}) = \min f \text{ \ given \ } g,\ h}
        \end{gathered}
    }
    \ \Leftrightarrow \ 
    \left(\hspace{5pt}
        \boxed{ \vec{\nabla} f(\bar{x}) \cdot \vec{s} \ \geq\ 0 }
        \hspace{5pt} , \hspace{5pt} 
        \boxed{ \langle \vec{s} \hspace{1pt} | H_f(\bar{x}) | \vec{s} \hspace{1pt} \rangle \ \geq\ 0 }
    \hspace{5pt}\right)
\)

% Optimization
\vspace{10pt}
\(
    \text{\underline{Optimization}}
    \hspace{10pt}
    \text{\scriptsize\(
        \begin{aligned}
            f:& \ \ \mathbb{R}^n \rightarrow \mathbb{R}\\
            g:& \ \ \mathbb{R}^n \rightarrow \mathbb{R}^{m}\\
            h:& \ \ \mathbb{R}^n \rightarrow \mathbb{R}^p\\[5pt]
        \end{aligned}
    \)}
    \hspace{20pt}
    \boxed{ 
        \min f(\vec{x}) = y 
        \ \ \ \text{\scriptsize w/} \ \ \
        \left(\begin{matrix}
            \vec{g} \text{\scriptsize\((\vec{x})\)} = 0\\
            \vec{h} \text{\scriptsize\((\vec{x})\)} \leq 0
        \end{matrix}\right) 
    }
    \hspace{20pt}
    {\scriptstyle
        \begin{aligned}
            \text{\underline{active}} :     &\ \ h_i(\bar{x}) = 0
                \\
            \text{\underline{inactive}} :   &\ \ h_i(\bar{x}) < 0
                \ \rightarrow\ 
                \begin{gathered}[b]
                    \text{\scriptsize(see KKT)}\\[-3pt]
                    \bar{\mu}_i = 0
                \end{gathered}
                \\
        \end{aligned}
    }
\)

\hspace{10pt} \(
    % Lagrangian
    \begin{aligned}
        &\boxed{ 
                \begin{aligned}
                    &\mathcal{L} {\scriptstyle(\vec{x}, \vec{\lambda}, \vec{\mu})}
                        = f(\vec{x}) + \vec{\lambda} \cdot \vec{g}(\vec{x})  + \vec{\mu} \cdot \vec{h}(\vec{x})  \\
                    &\hspace{10pt} = f + \text{\scriptsize\(\sum_i^m\)}\ \lambda_i g_i
                        + \cancel{ \text{\scriptsize\(\sum_i^p\)}\ \mu_i h_i }
                        \hspace{10pt} \begin{gathered}
                            \text{\scriptsize (KKT) if} \\[-9pt]
                            {\scriptstyle \vec{x} \ =\ \bar{x}}
                        \end{gathered}
                \end{aligned}
            }
            \hspace{7pt} , \hspace{7pt}
            % Gradient
            \boxed{
                \begin{aligned}
                    \nabla \mathcal{L} {\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})}
                    &= \text{\scriptsize\(
                            \left( \begin{matrix}
                                \nabla_x \mathcal{L} = 0\\
                                \nabla_\lambda \mathcal{L} = 0\\
                                \nabla_\mu \mathcal{L} \leq 0
                            \end{matrix} \right)
                        \)}
                        = \text{\scriptsize\(
                            \left( \begin{matrix}
                                \nabla f \text{\scriptsize\((\bar{x})\)} 
                                    + J_g^T \text{\scriptsize\((\bar{x})\)} \bar{\lambda} 
                                    + J_h^T \text{\scriptsize\((\bar{x})\)} \bar{\mu}\\
                                \vec{g} \text{\scriptsize\((\bar{x})\)}\\
                                \vec{h} \text{\scriptsize\((\bar{x})\)}
                            \end{matrix} \right)
                        \)}
                \end{aligned}
            } \\[5pt]
        % Hessian
        &H_\mathcal{L} {\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})} = 
            \text{\scriptsize\(\left( \begin{matrix}
                \nabla_{xx} \mathcal{L} & \nabla_{x\lambda} \mathcal{L} & \nabla_{x\mu} \mathcal{L}\\
                \nabla_{\lambda x} \mathcal{L} & \nabla_{\lambda\lambda} \mathcal{L} & \nabla_{\lambda\mu} \mathcal{L}\\
                \nabla_{\mu x} \mathcal{L} & \nabla_{\mu\lambda} \mathcal{L} & \nabla_{\mu\mu} \mathcal{L}  
            \end{matrix}\right)\)}
            =
            \begin{gathered}[t]
                \text{\scriptsize\(
                        \left( \begin{matrix}
                            \nabla_{xx} \mathcal{L} & J_g^T & J_{h}^T\\
                            J_g & 0 & 0\\
                            J_{h} & 0 & 0 
                        \end{matrix}\right)
                    \)}
                    \\
                \text{\scriptsize(can't be pos. def.)}
            \end{gathered}
            \hspace{3pt} , \hspace{10pt}
            \boxed{
                \nabla_{xx} \mathcal{L} {\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})}
                = H_f + \text{\scriptsize\(\sum_i^m\)}\ \bar{\lambda}_i H_{g_i}
                + \text{\scriptsize\(\sum_i^{\text{act} \leq p}\)}\ \bar{\mu}_i H_{h_i}
            }
    \end{aligned}
\)

%---------------------------------------------------------------------------------------------------------------------------------
%
%
%
\newpage
\hspace{10pt}\(
    % Cases for Solution
    \begin{aligned}
        &\bullet\ \underline{ \text{\scriptsize Assume \(m\leq n\) (not overdetermined)} }\\
        &\bullet\ y = f(\bar{x}) : \ \nabla \mathcal{L}{\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})} \ ...
            \ , \ 
            \boxed{ p = 0 :\ \ Z^T (\nabla_{xx} \mathcal{L}) Z > 0 
            \hspace{15pt} {\scriptstyle \text{col. of } Z \ =\ \text{basis of null}(J_g) } }
            \\[5pt]
        &\bullet\ \underline{ \text{\scriptsize Assume \(h_i\) don't contradict each other?} }
            \hspace{15pt}
            \underline{ \text{\scriptsize Assume full rank\((J_{h_\text{act}})\)} }\\
        &\bullet\ y = f(\bar{x}) : \ \nabla \mathcal{L}{\scriptstyle(\bar{x}, \bar{\lambda}, \bar{\mu})} \ ...
            \ , \ 
            \boxed{ p > 0 ,\ \text{\scriptsize Karush-Kuhn-Tucker (KKT)}:\ \ 
            \bar{\mu}_i \geq 0 ,\ \bar{\mu}_i h_i(\bar{x}) = 0 }
            \hspace{10pt} \begin{gathered}
                \text{\scriptsize (2nd deriv. cond. }\\[-9pt]
                \text{\scriptsize not given)}
            \end{gathered}\\
    \end{aligned}    
\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% One Dimension Minimum
\vspace{5pt}
\subsection{Unconstrained One Dimension/Independent Variable}

% Interval Golden Search
\underline{[Interval] Golden-Section Search (if Unimodal)}: 
\ \(\boxed{ \tau^2 = 1 - \tau = .382 }\ ,\ \boxed{r = 1}\ ,\ \boxed{C = \tau}\)

\vspace{5pt}
\hspace{10pt}\(
    [a < x_1 < x_2 < b]: \ 
    \left\{ \ \arraycolsep=0pt \begin{array}{r c c c c c c c c}
        f(x_1) > f(x_2)     &\ \ \rightarrow\ \ & \big[ & x_1  &\ <\ & x_2 \ <\ x_1 + \tau (b-x_1)    &\ <\ & b   & \big] \\[5pt]
        f(x_1) \leq f(x_2)  &\ \ \rightarrow\ \ & \big[ & a    &\ <\ \ & a + (1-\tau)(x_2 - a) \ <\ x_1 &\ \ <\ \ & x_2 & \big]
    \end{array}\right.
\)

% Newton's Method
\vspace{15pt}
\underline{Newton's Method}: \ \(
    f(\bar{x}) = f(x+h) 
    \ \approx\ f(x) + f'(x) h + \tfrac{1}{2} f''(x) h^2 
    = g(h)
\) 

\vspace{5pt}
\hspace{10pt}\(
    g(\tfrac{-b}{2a}) = \min g \ \text{\scriptsize(or max)}
    \ \Rightarrow\ \boxed{ x_{k+1} = x_k + h_k = x_k - \tfrac{b}{2a} = x_k - \tfrac{f'(x)}{f''(x)} } \ ,\ \boxed{r=2}
\)

% Linear Interpolation
\vspace{15pt}
\underline{Sucessive Linear Interpolation [Secant Method]}: \ {\scriptsize Not useful, since lines have no unique minimun}

% Sucessive Parabolic Interpolation
\vspace{5pt}
\underline{Sucessive Parabolic Interpolation}: \ {
    \scriptsize Use 3 pts to approx. a parabola w/ \(\boxed{r = 1.324}\) \ (not guarenteed)
}

%---------------------------------------------------------------
%
%
%
% Unconstrained m-dimensions
\vspace{5pt}
\subsection{Unconstrained \(m\)-Dimensions/Independent Variables}

% Steepest Descent
\vspace{5pt}
\underline{Steepest [Gradient] Descent/Line Search (go down \( -\nabla f{\scriptstyle(\vec{x}_k)} \))}: 

\hspace{10pt} \(\begin{aligned}
    % Descent Algorithm
    &\boxed{ \phi(\alpha) = f \big( \vec{x} - \alpha \vec{\nabla} f{\scriptstyle(\vec{x})} \big) }
        \ , \
        \boxed{ \phi(\alpha_k) = \min \phi }
        \ \Rightarrow \ 
        \boxed{ \vec{x}_{k+1} = \vec{x}_k - \alpha_k \vec{\nabla} f(\vec{x}_k) } 
        \hspace{20pt}
        \boxed{r=1 \ ,\ C \text{\scriptsize varies} }
        \\[10pt]
    % Notes
    &\bullet
        \ \vec{\nabla} f{ \scriptstyle ( \vec{x}_k ) } \cdot \vec{\nabla} f{ \scriptstyle ( \vec{x}_{k+1} ) } = 0
        \ \Rightarrow \ 
        \text{\scriptsize Path will zig-zag to the min. (not too efficient)}
\end{aligned}\)

% Newton's Method
\vspace{5pt}
\underline{Newton's Method}: \ \(
    f(\bar{x}) = f(\vec{x} + \vec{h})
    \ \approx \ f(\vec{x})
    + \vec{\nabla} f(\vec{x}) \cdot \vec{h} 
    + \tfrac{1}{2} \langle \vec{h} | H_f(\vec{x}) | \vec{h} \rangle
\)

\hspace{10pt}\(\begin{aligned}
    &\boxed{ H_f(\vec{x}_k) \hspace{1pt} \vec{h}_k = - \vec{\nabla} f(\vec{x}_k) } 
        \ \Rightarrow \ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
        \hspace{10pt}, \hspace{10pt} \boxed{ r=2 }
\end{aligned}\)

% Secant Updating
\vspace{10pt}
\underline{BFGS [Secant Updating] Method}: \ \(
    \boxed{ B_k \vec{h}_k = - \vec{\nabla} f(\vec{x}_k) } \ ,\ \boxed{ \vec{y}_k = \vec{\nabla} f(x_{k+1}) - \vec{\nabla} f(x_{k}) }     
\)

\hspace{5pt} \(\begin{aligned}
    &\Rightarrow \ \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
        \ , \ \boxed{ 
            B_{k+1} = B_k 
            + \tfrac{| y_k \rangle \langle y_k |}{\langle y_k | h_k \rangle} 
            - \tfrac{B_k | h_k \rangle \langle h_k | B_k}{\langle h_k | B_k | H_k \rangle}
        }
        \hspace{15pt} \text{\scriptsize(cost is \(\mathcal{O}(n^3)\))}
        \\
    &\bullet\ \text{\scriptsize Preserves symmetry and pos. def.}\\
    &\bullet\ B_{k} \text{\scriptsize \ factorization is updated to factorization of } 
        B_{k+1} \text{\scriptsize \ at cost } \mathcal{O}(n^2) \text{\scriptsize \ instead of directly from the above eq.}
        \\
    &\bullet\ \text{\scriptsize Lower cost of iter. offsets the larger number of iter. compared to Newton's Method with derivatives}
\end{aligned}\)

%-------------------------------------------------------------------------------------------------------------------------------------
%
%
%
\newpage
% Conjugate Gradient 
\underline{Conjugate Gradient [Line Search]} : \ 

\vspace{5pt}
\hspace{10pt}\(
    \boxed{ 
        \vec{h}_{k+1} = \vec{\nabla} f(\vec{x}_{k+1}) 
        - \tfrac{ 
            \vec{\nabla} f(\vec{x}_{k+1}) \cdot \vec{\nabla} f(\vec{x}_{k+1})
        }{
            \vec{\nabla} f(\vec{x}_{k}) \cdot \vec{\nabla} f(\vec{x}_k)
        } 
        \ \vec{h}_{k} 
    } 
    \hspace{10pt}
    \text{\scriptsize(Fletcher and Reeves)}
    \ \Rightarrow\ \boxed{ \vec{x}_{k+1} = \vec{x}_k - \alpha_k \vec{h}_k }
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \text{\scriptsize Seq. of conj. (where \((a,b) = \langle a | H_f | b \rangle \)) search directions
        implicitly accumulates info. about \(H_f\).}
        \\
    &\bullet\ \text{\scriptsize Better for nonlin. to use } \
        \boxed{ 
            \vec{h}_{k+1} = \vec{\nabla} f(\vec{x}_{k+1}) 
            - \tfrac{ 
                \vec{\nabla} f(\vec{x}_{k+1}) \cdot \vec{\nabla} f(\vec{x}_{k+1}) 
                - \vec{\nabla} f(\vec{x}_{k}) \cdot \vec{\nabla} f(\vec{x}_{k+1})
            }{
                \vec{\nabla} f(\vec{x}_{k}) \cdot \vec{\nabla} f(\vec{x}_k)
            } 
            \ \vec{h}_{k} 
        }
        \hspace{10pt} \text{\scriptsize(Polak and Ribiere)}
        \\
    &\bullet\ \text{\scriptsize Restart algorithm after \(n\) iter. using last point as the new initial; 
        a quadratic func. finishes after at most \(n\) iter.}
\end{aligned}\)

%--------------------------------------------------------------
%--------------------------------------------------------------
% Nonlinear Least Squares
\subsubsection{Nonlinear Least Squares, \(\big\{ \min \Vert \vec{r}(\vec{x}) \Vert^2 : \
    \vec{f}{\scriptstyle(\vec{a}, \vec{x})} + \vec{r}{\scriptstyle(\vec{x})} = \vec{b} \hspace{1pt} \big\}    
\)}

% Diff. between Linear and Nonlinear Least Squares
\vspace{10pt}
\[
    \arraycolsep=2pt \begin{array}{c c c}
        \text{Linear Least Squares} 
            &
            &\text{Nonlinear Least Squares}
            \\[5pt] 
        \left(\begin{matrix}
                \vdots\\
                - \vec{a}_i -\\
                \vdots
            \end{matrix}\right)
            \left(\begin{matrix}
                |\\
                \vec{x}\\
                |
            \end{matrix}\right)
            +
            \left(\begin{matrix}
                |\\
                \vec{r}\\
                |
            \end{matrix}\right) 
            =
            \left(\begin{matrix}
                |\\
                \vec{b}\\
                |
            \end{matrix}\right) 
            &\ \ \ \Rightarrow \ \ \
            &\left(\begin{matrix}
                    |\\
                    \vec{f}{\scriptstyle (\vec{a}, \vec{x}) }_i\\
                    |
                \end{matrix}\right)
                +
                \left(\begin{matrix}
                    |\\
                    \vec{r}\\
                    |
                \end{matrix}\right) 
                =
                \left(\begin{matrix}
                    |\\
                    \vec{b}\\
                    |
                \end{matrix}\right) 
    \end{array}
\]

% Normal Algorithm
\vspace{5pt}
\(
    \begin{gathered}
        \boxed{ \phi(\vec{x}) \ \equiv\ \tfrac{1}{2} \vec{r} \cdot \vec{r} \hspace{1pt} }
            \ , \
            \boxed{ - \vec{\nabla} \phi(\vec{x}) = - J_r^T \vec{r} }
            \\[5pt]
        \boxed{ H_\phi(\vec{x}) = J_r^T J_r + \sum_i H_{r_i} \vec{r}_i }
    \end{gathered}
    \ \ : \ \
    \begin{gathered}
        \text{\scriptsize Newton's Method}\\
        \boxed{ H_\phi(\vec{x}_k) \hspace{1pt} \vec{h}_k = - \vec{\nabla} \phi(\vec{x}_k) }\\[-3pt]
        \text{\scriptsize(usually expensive to compute)}
    \end{gathered}
    \ \Rightarrow \ 
    \boxed{ \vec{x}_{k+1} = \vec{x}_k + \vec{h}_k }
\)

% Gauss-Newton Method
\vspace{15pt}
\underline{Gauss-Newton Method}: \ \(
    \text{If \(\vec{r}\) is small} 
    \ \Rightarrow\ H_\phi \approx J_r^T J_r
    \ \Rightarrow\ \boxed{
        J_r^T ( J_r \vec{h}_k ) = - J_r^T \vec{r}{\scriptstyle(\vec{x}_k)}
        \hspace{20pt} \begin{gathered}
            \text{\scriptsize System of}\\[-5pt]
            \text{\scriptsize Normal Equations}
        \end{gathered} 
    }
\)

% Levenberg-Marquardt Method
\vspace{15pt}
\underline{Levenberg-Marquardt Method (Gauss-Newton + Line Search)}: 

\vspace{5pt}
\(
    \hspace{10pt}\begin{aligned}
        &\boxed{ 
                (J_r^T J_r + \mu_k I) \vec{h}_k = - J_r^T \vec{r}{\scriptstyle(\vec{x}_k)} 
                \ \Rightarrow \ 
                \vec{x}_{k+1} = \vec{x} + \vec{h_k}
            }
            \\[5pt]
        &\Rightarrow\ \boxed{
                \left(\begin{matrix}
                    J_r^T{\scriptstyle(\vec{x})} & \sqrt{ \mu_k } I
                \end{matrix}\right)
                \left(\begin{matrix}
                    J_r{\scriptstyle(\vec{x})} \\
                    \sqrt{ \mu_k } I
                \end{matrix}\right)
                \vec{h}_k
                = 
                \left(\begin{matrix}
                    J_r^T{\scriptstyle(\vec{x})} & \sqrt{ \mu_k } I
                \end{matrix}\right)
                \left(\begin{matrix}
                    - \vec{r}{\scriptstyle(\vec{x}_k)} \\
                    0
                \end{matrix}\right)
            }
    \end{aligned}
\)
\hfill
\(
    \begin{aligned}
        &\text{\underline{\scriptsize Regularization}}\\
        &\bullet\ \parbox[t]{4.4cm}{\scriptsize Replacing \(H_{r_i} \vec{r}_i\) terms with a \\ scalar mult. of \(I\).} \\
        &\bullet\ \parbox[t]{4.4cm}{\scriptsize Shifting the Gauss-Newton Hessian to make it pos. def (or boosting its rank).}\\
    \end{aligned}
\)

%-----------------------------------------------------------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------------------------------------------
% Constrained Optimization
\vspace{10pt}
\subsection{Constrained \(m\)-Dimensions/Independent Variables}

\(
    \begin{gathered}
        \text{\scriptsize Newton's Method}\\
        \boxed{ H_\mathcal{L} \hspace{1pt} \vec{h}_k = - \vec{\nabla} \mathcal{L} }
    \end{gathered}
    \ \ \vline \ \
    % KKT Matrix
    \begin{gathered}
        \underline{\text{\scriptsize KKT Matrix (Eq. Constr)}}\\
        \begin{aligned}
                \text{\scriptsize\(
                    \left( \begin{matrix}
                        \nabla_{xx} \mathcal{L} & J_g^T \\
                        J_g & 0
                    \end{matrix}\right)
                    \left( \begin{matrix}
                        \vec{s}_k\\
                        \ \vec{\delta}_k\ 
                    \end{matrix}\right)
                \)}
                &= - \text{\scriptsize\(
                        \left( \begin{matrix}
                            \nabla f \text{\scriptsize\((\bar{x})\)} 
                                + J_g^T \text{\scriptsize\((\bar{x})\)} \bar{\lambda}\\
                            \vec{g} \text{\scriptsize\((\bar{x})\)}
                        \end{matrix} \right)
                    \)}
                    \\[5pt]
                \Aboxed{
                    \text{\scriptsize\(
                        \left( \begin{matrix}
                            B & J^T \\
                            J & 0
                        \end{matrix}\right)
                        \left( \begin{matrix}
                            {s}\\
                            {\delta} 
                        \end{matrix}\right)
                    \)}
                    &= - \text{\scriptsize\(
                            \left( \begin{matrix}
                                {w}\\
                                {g}  
                            \end{matrix} \right)
                        \)}
                }
            \end{aligned}
    \end{gathered}
    \hspace{5pt} \Rightarrow \hspace{5pt}
    \begin{gathered}
        \text{\scriptsize \underline{[Sequential] Quadratic Programming (SQP) Problem}}\\[5pt]
        \min_s \Big( \vec{s}_k \cdot \vec{\nabla}_{x} \mathcal{L} 
            + \tfrac{1}{2} \langle \vec{s}_k | \vec{\nabla}_{xx} \mathcal{L} | \vec{s}_k \rangle \Big)
            \\[5pt]
        \text{s.t.} \ \ \ J_g{\scriptstyle(\vec{x}_k)} \hspace{1pt} \vec{s}_k + \vec{g}{\scriptstyle(\vec{x}_k)} = 0
    \end{gathered}
\)

%---------------------------------------------------------------------------------------------------------------------------------
%
%
%

% Direct Solution
% \vspace{10pt}
\underline{Direct Solution}: \ \parbox{.8\textwidth}{
    \scriptsize KKT Matrix is sym. and sparse 
    \(\rightarrow\) solve for \(\vec{h}_k\) using sym. indef. factorization w/ some pivoting
}

% Column/Range-Space Method
\vspace{10pt}
\underline{\(\begin{gathered}[b]
        \text{\scriptsize(Column-Space)}\\
        \text{Range-Space}
    \end{gathered}\) 
    Method}: \ 
\(
    \boxed{Bs = -w - J^T \delta}
    \hspace{15pt} , \hspace{15pt}
    \begin{aligned}
        Js = -g \ &\rightarrow \ JB^{-1}(- w - J^T \delta) = - g\\
        &\rightarrow\ \boxed{ (JB^{-1}J^T) \delta = g - JB^{-1} w }
    \end{aligned}
\)

% Notes
\vspace{5pt}
\hspace{10pt}\(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Solve for \(\delta\), then for \(s\).}\\
    &\bullet\ \text{\scriptsize \(B\) must be nonsingular and \(J\) full rank.}
\end{aligned}\)
\hspace{20pt}
\(\begin{aligned}[t]
    &\bullet\ \parbox[t]{.5\textwidth}{\scriptsize Forming \((JB^{-1}J^T)_{m \times m}\) leads to issues 
        similar to forming \(A^T A\) (loss of info. and degrades conditioning).}
        \\
    &\bullet\ \text{\scriptsize Useful if \(m\) is small.}
\end{aligned}\)

% Null-Space Method
\vspace{5pt}
\underline{Null-Space Method}: \ \(
    J^T = \text{\scriptsize\(
        \Big( Q_\parallel \ \ Q_\perp \Big) 
        \left(\begin{matrix}
            R\\ 
            0
        \end{matrix}\right)
    \)}
    \hspace{20pt}
    {\scriptstyle(Q_\parallel \hspace{1pt} \in\hspace{1pt} \mathbb{R}^{n\times m})}
    \hspace{10pt} \Rightarrow \hspace{10pt}
    \boxed{ \begin{aligned}
        J Q_\parallel &= R^T\\
        J Q_\perp &= 0
    \end{aligned} }
\)

% Solving for s and delta
\hspace{10pt}\(
    \arraycolsep=2pt \begin{array}{r c c c l}
        \text{Find } u_\parallel &:\ 
            &Js \ \equiv\ \Big( JQ_\parallel u_\parallel + \cancel{JQ_\perp} u_\perp \Big)
            &= \
            &\boxed{ R^T u_\parallel = -g }
            \\[10pt]
        \text{Find } u_\perp &:\
            &Q_\perp^T \big( Bs + J^T \delta= -w \big)
            &\rightarrow\ 
            &(Q_\perp^T B Q_\parallel) u_\parallel + (Q_\perp^T B Q_\perp) u_\perp = - Q_\perp^T w     
                - (\cancel{JQ_\perp})^T \delta
            \\[5pt]
        &
            &
            &
            &\boxed{
                    (Q_\perp^T B Q_\perp) u_\perp 
                    = - Q_\perp^T w - (Q_\perp^T B Q_\parallel) u_\parallel
                }
            \\[10pt]
        \text{Find } \delta &:\
            &Q_\parallel^T \big( J^T \delta = -w - Bs \big) 
            &\rightarrow\
            &\boxed{ R \delta
                = - Q_\parallel^T w - Q_\parallel^T B ( Q_\parallel u_\parallel - Q_\perp u_\perp ) }
    \end{array}
\)

% Notes
\vspace{5pt}
\hspace{10pt}\(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Near a min., \((Q_\perp^T B Q_\perp)\) can be Cholesky factored.}\\
    &\bullet\ \text{\scriptsize \(J\) must be full rank and \(R\) nonsingular.}
\end{aligned}\)
\hspace{20pt}
\(\begin{aligned}[t]
    &\bullet\ \text{\scriptsize Avoids issues with loss of info. and degraded conditioning.}\\
    &\bullet\ \text{\scriptsize Useful if \(m\) is large, so \(n-m\) is small.}
\end{aligned}\)

% Line Divider
\vspace{5pt}
\rule{1\textwidth}{.5pt}
\vspace{5pt}

% Decent Initial Guess for Lambda
\underline{Decent Initial \(\vec{\lambda}_0\) Guess Given an \(\vec{x}_0\)}: \ \(
    \boxed{    
        J_g^T{\scriptstyle(\vec{x}_0)} \hs \vec{\lambda}_0 + \vec{r} = - \vec{\nabla} f{\scriptstyle(\vec{x}_0)}
    }
    \hspace{15pt}
    \text{\scriptsize(Linear Least Sq.)}
\)

% Penalty Method
\vspace{5pt}
\( 
    \begin{gathered}
        \underline{\text{Penalty Func. Method}}\\[2pt]
        \boxed{ \lim_{\rho \rightarrow \infty} \vec{x}_\rho = \bar{x} } 
            \hspace{5pt} \text{\scriptsize (not explained)}\\
        (\text{\scriptsize ``Under approp. conds.''})
    \end{gathered}
    \ \vline \ \ \ 
    \boxed{
        \begin{array}{r l}
            % Simple Penalty Func.
            \begin{gathered}
                \text{\scriptsize One Simple Function}\\[-5pt]
                (\text{\scriptsize Ill-conditioned \(\rho \gg 1\)})
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \phi_\rho(\vec{x}) = f(\vec{x}) + \tfrac{1}{2} \rho \Vert g(\vec{x}) \Vert^2
                \\[15pt]
            % Augmented Lagrangian
            \begin{gathered}
                \text{\scriptsize Augmented Lagrangian}\\[-5pt]
                (\text{\scriptsize Less Ill-conditioned})
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \mathcal{L}_\rho(\vec{x}) = f(\vec{x}) + \vec{\lambda}_0 \cdot \vec{g}(\vec{x}) 
                + \tfrac{1}{2} \rho \Vert g(\vec{x}) \Vert^2
        \end{array}
    }
\)

% Barrier Method
\vspace{10pt}
\( 
    \begin{gathered}
        \underline{\text{Barrier Func. Method}}\\[2pt]
        \boxed{ \lim_{\rho \rightarrow 0} \vec{x}_\rho = \bar{x} }\\
        (\text{\scriptsize ``Under approp. conds.''})
    \end{gathered}
    \ \vline \ \ \ 
    \boxed{
        \begin{array}{r l}
            % Inverse
            \begin{gathered}
                \text{\scriptsize Inverse}
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \phi_\rho(\vec{x}) = f(\vec{x}) 
                - \rho \sum_i^p \frac{1}{ h_i{\scriptstyle(\vec{x})} }
                \\[15pt]
            % Logarithmic
            \begin{gathered}
                \text{\scriptsize Logarithmic}
            \end{gathered} :
                &\displaystyle \min_{\vec{x}} \phi_\rho(\vec{x}) = f(\vec{x}) 
                - \rho \sum_i^p \log{ ( -h_i{\scriptstyle(\vec{x})} ) }
        \end{array}
    }
    \hspace{10pt}
    \boxed{ \text{\scriptsize(For Ineq. Constr.)} }
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \parbox[t]{.95\textwidth}{\scriptsize Along with line search and trust region (not explained), 
        a merit func. - using perhaps a penalty func. - can be used to make an algorithm more robust.}
        \\
    &\bullet\ \text{\scriptsize An active set strategy (not explained) can be used with an SQP method 
        for ineq.-constr. problems.}
        \\
    &\bullet\ \text{\scriptsize A penalty method penalizes points that violates constraints, 
        but doesn't avoid them. Barrier methods do.}
\end{aligned}\)

%----------------------------------------------------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------------------------------------------------
% Polynomial Interpolation
\section{[Polynomial] Interpolation, \(f(t_i) = \hat{f}(t_i) = \sum_j x_j \hs \phi_j(t_i) \)}

\(
    \begin{aligned}
        \hat{f}(t_i) &= \sum_j x_j \hs \phi_j(t_i)\\
        &= \hsvec{\phi}\hs(t_i) \cdot \hsvec{x}
    \end{aligned}
    \hspace{10pt}
    \vline
    \hspace{10pt}
    \begin{gathered}
        \det(A) \neq 0\\[5pt]
        \text{Given } \hsvec{\phi},\\[-2pt]
        \text{solve for } \hsvec{x}
    \end{gathered}
    \hspace{10pt}
    \vline
    \hspace{10pt}
    \begin{aligned}
        &A\hsvec{x} 
            = \left(\begin{matrix}
                \vdots\\
                - \ \hsvec{\phi}{\scriptstyle(t_i)} \ -\\
                \vdots
            \end{matrix}\right)
            \left(\begin{matrix}
                |\\
                \hsvec{x}\\
                |
            \end{matrix}\right)
            = \hsvec{y} = \left(\begin{matrix}
                \vdots\\
                f{\scriptstyle(t_i)}\\
                \vdots
            \end{matrix}\right)
    \end{aligned}
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \parbox[t]{.95\textwidth}{\scriptsize Runge Phenom.: As \(n\) increases, evenly-spaced \(t_i\) could produce a 
        high-dimensional polynomial \(\hat{f}(t)\) that tends to be extremely wavey near the endpoints (like Gibbs phenom.).
        Choosing \(t_i\) to be Chebyshev nodes between the two endpoints mitigates this.}
        \\
    &\bullet\ \text{\scriptsize Interpolation w/ other func. like rationals are possible.}
        \\
    &\bullet\ \text{\scriptsize Error: } {\scriptstyle 
        \text{\scriptsize\(
            \begin{gathered}
                \max\\[-5pt]
                {t \in [t_1,t_n]}
            \end{gathered}\)} 
            \ \left| \hat{f} - f 
            \ =\ \frac{ f^{(n)}(\xi) }{n!} \prod_i (t-t_i) \right| 
            \ \leq\ \left| \max \frac{ f^{(n)}(t) }{n!} \right| 
            \left| \frac{(n-1)! h^n}{4} \right|
            \ =\ \boxed{ 
                \text{\scriptsize\(
                    \begin{gathered}
                        \max\\[-5pt]
                        {t \in [t_1,t_n]}
                    \end{gathered}
                    \ \left| f^{(n)}(t)\hs \frac{ h^n }{ 4n } \right|
                \)}
            } 
        } 
        \hspace{2pt} \rightarrow \hspace{2pt}
        \parbox{3cm}{\scriptsize error decreases if \(f^{(n)}\)\\ is well behaved}
    \end{aligned}\)

%------------------------------------------------------------------
%------------------------------------------------------------------
% Taylor Series Polynomial Interpolation
\subsection{Taylor Series Polynomial Interpolation}
\vspace{5pt}
\[
    \arraycolsep=3pt
    \begin{array}{l c c c l c l c l c l}
        \hat{f}_n(t) &=& f(t_0) 
            &+& f'(t_0) (t-t_0) 
            &+& \tfrac{ f''(t_0) }{2} (t-t_0)^2 
            &+& \dots &+& \tfrac{ f^{(n-1)}(t_0) }{(n-1)!} (t-t_0)^{n-1}\\[5pt]
        \hat{f}_n(t+h) &=& f(t) 
            &+& f'(t) h 
            &+& \tfrac{ f''(t) }{2}\hs h^2 
            &+& \dots 
            &+& \tfrac{ f^{(n-1)}(t) }{(n-1)!}\hs h^{n-1}
    \end{array}
\]

% Notes
\vspace{-5pt}
\(\begin{aligned}
    &\bullet\ \text{\scriptsize Can interpolate an \(n\)-polynomial from \(n+1\) points/derivatives/info.}
\end{aligned}\)

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Monomial Basis Functions/Vandermonde Matrix
\subsection{Monomial Basis Functions \(\rightarrow\) Vandermonde Matrix}

\vspace{5pt}
\(
    % Basis Functions
    \boxed{ 
        \begin{aligned}
            \hsvec{\phi}(t) &= \big( 1, t, t^2,\ \dots\ , t^{n-1} \big)^T\\[5pt]
            \hat{f}(t) &= x_1 + x_2 t + \dots + x_n t^{n-1}
        \end{aligned}
    }
    \hspace{10pt}
    \vline
    \hspace{10pt}
    % Matrix
    \begin{gathered}[b]
        \text{\scriptsize(Full, Dense}
            \\[-7pt]
        \text{\scriptsize Vandermonde Matrix) }
            \\
        \left(\begin{matrix}
            1 & t_1 & \dots & t_1^{n-1}\\
            \vdots & \vdots & &\vdots \\
            1 & t_n & \dots & t_n^{n-1}\\
        \end{matrix}\right)
    \end{gathered}
    \left(\begin{matrix}
        \vdots\\
        x_i\\
        \vdots
    \end{matrix}\right)
    = \vec{y}
    % Notes
    \hfill
    \begin{aligned}
        &\bullet\ \parbox[t]{4cm}{\scriptsize Solved with \(\mathcal{O}(n^3)\) work using 
            Gauss. Elim. (\(\mathcal{O}(n^2)\) is possible with other tech.).}
            \\
        &\bullet\ \parbox[t]{4cm}{\scriptsize Ill-conditioned since sucessive \\ 
            \(t^j\) look the same at higher \(j\).}
            \\[20pt]
    \end{aligned}
\)

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Lagrange Basis Functions (Fundamental Polynomials)/Identity Matrix
\subsection{Lagrange Basis Functions (Fund. Polynomials) \(\rightarrow\) Identity Matrix}

\vspace{5pt}
% Basis Functions
\(
    \begin{aligned}
        &\boxed{ 
            \begin{aligned}
                l(t) &= (t-t_1)(t-t_2)\dots(t-t_n)\\
                w_j &= (t_j - t_j) / l(t_j) \hspace{10pt} \text{\scriptsize(barycentric weights)}
            \end{aligned} 
        }
            \\[10pt]
        &\begin{aligned}
            \Aboxed{ \phi_j(t) &= \tfrac{ l(t) / (t-t_j) }{ l(t_j) / (t_j-t_j) } 
                = l(t) \frac{w_j}{t-t_j} }
                \\
            \phi_j(t_i) &= \delta_{ij} \ \Rightarrow \ \boxed{ \hsvec{\phi}(t_i) = \vec{e}_i }
        \end{aligned}
            \\[10pt]
        &\begin{aligned}
            \Aboxed{ \hat{f}(t) &= \hsvec{x} \cdot \hsvec{\phi}(t)
                = l(t) \Big[ x_1 \tfrac{w_1}{t-t_1} + \dots + x_n \tfrac{w_n}{t-t_n} \Big] }
                \\
            \hat{f}(t_j) &= x_j = y_i
        \end{aligned}
    \end{aligned}
\)
\hfill
\vline
\hfill
\begin{minipage}{.45\textwidth}
    % Matrix
    \[
        \begin{gathered}[b]
            \text{\scriptsize(Diag. Iden. Matrix)}\\
            \text{\scriptsize\(
                \left(\begin{matrix}
                    1 & 0 & \dots\\
                    0 & 1 & \ddots\\
                    \vdots & \ddots & \ddots
                \end{matrix}\right) 
                \)}
        \end{gathered}
        \hs \vec{x} = \vec{y}
    \]

    % Notes
    \vspace{5pt}
    \(\begin{aligned}
        &\bullet\ \text{\scriptsize Finding \(w_j\) is \(\mathcal{O}(n^2)\) work.}\\
        &\bullet\ \text{\scriptsize Finding \(\hat{f}(t)\) from \(w_j\)'s is \(\mathcal{O}(n)\) work.}\\
        &\bullet\ \boxed{ \parbox[t]{.9\textwidth}{\scriptsize Updating with an extra point \((t_{n+1}, y_{n+1})\) 
            is \(\mathcal{O}(n)\) work \\by changing \(w_j = w_j/(t_j-t_{n+1})\) and finding \(w_{n+1}\).} }\\
        &\bullet\ \text{\scriptsize Basis func. are more varied \(\rightarrow\) better-conditioned.}\\
        &\bullet\ \boxed{ \int_{t_1}^{t_n} \hat{f}(t) dt = \sum_{i=1}^n y_i \int_{t_1}^{t_n} \phi_i(t) dt }
    \end{aligned}\)
\end{minipage}  

%---------------------------------------------------------------------------------------------------------------------------------
%
%
%---------------------------------------------------------------------------------------------------------------------------------
% Newton Basis Functions/Lower Triangular Matrix
\newpage
\subsection{Newton Basis Functions \(\rightarrow\) Low. Triang. Matrix}

\vspace{5pt}
\(
    % Basis Functions
    \begin{aligned}
        &\boxed{ 
            \begin{aligned}
                \phi_j(t) &= (t-t_1)(t-t_2)\dots(t-t_{j-1})\\
                \hsvec{\phi}(t) &= \big[ 1, (t-t_1), (t-t_1)(t-t_2),\ \dots \big]^T
            \end{aligned}
            }
            \\[5pt]
        &\boxed{ \hat{f}(t) = x_1 + x_2(t-t_1) + \dots + x_n \phi_n(t) }
    \end{aligned}
    \hspace{10pt}
    \vline
    \hspace{10pt}
    % Matrix
    \begin{aligned}
        &\begin{gathered}[b]
            \text{\scriptsize(Low. Triang. Matrix)}\\
            \text{\scriptsize\(
                \left(\begin{matrix}
                    1 & 0 & 0 & \dots \\
                    1 & t_1-t_2 & 0 & \dots\\
                    1 & t_3-t_2 & (t_3-t_1)(t_3-t_2) & \ddots\\
                    \vdots & \vdots & \vdots & \ddots
                \end{matrix}\right) 
                \)}
        \end{gathered}
            \text{\scriptsize\( 
                \left(\begin{matrix}
                    \vdots\\
                    x_i\\
                    \vdots
                \end{matrix}\right)
            \)} 
            = \vec{y}
    \end{aligned}
\)

% Notes
\vspace{5pt}
\(\begin{aligned}
    &\bullet\ \text{\scriptsize For. sub. is \(\mathcal{O}(n^2)\).}\\
    &\bullet\ \text{\scriptsize Cond. of \(A\) depends on ordering of points \(\rightarrow\) best to order points from
        their dist. to their mean/other num.}\\
    &\bullet\ \text{\scriptsize Basis func. are more varied \(\rightarrow\) better-conditioned.}
\end{aligned}\)

% Updating
\vspace{15pt}
\begin{minipage}[t]{.5\textwidth}
    \underline{Incremental Updating Newton Interpolation}:
    
    \vspace{5pt}
    \(\hat{f}_{n+1}(t) = \hat{f}_n(t) + x_{n+1}\hs \phi_{n+1}(t)\)

    \vspace{10pt}
    \(
        \begin{aligned}
            &\begin{aligned}
                y_{n+1} &= \hat{f}_{n+1}(t_{n+1})\\
                &= \hat{f}_n(t_{n+1}) + x_{n+1}\hs \phi_{n+1}(t_{n+1})
            \end{aligned}
                \\[5pt]
            &\Rightarrow\ \boxed{ 
                \hat{f}_{j+1}(t) = \hat{f}_j(t) + \tfrac{ y_{j+1} - \hat{f}_j(t_{j+1}) }{ \phi_{j+1}(t_{j+1}) } \phi_{j+1}(t) 
                }
        \end{aligned}
    \)
\end{minipage}
% Divided Differences
\begin{minipage}[t]{.49\textwidth}
    \underline{Divided Differences Newton Interpolation}:
    
    \vspace{5pt}
    \(\displaystyle
        g[t_1, \dots , t_k] \ \equiv\ \frac{ g[t_2, \dots , t_k] - g[t_1, \dots , t_{k-1}] }{t_k - t_1}
    \)

    \vspace{5pt}
    \(
        \boxed{
            \vec{x} 
            = \text{\scriptsize\(
                \left(\begin{matrix}
                    x_1\\
                    x_2\\
                    x_3\\
                    \vdots
                \end{matrix}\right)
            \)}
            = \text{\scriptsize\(
                \left(\begin{matrix}
                    g[t_1]\\
                    g[t_1, t_2]\\
                    g[t_1, t_2, t_3]\\
                    \vdots        
                \end{matrix}\right)
            \)}
        }
        \hspace{10pt}
        % Notes
        \begin{aligned}
            &\bullet\ \text{\scriptsize Also costs \(\mathcal{O}(n^2)\).}\\
            &\bullet\ \parbox[t]{3cm}{\scriptsize Less prone to \\ over/underflow.}
        \end{aligned}
    \)
\end{minipage}

%------------------------------------------------------------------
%------------------------------------------------------------------
% Orthogonal Polynomial Basis
\vspace{5pt}
\subsection{Orthogonal Polynomial Basis (no method given)}

% Inner Product
\vspace{10pt}
\parbox[t]{.55\textwidth}{%
    \underline{Inner Product}:\ \ \(
        \boxed{
            \langle \hsvec{u} | \hsvec{v} \hs \rangle_{ab}^w 
            = \int_a^b \left[ u(t) v(t) \right] w(t) \ dt 
        }
    \)
}
% Orthogonal Polynomials
\parbox[t]{.44\textwidth}{%
    \underline{Orthogonal Polynomials}:\ \ \(
        \boxed{
            \langle u_i | u_j \rangle
            = \delta_{ij}
        }
    \)
}

% Three-Term Recurrence
\vspace{5pt}
\underline{Three-Term Recurrence}: \ \ \(
    \boxed{ \hat{f}_{k+1}(t) = \big[ A(k)t + B(k) \big] \hat{f}_k(t) - C(k) \hat{f}_{k-1}(t) } 
    \hspace{20pt} {\scriptstyle(A(k) \hs \neq\hs 0)}
\)

%----------------------------------------------------------------
%----------------------------------------------------------------
% Piecewise Hermite Cubic Interpolation
\subsection{Piecewise [Hermite] Cubic Interpolation}

% Picewise Cubic
\parbox[t]{5cm}{
    \underline{Piecewise Cubic}: \\[5pt]
    {\scriptsize 
        \(n\) knots/pts. \(\Rightarrow\ n-1\) cubics \\
        \(\Rightarrow\) \fbox{ \(4(n-1)\) param./eq.} 
    }
}
% Hermite Interpolation Def
\parbox[t]{5cm}{
    \underline{Hermite Interpolation}: \\[5pt]
    {\scriptsize 
        Using \(k\)-th derivatives as info.\\
        Extra equations can be used \\
        for monotonicity/convexity.
    }
}
% Hermite Cubic Interp.
\parbox[t]{7cm}{
    \underline{Hermite Cubic Interpolation}: \\[5pt] 
    {\scriptsize 
        Continuous 0th and 1st derivatives; \ \(n-1\) cubics \\
        \(\Rightarrow [2(n-1)]_\text{1st deriv. eq} + [n-2]_\text{2nd deriv. eq.}\) \\
        \(=\) \fbox{ \(3n-4\) eq. \(\Rightarrow\ n\) free/extra param./eq}
    }
}

%----------------------------------------------------------------
%----------------------------------------------------------------
% Piecewise Cubic [Spline] Interpolation
\subsection{Piecewise Cubic [Spline] Interpolation}

% Spline
\parbox[t]{4.5cm}{
    \underline{Spline}: \\[4pt] 
    {\scriptsize 
        A piecewise func. of \(n\)-polynomials that is \(n\)-differentiable (of differentiability class \(C^{n-1}\), 
        or \(n-1\) cont. differentiable).
    }
}
\hspace{.75cm}
% Cubic Spline Interp.
\parbox[t]{6.5cm}{
    \underline{Cubic Spline Interpolation}: \\[5pt] 
    {\scriptsize 
        Cont. 0th, 1st, and 2nd derivatives; \ \(n-1\) cubics \\
        \(\Rightarrow [2(n-1)]_\text{1st} + [n-2]_\text{2nd} + [n-2]_\text{3rd}\)\\
        \(=\) \fbox{ \(4n-6\) eq. \(\Rightarrow\ 2\) free/extra param./eq}
    }
}
\hspace{.75cm}
% B-Spline Basis Func.
\parbox[t]{4.5cm}{
    \underline{\(B\)-splines (basis func.)}: \\[4pt] 
    {\scriptsize 
        Orthog. \(\{\phi_j(t)\}\) are \(j\)-poly. splines w/ local compact support and look like bells.
        (not much detail here).
    }
}

%------------------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------------------
% Numerical Integration
\newpage
\section{Numerical Integration/Quadrature, \(I(f) \equiv \int_a^b f(x)\hs dx\)}

% Infinity Norm and Ccondition Number
\subsection{\(\infty\)-Norm and Condition Number}

\vspace{5pt}
% Infinity Norm
\begin{minipage}[t]{.36\textwidth}
    \underline{Function \(\infty\)-Norm}: \\[10pt] 
    \fbox{ \(\Vert f(x) \Vert_\infty = \max_{x \in [a,b]} f(x)\) }    
\end{minipage}
% Abs. Cond Number for Integration if \hat{b}
\begin{minipage}[t]{.63\textwidth}
    \underline{[Abs.] Integration Condition Number if \(\hat{b}\)}: \\[10pt]
    \(
        \left\vert \int_a^{\hat{b}} f(x)\hs dx - \int_a^b f(x)\hs dx \right\vert
        = \left\vert \int_b^{\hat{b}} f(x)\ dx \right\vert
        \ \leq\ \boxed{ (\hat{b}-b) \Vert f(x) \Vert_\infty }
    \)    
\end{minipage}

\vspace{5pt}
% Abs. Cond Number for Integration if \hat{f}
\begin{minipage}[t]{.56\textwidth}
    \underline{[Abs.] Integration Condition Number if \(\hat{f}\)}: \\[10pt]
    \(
        \begin{aligned}
            \left\vert \int_a^b \hat{f}(x) - f(x)\ dx \right\vert
                &\ \leq\ \int_a^b \left\vert \hat{f}(x) - f(x) \right\vert dx
                \\
            &\ \leq\ (b-a) \Vert \hat{f}(x) - f(x) \Vert_\infty
                \\
            \left\vert \frac{\Delta I}{\Delta f} \right\vert 
                &\ \leq\ \boxed{ b-a }
        \end{aligned}
    \)    
\end{minipage}
% Rel. Cond Number for Integration if \hat{f}
\begin{minipage}[t]{.43\textwidth}
    \underline{[Rel.] Integration Condition Number if \(\hat{f}\)}: \\[10pt]
    \(
        \begin{aligned}
            \left\vert \frac{\Delta I / I}{\Delta f / f} \right\vert 
                &\ \leq\ \frac{(b-a) / \left\vert \int_a^b f(x) dx \right\vert}{1 / \Vert f(x) \Vert_\infty}
                \\[5pt]
            &\ =\ \boxed{ \frac{(b-a) \Vert f(x) \Vert_\infty }{\left\vert \int_a^b f(x) dx \right\vert} }
        \end{aligned}
    \)
\end{minipage}

%---------------------------------------------------------------
%---------------------------------------------------------------
% Interpolary Quadrature Rule
\vspace{-5pt}
\subsection{1-D [Interpolary] Quadrature Rule for \(f \approx \hat{f}\)}

% Basis for \hat{f}
\(
    \underline{ \hat{f} \in P_{n-1} } :
    \ \ \hat{f}(x) 
    = \left(\begin{gathered}
        \hsvec{y} \cdot \hsvec{\phi}(x) = \sum_{i=1}^n f(x_i)\hs \phi_i(x) \\
        \text{\scriptsize(Lagrange Basis Vectors)}
    \end{gathered}\right)
    = \left(\begin{gathered}
        \sum_{j=0}^{n-1} c_j\hs x^j \\
        \text{\scriptsize(Monomial Basis Vetors)}
    \end{gathered}\right)
    \hspace{20pt} \begin{aligned}
        &{\scriptstyle\bullet\ x_1\ <\ \dots\ <\ x_n}\\ 
        &{\scriptstyle\bullet\ f(x_i) \ =\ \hat{f}(x_i)}
    \end{aligned}
\)

% Quadrature Rule
\(\displaystyle
    \Rightarrow\ \boxed{ Q_n(f) 
        \ \equiv\ I(\hat{f}) 
        =  \int_a^b \hat{f}(x)\hs dx 
        = \sum_{i=1}^n f(x_i) \int_a^b \phi_i(x)\hs dx 
        = \sum_{i=1}^n f(x_i)\hs w_i
        }
        \hspace{20pt} \begin{aligned}
            &{\scriptstyle \bullet\ \boxed{\scriptstyle x_i,\ w_i \ \rightarrow\ 2n \text{\scriptsize\ max param.} } }\\[-4pt]
            &{\scriptstyle \bullet\ a\ \leq\ x_1\ <\ \dots\ <\ x_n\ \leq\ b} \\[-7pt]
            &\text{\scriptsize \(\bullet\ \) closed if equality, open if not}
        \end{aligned}
\)

% Math to Method of Undetermined Coefficients
\vspace{5pt}
\hspace{5pt} \(\arraycolsep=2pt
    \begin{array}{c c c}
        \displaystyle \int_a^b \left( \sum_{j=0}^{n-1} c_j\hs x^j \right) dx 
            &=
            &\displaystyle \sum_{i=1}^n \left( \sum_{j=0}^{n-1} c_j\hs x_i^j \right) w_i
            \\[30pt]
        \displaystyle \sum_{j=0}^{n-1} c_j \left( \int_a^b x^j dx \right) 
            &=
            &\displaystyle \sum_{j=0}^{n-1} c_j \left( \sum_{i=1}^n x_i^j \hs w_i \right)
            \\[20pt]
        \multicolumn{3}{c}{ \text{\scriptsize(maybe some dot product to isolate terms)} }
    \end{array}
    \hspace{5pt} \Rightarrow \hspace{5pt}
\)
% Method of Undetermined Coefficients
\begin{minipage}{.48\textwidth}
    \hspace{17pt} \underline{Method of Undetermined Coefficients}    
    
    \vspace{8pt}
    \(
        \begin{aligned}
            \int_a^b x^j dx = \sum_{i=1}^n x_i^j \hs w_i 
                &\ =\ \tfrac{ b^{j+1} - a^{j+1} }{ j+1 } \ \equiv\ z_j
                \\[-7pt]
            &\hspace{20pt} \rightarrow\ \boxed{ \scriptstyle z_0 \ =\ \sum w_i \ =\ b-a }
                \\[5pt]
            \Aboxed{ 
                \begin{gathered}[b]
                    \text{\scriptsize(Vandermode Matrix)}
                        \\
                    \text{\scriptsize\(
                            \left(\begin{matrix}
                                1 & 1 & 1 & \dots\\
                                x_1 & x_2 & x_3 & \dots\\[3pt]
                                x_1^2 & x_2^2 & x_3^2 & \dots\\
                                \vdots & \vdots & \vdots
                            \end{matrix}\right)
                        \)}
                \end{gathered}
                \hsvec{w}
                &\ =\ \hsvec{z}
                \hspace{20pt} \begin{gathered}
                    \text{\scriptsize \underline{System of Moment}}\\[-5pt]
                    \text{\scriptsize \underline{Equations}}
                \end{gathered}
                \ \
            }
        \end{aligned}
    \)
\end{minipage}

% Notes
\vspace{15pt}
\(\begin{aligned}
    &\underline{ \text{Error } I}:
        \ \ | \Delta I | 
        \ \leq\ (b-a) \Vert f-\hat{f} \Vert_\infty 
        \ \leq\ \tfrac{b-a}{4n}\hs h^n \Vert f^{(n)} \Vert_\infty 
        \ \leq\ \boxed{ \tfrac{h^{n+1}}{4} \Vert f^{(n)} \Vert_\infty }
        \hspace{5pt} \rightarrow \hspace{5pt}
        \parbox{3cm}{\scriptsize error decreases if \(f^{(n)}\)\\ is well behaved}
        \\[10pt]
    &\underline{ \text{Error } Q_n}:
        \ \begin{aligned}[t]
            &g \approx f 
                \ \rightarrow \ \begin{aligned}[t]
                    &| Q_n(f) - Q_n(g) | \\
                    &= {\scriptstyle \left| \sum w_i \big[ f{\scriptstyle(x_i)} - g{\scriptstyle(x_i)} \big] \right| }
                \end{aligned}
                \ \leq\ \boxed{ \scriptstyle \sum | w_i | \cdot \Vert f - g \Vert_\infty }
                \ \Rightarrow\ \begin{aligned}[t]
                    &\boxed{\forall w_i \geq 0 \ \rightarrow\ \text{\scriptsize cond}(Q_n) = b-a }\\
                    &\text{\scriptsize(otherwise using \(Q_n\) might be unstable.)}
                \end{aligned}
        \end{aligned}
\end{aligned}\)

%
%
%
%------------------------------------------------------------------------------------------------------------------------------
\newpage

% Rule Degree
\underline{[Rule] Degree, \(d\)}: \ \fbox{ 
    \(\forall p(x) \in P_d\), rule \(Q(p) = I(p)\), but not \(\forall p \in P_{d+1}\) 
}

% Newton-Cotes Quadrature
\vspace{10pt}
\underline{Newton-Cotes Quadrature [Rule]}: \ \fbox{\(n\) evenly-spaced \(x_i \ \rightarrow\ n\) param. for \(w_i\)}

% Midpoint, Trap, Simpson Rules
\(\hspace{10pt} \arraycolsep=2pt \begin{array}{r c c c l}
    \text{Midpoint Rule}\ {\scriptstyle(Q_1)}
        &:\ \
        &M(f) = \tfrac{b-a}{1}\hs f(\tfrac{a+b}{2})
        &\hspace{30pt} 
        &\hsvec{w} = (b-a)\hs [1]^T
        \\[5pt]
    \text{Trapezoidal Rule}\ {\scriptstyle(Q_2)}
        &:\ \
        &T(f) = \frac{b-a}{2}\ [f(a) + f(b)]
        &
        &\hsvec{w} = (b-a)\hs [\tfrac{1}{2}, \tfrac{1}{2}]^T
        \\[5pt]
    \text{Simpsons's Rule}\ {\scriptstyle(Q_3)}
        &:\ \
        &S(f) = \frac{b-a}{6}\ [f(a) + 4f(\tfrac{a+b}{2}) + f(b)]
        & 
        &\hsvec{w} = (b-a)\hs [\tfrac{1}{6}, \tfrac{4}{6}, \tfrac{1}{6}]^T
\end{array}\)

% Notes
\vspace{10pt}
\(\hspace{10pt} \begin{aligned}
    % Taylor Expansion and Error
    &\bullet\ \text{Taylor Expansion and Error}\\
    &\begin{aligned}[t]
            % Taylor Expansion
            f(x) &= \boxed{ \sum_{m=0} \tfrac{f^{(m)}(\tfrac{a+b}{2})}{m!} (x-\tfrac{a+b}{2})^m }
                \\[10pt]
            % Trapezoid Expansion
            T(f) &= \tfrac{b-a}{2}
                \sum_{m=0} \tfrac{f^{(m)}(\tfrac{a+b}{2})}{m!} \tfrac{ (b-a)^m }{ 2^m } \left[(-1)^m + 1\right] 
                \\[5pt]
            &= \sum_{m=0}^\text{even} \dboxed{ \tfrac{f^{(m)}(\tfrac{a+b}{2})}{2^m m!} }\hs (b-a)^{m+1} \\
            &= \boxed{ M(f) + \sum_{m=2}^\text{even} \dboxed{ E_m(f) }\hs h^{m+1} }
                \\[10pt]
            % Simpson Expansion
            S(f) &= \boxed{ \tfrac{2}{3} M(f) + \tfrac{1}{3} T(f) }
        \end{aligned}
        \hspace{10pt}
        \begin{aligned}[t]
            % Integral Expansion
            I(f) &= \sum_{m=0} \tfrac{f^{(m)}(\tfrac{a+b}{2})}{(m+1)!} \tfrac{ (2x-a-b)^{m+1} }{ 2^{m+1} } \big|_a^b \\[5pt]
            &= \sum_{m=0}^\text{even} \tfrac{ f^{(m)}(\tfrac{a+b}{2}) }{ 2^{m} (m+1)! }\hs (b-a)^{m+1}\\
            % Midpoint Error
            &= \boxed{ M(f) + \sum_{m=2}^\text{even} \tfrac{ E_m(f) }{ m+1 }\hs h^{m+1} }
                \hspace{10pt} \begin{gathered}
                    \text{\scriptsize \(Q_1\) error is \(f^{(2)}\)}\\[-5pt]
                    \text{\scriptsize derivative, not \(f^{(1)}\)!}
                \end{gathered}
                \\
            % Trapezoid Error
            &= \boxed{ T(f) - \sum_{m=2}^\text{even} m\hs \tfrac{E_m(f)}{m+1}\hs h^{m+1} }
                \hspace{10pt} \begin{gathered}
                    \text{\scriptsize \(Q_2\) error is \(f^{(2)}\) \&}\\[-5pt]
                    \text{\scriptsize twice as large as \(Q_1\)}
                \end{gathered}
                \\
            % Simpson Error
            &= \boxed{ S(f) - \sum_{m=4}^\text{even} \tfrac{m-2}{3} \hs \tfrac{ E_m(f) }{ m+1 } \hs h^{m+1} }
                \hspace{10pt} \begin{gathered}
                    \text{\scriptsize \(Q_3\) error is \(f^{(4)}\)}\\[-5pt]
                    \text{\scriptsize derivative, not \(f^{(3)}\)!}
                \end{gathered}
        \end{aligned}
        \\[10pt]    
    % Degree/Error
    &\bullet\ \arraycolsep=2pt 
        \begin{array}[t]{r c c c c l}
            n \text{ is even} &: \ \
                &\text{\scriptsize \(Q_n\) error is expected \(f^{(n)}\) derivative}
                &\hspace{10pt}
                &Q(p_{n-1}) = I(p_{n-1})
                &\rightarrow\ \boxed{d = n-1}
                \\[5pt]
            n \text{ is odd} &: \ \
                &\text{\scriptsize \(Q_n\) error is \(f^{(n+1)}\) derivative}
                &
                &Q(p_{n}) = I(p_{n})
                &\rightarrow\ \boxed{d = n}
        \end{array}
        \\[5pt]
    % Error Difference
    &\bullet\ \text{\scriptsize \underline{2 Rule Error}: \ Est. diff. between \(T(f)\) and \(M(f)\) can be used 
        to est. \(I(f)\) error in using either.}
        \\[5pt]
    % Progressive
    &\bullet\ \text{\scriptsize Can use subinterval, so can be \underline{progressive}.}\\
    % Runge Phenom
    &\bullet\ \text{\scriptsize Evenly-spaced \(x_i\) exibit the Runge Phenom.} 
        \rightarrow\ \boxed{ Q_\infty(f) \text{ isn't always } I(f) }
        \\[5pt]
    % Ill Conditioned
    &\bullet\ \boxed{ \text{\scriptsize Ill-conditioned and unstable} }:\ 
        (n \geq 11 \Rightarrow \exists w_i < 0) ,\ \left( {\scriptstyle \sum_i^\infty}\hs |w_i| \rightarrow \infty \right) 
\end{aligned}\)

% Clenshaw-Curtis Quadrature
\vspace{15pt}
\underline{Curtis-Clenshaw Quadrature [Rule]}:\ \ \fbox{\(n\) Chebyshev Nodes, \(x_i \ \rightarrow\ n\) param. for \(w_i\)}

% Notes
\vspace{10pt}
\(\hspace{10pt} \begin{aligned}[t]
    &\bullet\ \forall n: \forall w_i > 0 \ \Rightarrow\ \text{\scriptsize cond}(Q) = b-a\\
    &\bullet\ \lim_{n\rightarrow\infty} C_n(f) = I(f)\\
    &\bullet\ \boxed{ d_n = n-1 }
\end{aligned}
\hfill
\begin{aligned}[t]
    &\bullet\ \parbox[t]{6cm}{\scriptsize \(\exists\)\ an algorithm w/ Chebyshev polynomials 
        to find integrand w/o solving for \(w_i\).}\\
    &\bullet\ \text{\scriptsize Using Chebyshev polynomial zeroes is the classical CCQ.}\\
    &\bullet\ \text{\scriptsize Using Chebyshev extrema leads to a progressive rule [practical CCQ].} 
\end{aligned}\)

%
%
%
%------------------------------------------------------------------------------------------------------------------------------
\newpage

% Guassian Quadrature
\underline{Guassian Quadrature [Rule]}:\ \ \fbox{\(2n\) free param. for \(x_i,\ w_i\)} \(\Rightarrow\ \boxed{ d_n = 2n-1 }\) 

% Notes
\vspace{10pt}
\(\hspace{10pt} \begin{aligned}[t]
    % System of Moment Equations
    &\bullet\ x_i, w_i : \ \
        x_{n < i \leq 2n} = w_{n < i \leq 2n} = 0
        \ \rightarrow \
        \boxed{
            \text{\scriptsize\(
                \left(\ \begin{matrix}
                    1 & \dots & 1 & 0 & \dots\\
                    x_1 & \dots & x_n & 0 & \dots\\[3pt]
                    x_1^2 & \dots & x_n^2 & 0 & \dots\\
                    \vdots&  & \vdots & \vdots
                \end{matrix}\ \right)
                \left(\begin{matrix}
                    \vdots\\
                    w_n\\
                    0\\[-4pt]
                    \vdots\\
                \end{matrix}\right) 
            \)}
            \ =\ \hsvec{z}(a,b)
        }
        \hspace{20pt}
        \boxed{ \text{\scriptsize usually \(x_i \notin \mathbb{Q}\)} }
        \\
    % Orthogonal Polynomials
    &\bullet\ \text{Ortho. Poly.}:\ \ 
        \begin{gathered}[t]
            \langle p_n{\scriptstyle(x)} | x^k \rangle_{ab} = 0 \\
            {\scriptstyle(k = 0,\ \dots,\ n-1)}
        \end{gathered}
        \ \Rightarrow\ \boxed{ 
            x_i:\ \begin{array}[t]{c c}
                p_n(x_i) = 0, & x_i \in \mathbb{R}, \\
                x_i \neq x_{j \neq i}, & x_i \in (a,b)
            \end{array}
        } 
        \hspace{20pt}
        \begin{aligned}
            \\[-9pt]
            {\scriptstyle[-1,1]} 
                &- \text{\scriptsize Legendre}
                \\[-7pt]
            {\scriptstyle(-\infty,\infty)} 
                &- \text{\scriptsize Hermite}
                \\[-7pt]
            {\scriptstyle[0,\infty)} 
                &- \text{\scriptsize Laguerre}
                \\[-7pt]
        \end{aligned}
        \\[5pt]
    % Interval Transform
    &\bullet\ \text{\scriptsize Interval Transform}:\ \boxed{ 
        \int_a^b f(t)\hs dt = \tfrac{b-a}{\beta-\alpha} \int_\alpha^\beta f(t)\hs dx
        \hspace{20pt} t = \tfrac{ (b-a)x + a\beta - b\alpha }{ \beta - \alpha } 
    }
        \\[5pt]
    % Convergence
    &\bullet\ \forall n: \forall w_i > 0 \ \Rightarrow\ \text{\scriptsize cond}(Q) = b-a
        \hspace{25pt} \bullet\ \lim_{n\rightarrow\infty} G_n(f) = I(f)
        \\
    % Not progressive
    &\bullet\ n = 2m+1\ \rightarrow\ \tfrac{a+b}{2} \in \{x_i\}_n;\ \ 
        \text{\scriptsize otherwise usually } \{x_i\}_n \cup \{x_i\}_{\neq n} = 0
        \ \rightarrow\ \boxed{ \text{\scriptsize Not progressive} }
        \\[5pt]
    % Progressive Rules
    &\bullet\ \text{\scriptsize\(
        \arraycolsep=2pt \begin{array}[t]{r c c l}
            % Gauss-Kronrod
            \underline{ \text{Progressive Gauss-Kronrod} , K_{2n+1} }: 
                &n \text{ from } G_n
                &\rightarrow\ \begin{aligned}
                        n+1 &\text{ param for } x_{i>n}\\[-3pt]
                        2n+1 &\text{ param for } w_i
                    \end{aligned}
                &\Rightarrow \ \boxed{d_{2n+1} = 3n+1 < 4n+1}
                \\[5pt]
            % GK 2 Rule Error
            \text{GK 2-Rule Error}:
                &\multicolumn{3}{l}{ 
                    \boxed{ \Delta I(f) \approx (200|G_n - K_{2n+1}|)^{1.5} } 
                    }
                \\[10pt]
            % Gauss Patterson
            \text{Progressive Gauss-Patterson}, P_{4n+3}: 
                &2n+1 \text{ from } K_{2n+1}
                &\rightarrow\ \begin{aligned}
                        2n+2 &\text{ param for } x_{i>n}\\[-3pt]
                        4n+3 &\text{ param for } w_i
                    \end{aligned}
                &\Rightarrow \ \boxed{d_{4n+3} = 6n+4 < 8n+5}
        \end{array}
        \)}
        \\
    % Closed Intervals
    &\bullet\ \text{\scriptsize\(
        \arraycolsep=2pt \begin{array}[t]{r r l}
            \text{Closed Gauus-Randau}: 
                &x_i \in [a,b) \text{ or } (a,b] 
                &\rightarrow \ \boxed{d = 2n-2}
                \\[3pt]
            \text{Closed Gauus-Lobatto}: 
                &x_i \in [a,b] 
                &\rightarrow \ \boxed{d = 2n-3}
        \end{array}
        \)}
\end{aligned}\)

% Composite [Subinterval] Quadrature
\vspace{10pt}
\underline{Composite [\(k\)-Subintervals] Quadrature for Rule \(Q_n\)}: \ \(
    Q_n \ \rightarrow \ Q_{kn} \text{ \ or \ } Q_{kn-(k-1)} , 
\)

\(\hspace{10pt}\begin{aligned}
    &\bullet\ \lim_{k\rightarrow\infty} C_{k,n} 
        = \mathscriptsize{ \sum_{j=1}^{k\rightarrow\infty} } \left[ 
            \mathscriptsize{ \sum_{i=1}^n }\hs 
            w_i f{\scriptstyle(x_{ji})} 
        \right]
        = \mathscriptsize{ \sum_{i=1}^n }\hs 
            \tfrac{w_i}{h_k} \left[ 
            \mathscriptsize{ \sum_{j=1}^{k\rightarrow\infty} } 
            h_k f{\scriptstyle(x_{ji})} 
        \right]
        = I(f) \mathscriptsize{ \sum_{i=1}^n }\hs \tfrac{w_i}{h_k} 
        = I(f)     
        \hspace{25pt} \mathscriptsize{ 
            \begin{aligned}
                &\begin{aligned}
                    h_k &= (b-a)/k \\[-2pt]
                    &\geq (x_{jn}- x_{j1})
                \end{aligned}
                    \\[2pt]
                &\boxed{d \geq 0} \ \Rightarrow\ \sum w_i = h_k 
            \end{aligned}
        }
        \\
    &\bullet\ \text{Error}:\ \mathcal{O}(h^{m+1}) 
        \ \rightarrow\ \mathcal{O}(kh_k^{m+1}) 
        = \boxed{ \mathcal{O}(h_k^{m}) }
        \hspace{20pt} {\scriptstyle(k>1)}
\end{aligned}\)

% Adaptive Quadrature
\vspace{10pt}
\underline{Adaptive Quadrature for Rule \(Q_n\)}:\ \ Divide subinterval until a tolerance is met.

%------------------------------------------------------------------
%------------------------------------------------------------------
% n-Dim Integration
\subsection{\(n\)-D Integration}

{\tabcolsep=5pt
    \begin{tabular}{r l}
        {Double Integral}:
            &Use a pair of 1-D routines for the inner/outer integral.
            \\[5pt]
        {\(\scriptstyle (n>2)\)-Dimension Integral}:
            &Monte Carlo is best (error \(\scriptstyle 1/\sqrt{n} \ \rightarrow\ 0\)).
    \end{tabular}
}

%------------------------------------------------------------------
%------------------------------------------------------------------
% Other Integrals
\subsection{Other Integrals}

{\tabcolsep=5pt
    \begin{tabular}{r l}
        Tabular Data:
            &Integrate a piecewise interpolant.
            \\[5pt]
        Improper Integral:
            &\parbox[t]{10cm}{Separate the integral, do a variable change, \\or add/subtract a term to remove singularities.}
            \\[20pt]
        (Fredholm) Integral Equations:
            &skipped 
    \end{tabular}
}

\newpage
%------------------------------------------------------------------
%
%
%------------------------------------------------------------------
% Richardson Extrapolation [Romberg Integration]
\subsection{Richardson Extrapolation [for Integration]}

\vspace{5pt}
\(\hspace{10pt}
    \begin{aligned}
        % Richardson Extrap.
        &\arraycolsep=2pt \begin{array}{l c c c c c c}
                F(h) &= 
                    &I(f) 
                    &+ 
                    &a_1 h^p 
                    &+ 
                    &\mathcal{O}(h^{q > p})
                    \\[5pt]
                F(\tfrac{h}{k}) &=   
                    &I(f) 
                    &+ 
                    &a_1 ( \tfrac{h}{k} )^p 
                    &+ 
                    &\mathcal{O}(h^{r \geq q})
            \end{array}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            \boxed{ I(f) = \frac{ k^{p} \hs F( \tfrac{h}{k} ) - F(h) }{ k^{p} - 1 } + \mathcal{O}(h^{q>p}) }
            \\[10pt]
        % Romberg Integr.
        &\bullet\ \text{Romberg Integration [Quadratic Extrapolation for Comp. Trapezoidal Rule]}:
            \\[5pt]
        % Taylor Exp. for Trap. Rule w/ Error
        &\begin{aligned}
                T(f, \tfrac{h}{2^k} ) &= I(f) 
                    + 2^k \left[ a_1 ( \tfrac{h}{2^k} )^3 
                    + \mathcal{O}(\tfrac{h}{2^k}^{5}) \right]
                    \\
                T_{k,j=0} &= I(f) 
                    + h a_1 [\tfrac{h}{2^k}]^2 
                    + h\hs \mathcal{O}([\tfrac{h}{2^k}]^4)
                    \\[15pt]
                4T_{k+1,0} &= 4I(f) 
                    + h a_1 [\tfrac{h}{2^k}]^2 
                    + \tfrac{h}{4}\hs \mathcal{O}( [\tfrac{h}{2^{k}}]^4)
            \end{aligned}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            % Recursive Alg.
            \begin{aligned}
                T_{k+1, j+1} &\equiv \frac{ 4^{j+1}\ T_{k+1, j} - T_{k, j} }{ 4^{j+1} - 1 }
                    \hspace{20pt} {\scriptstyle (1 \leq j \leq k)}
                    \\[5pt]
                \Aboxed{ I(f) &= T_{k, j} + \mathcal{O}(h^{2j+2}) }
            \end{aligned}
    \end{aligned}
\)

%-----------------------------------------------------------------
%-----------------------------------------------------------------
%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Numerical Differentiation
\vspace{5pt}
\section{Numerical Differentiation}

% Conditioning
\underline{Conditioning}:\ \ {\scriptsize Inverse of Integration - which smoothes noisy data - so %
    derivatives are inherently sensitive to small changes.%
} 

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Finite-Difference Approx
\vspace{10pt}
\begin{minipage}[t]{.5\textwidth}
    \subsection{Finite-Difference Approx}

    \(\arraycolsep=2pt \begin{array}{r c c c l}
        f'(x) &= 
            &\frac{f(x+h) - f(x)}{h} 
            &- 
            &\mathscriptsize{\displaystyle\sum_{n=2}^\infty}\hs \frac{f^{(n)}(x)}{n!} h^{n-1}
            \\
        &= 
            &\frac{f(x) - f(x-h)}{h} 
            &- 
            &\mathscriptsize{\displaystyle\sum_{n=2}^\infty}\hs \frac{f^{(n)}(x)}{n!} (-h)^{n-1}
            \\
        &= 
            &\frac{f(x+h) - f(x-h)}{2h} 
            &- 
            &\mathscriptsize{\displaystyle\sum_{n=3}^\text{odd}}\hs \frac{f^{(n)}(x)}{n!} h^{n-1}
    \end{array}\)   
    
    % Notes
    \vspace{10pt}
    \(\bullet\ \text{\scriptsize Use more points \(n\) for higher order approx.}\)
\end{minipage}
%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Deriving Polynomial Interpolant
\begin{minipage}[t]{.49\textwidth}
    \subsection{Deriving Interpolant}

    \(\begin{aligned}
        &\begin{aligned}
                f(x) &\ \approx\ \hat{f}_n(x) = p_{n-1}(x) \in P_{n-1}\\
                f^{(m)}(x)&\ \approx\ \hat{f}_n^{(m)}(x)
            \end{aligned}   
            \\[5pt]
        &\bullet\ \text{\scriptsize Equivalent but easier than finite-diff. approach.}\\
        &\bullet\ \text{\scriptsize Using more points \(n\) leads to better accuracy.}\\
        &\bullet\ \text{\scriptsize Polynomials, or other interpolants like trig. func. can be used.}
    \end{aligned}\)
\end{minipage}

%-----------------------------------------------------------------
%-----------------------------------------------------------------
% Richardson Extrapolation
\vspace{10pt}
\subsection{Richardson Extrapolation [for Differentiation]}

\vspace{5pt}
\(\hspace{10pt}
    \begin{aligned}
        % Richardson Extrap.
        &\arraycolsep=2pt \begin{array}{l c c c c c c}
                F(h) &= 
                    &D(f) 
                    &+ 
                    &a_1 h^p 
                    &+ 
                    &\mathcal{O}(h^{q > p})
                    \\[5pt]
                F(\tfrac{h}{k}) &=   
                    &D(f) 
                    &+ 
                    &a_1 ( \tfrac{h}{k} )^p 
                    &+ 
                    &\mathcal{O}(h^{r \geq q})
            \end{array}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            \boxed{ D(f) = \frac{ k^{p} \hs F( \tfrac{h}{k} ) - F(h) }{ k^{p} - 1 } + \mathcal{O}(h^{q>p}) }
            \\[10pt]
        % Example
        &\bullet\ \text{E.g.} \hspace{10pt} D(f) = \tfrac{f(x+h) - f(x)}{h} + \mathcal{O}(h)
            \\[10pt]
        &\hspace{10pt}\begin{aligned}
                F(h) &= \frac{f(x+h) - f(x)}{h}\\[5pt]
                F(\tfrac{h}{2}) &= \frac{f(x + \tfrac{h}{2}) - f(x)}{h/2}
            \end{aligned}
            \hspace{10pt} \Rightarrow \hspace{10pt}
            \boxed{ 
                D(f) = \frac{ 
                    2 \cdot \tfrac{f(x + h/2) - f(x)}{h/2} 
                    - \tfrac{f(x+h) - f(x)}{h}
                }{ 2-1 }
                + \mathcal{O}(h^2)
            }
    \end{aligned}
\)

%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------------------------------------------
%
\newpage

\end{document}